{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Building a Chatbot Using Attention-Based Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The theory of attention within neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our sequence-to-sequence model for sentence translation (with no attention implemented), we used both encoders and decoders. **The encoder obtained a hidden state from the input sentence**, which was a representation of our sentence. **The decoder then used this hidden state to perform the translation steps**. A basic graphical illustration of this is as follows:\n",
    "\n",
    "![](graphical_repr_seq2seq_model.png)\n",
    "\n",
    "However, **decoding over the entirety of the hidden state is not necessarily the most efficient way of using this task**. This is because the hidden state represents the entirety of the input sentence; however, in some tasks (such as predicting the next word in a sentence), **we do not need to consider the entirety of the input sentence, just the parts that are relevant to the prediction** we are trying to make. We can show that by using attention within our sequence-to-sequence neural network. We can teach our model to only look at the relevant parts of the input in order to make its prediction, resulting in a much more efficient and accurate model.\n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "***I will be traveling to Paris, the capital city of France, on the 2nd of March. My flight leaves from London Heathrow airport and will take approximately one hour.***\n",
    "\n",
    "Let's say that we are training a model to predict the next word in a sentence. We can first input the start of the sentence:\n",
    "\n",
    "***The capital city of France is _____.***\n",
    "\n",
    "We would expect our model to be able to retrieve the word Paris, in this case. If we were to use our basic sequence-to-sequence model, we would transform our entire input into a hidden state, which our model would then try to extract the relevant information out of. This includes all the extraneous information about flights. You may notice here that we only need to look at a small part of our input sentence in order to identify the relevant information required to complete our sentence:\n",
    "\n",
    "***I will be traveling to <font color=pink>Paris, the capital city of France</font>, on the 2nd of March. My flight leaves from London Heathrow airport and will take approximately one hour.***\n",
    "\n",
    "Therefore, if we can train our model **to only use the relevant information within the input sentence**, we can make **more accurate and relevant predictions. We can implement attention within our networks** in order to achieve this.\n",
    "\n",
    "There are two main types of attention mechanisms that we can implement: local and global attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparing local and global attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $2$ forms of attention that we can implement within our networks are very similar, but with subtle key differences. We will start by looking at local attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **<font color=pink>local attention</font>**, our model **only looks at a few hidden states from the encoder**. For example, if we are performing a sentence translation task and we are calculating the $2^{nd}$ word in our translation, **the model may wish to only look at the hidden states from the encoder related to the $2^{nd}$ word in the input sentence**. This would mean that **our model needs to look at the $2^{nd}$ hidden state from our encoder $(h_2)$ but maybe also the hidden state before it $(h_1)$**.\n",
    "\n",
    "In the following diagram, we can see this in practice:\n",
    "\n",
    "![](local_attention_model.png)\n",
    "\n",
    "We first **start by calculating the aligned position, $p_t$, from our final hidden state, $h_n$**. This tells us **which hidden states we need to be looking at to make our prediction**. We then **calculate our local weights and apply them to our hidden states in order to determine our context vector**. These weights may tell us **to pay more attention to the most relevant hidden state $(h_2)$ but less attention to the preceding hidden state $(h_1)$**.\n",
    "\n",
    "We then **take our context vector and pass it forward to our decoder in order to make its prediction.** In our **non-attention based sequence-to-sequence model, we would have only passed our final hidden state, $h_n$, forward**, but we see here that instead, we **only consider the relevant hidden states that our model deems necessary** to make its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **<font color=pink>global attention</font>** model **works in a very similar way**. **However, instead of only looking at a few of the hidden states, we want to look at all of our model's hidden states**—hence the name global. We can see a graphical illustration of a global attention layer here:\n",
    "\n",
    "![](global_attention_model.png)\n",
    "\n",
    "We can see in the preceding diagram that although this appears very similar to our local attention framework, **our model is now looking at all the hidden states and calculating the global weights across all of them**. This **allows our model to look at any given part of the input sentence that it considers relevant, instead of being limited to a local area determined by the local attention methodology**. Our model may wish to only look at a small, local area, but this is within the capabilities of the model. **An easy way to think of the global attention framework is that it is essentially learning a mask that only allows through hidden states that are relevant to our prediction:**\n",
    "\n",
    "![](combined_model.png)\n",
    "\n",
    "We can see in the preceding diagram that **by learning which hidden states to pay attention to, our model controls which states are used in the decoding step to determine our predicted output**. Once we decide which hidden states to pay attention to, we can combine them using a number of different methods—either by concatenating or taking the weighted dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a chatbot using sequence-to-sequence neural networks with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Acquiring our dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processing our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
      "\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
      "\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 Let's read the dataset\n",
    "import os\n",
    "corpus = \"movie_corpus\"\n",
    "corpus_name = \"movie_corpus\"\n",
    "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
    "with open(datafile, 'rb') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines[:3]:\n",
    "        print(str(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating the vocabulary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past, our corpus has comprised of several dictionaries consisting of the unique words in our corpus and lookups between word and indices. However, we can do this in a far more elegant way **by creating a vocabulary class that consists of all of the elements required**:\n",
    "\n",
    "1. We start by creating our `vocabulary` class. We initialize this class with empty dictionaries-`word2index` and `word2count`. We also initialize this `index2word` dictionary with placeholders for our `padding tokens`, as well as our `Start-of-Sentence (SOS)` and `End-of-Sentence (EOS)` tokens. We keep a running count of the number of words in our vocabulary, too (which is $3$ to start with as our corpus already contains the three tokens mentioned). These are the default values for an empty vocabulary; however, they will be populated as we read our data in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we create the functions that we will use to populate our vocabulary. `addWord` takes a word as input. **If this is a new word that is not already in our vocabulary, we add this word to our indices**, **set the count of this word** to $1$, and **increment the total number of words in our vocabulary** by $1$. **If the word in question is already in our vocabulary**, we simply **increment the count of this word** by $1$.\n",
    "\n",
    "```python\n",
    "    def addWord(self, w):\n",
    "        if w not in self.word2index:\n",
    "            self.word2index[w] = self.num_words\n",
    "            self.word2count[w] = 1\n",
    "            self.index2word[self.num_words] = w\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[w] += 1\n",
    "```\n",
    "\n",
    "3. We also use the `addSentence` function to apply the `addWord` function to all the words within a given sentence:\n",
    "\n",
    "```python\n",
    "    def addSentence(self, sent):\n",
    "        for word in sent.split(''):\n",
    "            self.addWord(word)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can do to **speed up the training of our model** is **reduce the size of our vocabulary**. This means that **any embedding layers will be much smaller and the total number of learned parameters within our model can be fewer**. An easy way to do this is to **remove any low-frequency words from our vocabulary**. Any words occurring just once or twice in our dataset are unlikely to have huge predictive power, and so removing them from our corpus and replacing them with blank tokens in our final model could reduce the time taken for our model to train and reduce overfitting without having much of a negative impact on our model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To remove low-frequency words from our vocabulary, we can implement a `trim()` function. The function first loops through the word count dictionary and if the occurrence of the word is greater than the minimum required count, it is appended to a new list:\n",
    "\n",
    "```python\n",
    "    def trim(self, min_cnt):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        words_to_keep = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_cnt:\n",
    "                words_to_keep.append(k)\n",
    "        print('Words to keep: {} / {} = {:.2%}'.format(len(words_to_keep), len(self.word2index), len(words_to_keep)/len(self.word2index)))\n",
    "```\n",
    "5. Finally, our indices are rebuilt from the new `words_to_keep` list. We set all the indice to their initial empty values and then repopulate them by looping through our kept words with the `addWord` function:\n",
    "\n",
    "```python\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", sos_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "\n",
    "        self.num_words = 3\n",
    "        for w in words_to_keep:\n",
    "            self.addWord(w)\n",
    "```\n",
    "\n",
    "We have defined a vocabulary class that can be easily poplulated with our input sentences, Next, we actually need to load in our dataset to create our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Here the complete code of the vocabulary class***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values for our empty vocabulary\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "# 1 We create the vocabulary class\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "    \n",
    "    # 2 The function use to populate our vocabulary\n",
    "    def addWord(self, w):\n",
    "        if w not in self.word2index:\n",
    "            self.word2index[w] = self.num_words\n",
    "            self.word2count[w] = 1\n",
    "            self.index2word[self.num_words] = w\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[w] += 1\n",
    "\n",
    "    # 3 To apply addWord to all the words of a given sentence\n",
    "    def addSentence(self, sent):\n",
    "        for word in sent.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    # 4 To remove low-frequency words from our vocabulary\n",
    "    def trim(self, min_cnt):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        words_to_keep = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_cnt:\n",
    "                words_to_keep.append(k)\n",
    "        print('Words to keep: {} / {} = {:.2%}'.format(len(words_to_keep), len(self.word2index), len(words_to_keep)/len(self.word2index)))\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "\n",
    "        self.num_words = 3\n",
    "        for w in words_to_keep:\n",
    "            self.addWord(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start loading in the data using the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 The first step for reading in our data is to perform any\n",
    "# necessary steps to clean the data and make it more\n",
    "# human-readable. We start by converting it from Unicode\n",
    "# into ASCII format. We can easily use a function to do this:\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# 2 Next, we want to process our input strings so that they are\n",
    "# all in lowercase and do not contain any trailing whitespace or\n",
    "# punctuation, except the most basic characters. We can do this\n",
    "# by using a series of regular expressions\n",
    "def cleanStrings(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# 3 Finally, we apply this function within a wider funtion\n",
    "# readVocs. This functions reads our data file into lines and\n",
    "# then applies the cleanString function to every line. It also\n",
    "# creates an instance of the vocabulary class that we created earlier\n",
    "# meaning, this function outputs both our data and vocabulary:\n",
    "def readVocs(datafile, corpus_name):\n",
    "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[cleanStrings(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Vocabulary(corpus_name)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we **filter our input pairs by their maximum length**. This is again done **to reduce the potential dimensionality of our model**. **Predicting sentences that are hundreds of words long would require a very deep architecture**. In the interest of training time, we want to limit our training data here to instances where the input and output are less than $10$ words long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. To do this, we create a couple of filter functions. The first one,\n",
    "# `filterPair`, returns a Boolean value based on whether the current\n",
    "# line has an input and output length that is less than the maximum\n",
    "# length. Our second function, `filterPairs`, simply applies this condition\n",
    "# to all the pairs within our dataset, only keeping the ones that meet this condition:\n",
    "def filterPair(p, max_length):\n",
    "    return len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length\n",
    "\n",
    "def filterPairs(pairs, max_length):\n",
    "    return [pair for pair in pairs if filterPair(pair, max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221282 Sentence pairs\n",
      "70015 Sentence pairs after trimming\n",
      "27501 Distinct words in vocabulary\n"
     ]
    }
   ],
   "source": [
    "# 5. Now, we just need to create one final function that applies all the previous\n",
    "# functions we have put together and run it to create our vocabulary and data pairs:\n",
    "def loadData(corpus, corpus_name, datafile, max_length):\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(str(len(pairs)) + \" Sentence pairs\")\n",
    "    pairs = filterPairs(pairs, max_length)\n",
    "    print(str(len(pairs)) + \" Sentence pairs after trimming\")\n",
    "    for p in pairs:\n",
    "        voc.addSentence(p[0])\n",
    "        voc.addSentence(p[1])\n",
    "    print(str(voc.num_words) + \" Distinct words in vocabulary\")\n",
    "    return voc, pairs\n",
    "\n",
    "max_length = 10\n",
    "voc, pairs = loadData(corpus, corpus_name, datafile, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our input dataset consists of over $200,000$ pairs. When we filter this to sentences where both the input and output are less than $10$ words long, this reduces to just $64,000$ pairs consisting of $18,000$ distinct word (previous result):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Pairs:\n",
      "['three minutes to go !', 'yes .']\n",
      "['yes .', \"what d'ya want to do to kill time ?\"]\n",
      "['another fifteen seconds to go .', 'do something ! stall them !']\n",
      "['yes, sir, name, please ?', 'food !']\n",
      "['food !', 'do you have a reservation ?']\n",
      "['do you have a reservation ?', 'food ! !']\n",
      "['grrrhmmnnnjkjmmmnn !', 'franz ! help ! lunatic !']\n",
      "[\"what o'clock is it, mr noggs ?\", \"eleven o'clock, my lorj 42\"]\n",
      "['stuart ?', 'yes .']\n",
      "['yes .', 'how quickly can you move your artillery forward ?']\n"
     ]
    }
   ],
   "source": [
    "#  6 We can print a selection of our processed input/output\n",
    "# pairs in order to verify that our functions have all worked correctly:\n",
    "print(\"Example Pairs:\")\n",
    "for pair in pairs[-10:]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we have successfully split our dataset into input and output pairs upon which we can train our network.\n",
    "\n",
    "Finally, before we begin building the model, we must remove the rare words from our corpus and data pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing rare words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, including words that only occur a few times within our dataset will increase the dimensionality of our model, increasing our model's complexity and the time it will take to train the model. Therefore, it is preferred to remove them from our training data to keep our model as streamlined and efficient as possible.\n",
    "\n",
    "You may recall earlier that we built a `trim()` function into our vocabulary, which will allow us to remove infrequently occurring words from our vocabulary. We can now create a function to remove these rare words and call the `trim()` method from our vocabulary as our first step. You will see that this removes a large percentage of words from our vocabulary, indicating that the majority of the words in our vocabulary occur infrequently. This is expected as the distribution of words within any language model will follow a long-tail distribution. We will use the following steps to remove the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words to keep: 9716 / 27498 = 35.33%\n",
      "Trimmed from 70015 pairs to 52223, 74.59% of total\n"
     ]
    }
   ],
   "source": [
    "# 1 We first calculate the percentage of words that we will keep within our model\n",
    "def removeRareWords(voc, all_pairs, minimum):\n",
    "    voc.trim(minimum)\n",
    "    # 2 we loop through all the words in the input and output sentences.\n",
    "    # If for a given pair either the input or output sentence has a word\n",
    "    # that isn't in our new trimmed corpus, we drop this pair from our dataset.\n",
    "    # We print the output and see that even though we have dropped over half\n",
    "    # of our vocabulary, we only drop around 17% of our training pairs. This\n",
    "    # again reflects how our corpus of words is distributed over our individual\n",
    "    # training pairs:\n",
    "    pairs_to_keep = []\n",
    "    for p in all_pairs:\n",
    "        keep = True\n",
    "        for word in p[0].split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "        for word in p[1].split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "        if keep:\n",
    "            pairs_to_keep.append(p)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.2%} of total\".format(len(all_pairs), len(pairs_to_keep), len(pairs_to_keep)/ len(all_pairs)))\n",
    "    return pairs_to_keep\n",
    "\n",
    "minimum_count = 3\n",
    "pairs = removeRareWords(voc, pairs, minimum_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transforming sentence pairs to tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that **our model will not take raw text as input**, but rather, **tensor representations of sentences**. We will also **not process our sentences one by one, but instead in smaller batches**. For this, **we require both our input and output sentences to be transformed into tensors**, where the **width of the tensor represents the size of the batch** that we wish to train on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "# 1 We start by creating helper functions, which we can use to transform\n",
    "# out pairs into tensors. We first create a indexFromSentence function,\n",
    "# which grabs the index of each word in the sentence from the vocabulary and\n",
    "# appends EOS token to the end\n",
    "def indexFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "# 2 Secondly, we create a zeroPad function which pads any tensors with\n",
    "# zeros so that all of the sentences within the tensor are effectively the\n",
    "# same length\n",
    "def zeroPad(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "# 3 Then, to generate our input tensor, we apply both of these functions\n",
    "#  First, we get the indices of our input sentence, then apply padding,\n",
    "# and then transform the output into LongTensor. We will also obtain the\n",
    "# lengths of each of our input sentences out output this as a tensor:\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    return padTensor, lengths\n",
    "\n",
    "# 4 Within our model, our padded tokens should gererally be ignored.\n",
    "# We don't want to train our model on these padded tokens, so we create a\n",
    "# boolean mask to ignore these tokens. To do so, we use a getMask function,\n",
    "# which we apply to our output tensor. This simply returns 1 if the output consists\n",
    "# of a word and 0 if it consists of a padding token\n",
    "def getMask(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# 5 We then apply this to our outputVar function, this is identical to the inputVar\n",
    "# function except that along with the indexed output tensor and the tensor of lengths,\n",
    "# we also return the boolean mask of our output tensor. This boolean mask just returns\n",
    "# True when there is a word within the output tensor and False when there is a padding\n",
    "# token. We also return the maximum length of sentences within our output tensor:\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    mask = torch.BoolTensor(getMask(padList))\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    return padTensor, mask, max_target_len\n",
    "\n",
    "# 6 Finally in order to create our input and output batches concurrently, we loop\n",
    "# through the pairs in our batch and create input and output tensors for both pairs\n",
    "# using the functions we created previously. We then return all the necessary variables:\n",
    "def batch2Train(voc, batch):\n",
    "    batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "\n",
    "    for p in batch:\n",
    "        input_batch.append(p[0])\n",
    "        output_batch.append(p[1])\n",
    "        inp, lengths = inputVar(input_batch, voc)\n",
    "        output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# 7 This function should be all we need to transform our training pairs into\n",
    "# tensors for training our model. We can validate that this is working correctly\n",
    "# by performing a single iteration of our batch2Train function on a random\n",
    "# selection of our data. We set our batch size to 5 and run this once:\n",
    "\n",
    "test_batch_size = 5\n",
    "\n",
    "batches = batch2Train(voc, [random.choice(pairs) for _ in range(test_batch_size)])\n",
    "\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can **validate that our input tensor has been created correctly**. Note how the **sentences end with padding** ($0$ tokens) where the **sentence length is less than the maximum length for the tensor** (in this instance, $9$). The **width of the tensor also corresponds to the batch size** (in this case, $5$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1061,  189, 2616,  802,  396],\n",
       "        [ 195,   16, 1802,   10,   10],\n",
       "        [ 167,  476,   35, 1064,    2],\n",
       "        [ 763, 1946,   46,   10,    0],\n",
       "        [  29,   15,   10,    2,    0],\n",
       "        [  64,    2,    2,    0,    0],\n",
       "        [ 196,    0,    0,    0,    0],\n",
       "        [  10,    0,    0,    0,    0],\n",
       "        [   2,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can also validate the corresponding output data and mask. Notice how the False values in the mask overlap with the padding tokens (the zeroes) in our output tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  33,  575,    3,   57,  805],\n",
       "        [  32,   10,  384,   15,   16],\n",
       "        [ 128,    2,   33,    2,  144],\n",
       "        [  78,    0,  505,    0,  876],\n",
       "        [1965,    0,   10,    0, 1076],\n",
       "        [  10,    0,    2,    0, 1218],\n",
       "        [   2,    0,    0,    0,   10],\n",
       "        [   0,    0,    0,    0,    2]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True],\n",
       "        [ True, False,  True, False,  True],\n",
       "        [ True, False,  True, False,  True],\n",
       "        [ True, False,  True, False,  True],\n",
       "        [ True, False, False, False,  True],\n",
       "        [False, False, False, False,  True]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained, cleaned, and transformed our data, we are ready to begin training the attention-based model that will form the basis of our chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start, as with our other sequence-to-sequence models, by creating our encoder. This will transform the initial tensor representation of our input sentence into hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Constructing the encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As with all our PyTorch models, we start by creating an `Encoder` class that inherits from `nn.Module`.\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0:\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "```\n",
    "Next we create our `RNN` module. In this chatbot, we will be using a `Gated Recurrent Unit (GRU)` instead of the `Long Short-Term Memory (LSTM)` models we saw before. **GRUs are slightly less complex than LSTMs as although they still control the flow of information through the RNN**, they **don't have separate forget and update gates** like the LSTM. We use GRUs in this instance for a few main reasons:\n",
    "\n",
    "a- GRUs have proven to be **more computationally efficient as there are fewer parameters to learn**. This means that our model will train much more quickly with GRUs than with LSTMs\n",
    "\n",
    "b- GRUs have proven to **have similar performance levels over short sequences of data as LSTMs**. LSTMs are more useful when learning longer sequences of data. In this instance we are only using input sentences with $10$ words or less, so GRUs should produce similar results.\n",
    "\n",
    "c- GRUs have proven to be **more effective at learning from small datasets than LSTMs**. As the size of our training data is small relative to the complexity of the task we're trying to learn, we should opt to use GRUs.\n",
    "\n",
    "2. We now define our GRU, taking into account the size of our input, the number of layers, and whether we should implement dropout:\n",
    "\n",
    "```python\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout)), bidirectional=True)\n",
    "```\n",
    "Notice here how **we implement bidirectionality into our model**. You will recall from previous chapters that **a bidirectional RNN allows us to learn from a sentence moving sequentially forward through a sentence, as well as moving sequentially backward**. This allows us to better capture the context of each word in the sentence relative to those that appear before and after it. Bidirectionality in our GRU means our encoder looks something like this:\n",
    "\n",
    "![](encoder_label_gru_bidirectional.png)\n",
    "\n",
    "> We maintain two hidden states, as well as outputs at each step, within our input sentence.\n",
    "\n",
    "3. Next, we need to create a forward pass for our encoder. We do this by first embedding our input sentences and then using the `pack_padded_sequence` function on our embeddings. This function \"packs\" our padded sequence so that all of our inputs are of the same length. We then pass out the packed sequences through our GRU to perform a forward pass:\n",
    "\n",
    "```python\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "```\n",
    "\n",
    "4. After this, we unpack our padding and sum the GRU outpus. We can then return this summed output, along with our final hidden state, to complete our forward pass\n",
    "\n",
    "```python\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + a outputs[:, : ,self.hidden_size:]\n",
    "        return outputs, hidden\n",
    "```\n",
    "\n",
    "Here is the complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0.0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:,:,:self.hidden_size] + outputs[:,:, self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, we will move on to creating an attention module in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Constructing the attention module**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by creating a class for the attention model\n",
    "\n",
    "```python\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "```\n",
    "\n",
    "2. Then create the `dot_score()` function within this class. This function simply calculates the dot product of our encoder output with the output of our hidden state by our encoder. While there are other ways of transforming these two tensors into a single representation, using a dot product is one of the simplest\n",
    "\n",
    "```python\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "```\n",
    "\n",
    "3. We then use this function within our forward pass. First, calculate the attention weigths/energies based on the `dot_score` method, then transpose the results, and return the softmax transformed probability scores:\n",
    "\n",
    "```python\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "```\n",
    "The complete code of the Attention class is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use this attention module within our decoder to create an attention-focused decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct the decoder, as follows:\n",
    "\n",
    "1. We begin by creating our DecoderRNN class, inheriting from nn.Module and defining the initialization parameters:\n",
    "```python\n",
    "class DecoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "```\n",
    "2. We then create our layers within this module. We will create an embedding layer and a corresponding dropout layer. We use GRUs again for our decoder; however, this time, we do not need to make our GRU layer bidirectional as we will be decoding the output from our encoder sequentially. We will also create two linear layers—one regular layer for calculating our output and one layer that can be used for concatenation. This layer is twice the width of the regular hidden layer as it will be used on two concatenated vectors, each with a length of `hidden_size`. We also initialize an instance of our attention module from the last section in order to be able to use it within our `Decoder` class:\n",
    "\n",
    "```python\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,  dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(hidden_size)\n",
    "```\n",
    "\n",
    "3. After defining all of our layers, we need to create a forward pass for the decoder. Notice how the forward pass will be used one step (word) at a time. We start by getting the embedding of the current input word and making a forward pass through the GRU layer to get our output and hidden states:\n",
    "\n",
    "```python\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "```\n",
    "4. Next, we use the attention module to get the attention weights from the GRU output. These weights are then multiplied by the encoder outputs to effectively give us a weighted sum of our attention weights and our encoder output:\n",
    "\n",
    "```python\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "```\n",
    "\n",
    "5. We then concatenate our weighted context vector with the output of our GRU and apply a tanh function to get out final concatenated output:\n",
    "\n",
    "```python\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "```\n",
    "\n",
    "6. For the final step within our decoder, we simply use this final concatenated output to predict the next word and apply a `softmax` function. The forward pass finally returns this output, along with the final hidden state. This forward pass will be iterated upon, with the next forward pass using the next word in the sentence and this new hidden state:\n",
    "\n",
    "```python\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "```\n",
    "The complete code for the Decoder class is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, output_size, n_layers=1, dropout=.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Defining the training process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the training process is to **define the measure of loss for our models**. As our input tensors may consist of padded sequences, owing to our input sentences all being of different lengths, we cannot simply calculate the difference between the true output and the predicted output tensors. To account for this, **we will define a loss function that applies a Boolean mask over our outputs and only calculates the loss of the non-padded tokens**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the following function, we can see that we calculate **cross-entropy loss** accross the whole output tensors. However, **to get the total loss, we only average over the elements of the tensor that are selected by the boolean mask**\n",
    "\n",
    "```python\n",
    "def NLLMaskLoss(inp, target, mask):\n",
    "    TotalN = mask.sum()\n",
    "    CELoss = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = CELoss.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, TotalN.item()\n",
    "```\n",
    "\n",
    "2. For the majority of our training, we need two main functions-one function `train()`, which performs training on a single batch of our training data and another function, `trainIters()`, which iterates through our whole dataset and calls `train()` on each of the individual batches. We start by defining `train()` in order to train on a single batch of data. Create the `train()` function, then get the gradients to $0$, define the device options, and initialize the variables:\n",
    "\n",
    "```python\n",
    "def train(input_variable, lengths, target_variable,\n",
    "    mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_otpimizer, batch_size, clip, max_length=max_length):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    input_variable = input_variable.to(devide)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "```\n",
    "\n",
    "3. Then, perform a forward pass of the inputs and sequence lengths through the ecndoer to get the output and hidden states:\n",
    "\n",
    "```python\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "```\n",
    "\n",
    "4. Next, we create our initial decoder input, starting with `SOS` tokens for each sentence. We then set the initial gidden state of our decoder to be equal to that of the encdoer:\n",
    "\n",
    "```python\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "```\n",
    "\n",
    "Next, we implement teacher forcing. If you recall from the last chapter, teacher forcing, when generating output sequences with some given probability, we use the true previous output token rather than the predicted previous output token to generate the next word in our output sequence. Using teacher forcing helps our model converge much more quickly; however, we must be careful not to make he teacher forcing ration too high or else our model will be too reliant on the teacher forcing and will not learn to generate the correct output independently.\n",
    "\n",
    "5. Determine wether we should use teacher forcing for the current step\n",
    "```python\n",
    "    use_TF = True if random.random() < teacher_forcing_ratio else False\n",
    "```\n",
    "6. Then, if we do need to implement teacher forcing, run the following code. We pass each of our sequence batches through the decoder to obtain our output. We then set the next input as the true output (`target`). Finally, we calculate and accumulate the loss using our loss function and print this to the console:\n",
    "\n",
    "```python\n",
    "    for t in range(max_target_len):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_output, decoder_hidden, encoder_outputs)\n",
    "        decoder_input = target_variable[t].view(1, -1)\n",
    "        mask_loss, nTotal = NLLMaskLoss(decoder_output, target_variable[t], mask[t])\n",
    "        loss += mask_loss\n",
    "        print_losses.append(mask_loss.item() * nTotal)\n",
    "        n_totals += nTotal\n",
    "```\n",
    "\n",
    "7. If we do not implement teacher forcing on a given batch, the procedure is almost identical. However, instead of using the true output as the next input into the sequence, we use the one generated by the model:\n",
    "\n",
    "```python\n",
    "        _, topi = decoder_output.topk(1)\n",
    "\n",
    "        decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "        decoder_input = decoder_input.to(device)\n",
    "```\n",
    "\n",
    "8. Finally, as with all of our models, the final steps are to perform backpropagation, implement gradient clipping, and step through both of our encoder and decoder optimizers to update the weights using gradient descent. Remember that we clip out gradients in order to prevent the vanishing/exploding gradient problem, which was discussed in earlier chapters. Finally, our training step returns our average loss:\n",
    "\n",
    "```python\n",
    "    loss.backward()\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return sum(print_losses) / n_totals\n",
    "```\n",
    "\n",
    "9. Next, as previously stated, we need to create the `trainIters()` function, which repeatedly calls our training function on different batches of input data. We start by splitting our data into `batches` using the batch2Train function we created earlier:\n",
    "\n",
    "```python\n",
    "def trainIters(model_name, voc, pairs, encoder, decoder,\\\n",
    "               encoder_optimizer, decoder_optimizer,\\\n",
    "               embedding, encoder_n_layers, \\\n",
    "               decoder_n_layers, save_dir, n_iteration,\\\n",
    "               batch_size, print_every, save_every, \\\n",
    "               clip, corpus_name, loadFilename):\n",
    "\n",
    "    training_batches = [batch2Train(voc, [random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)]\n",
    "```\n",
    "\n",
    "\n",
    "10. We then create a few variables that will allow us to count iterations and keep track of the total loss over each epoch:\n",
    "\n",
    "```python\n",
    "    print('Starting ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "```\n",
    "\n",
    "11. Next, we define our training loop. For each iteration, we get a training batch from our list of batches. We then extract the relevant fields from our batch and run a single training iteration using these parameters. Finally, we add the loss from this batch to our overall loss:\n",
    "\n",
    "```python\n",
    "        print(\"Beginning Training...\")\n",
    "        for iteration in range(start_iteration, n_iteration + 1):\n",
    "            training_batch = training_batches[iteration - 1]\n",
    "            input_variable, lengths, target_variable, mask, \\\n",
    "                  max_target_len = training_batch\n",
    "            loss = train(input_variable, lengths,\\\n",
    "                         target_variable, mask, max_target_len,\\\n",
    "                         encoder, decoder, embedding, \\\n",
    "                         encoder_optimizer, decoder_optimizer,\\\n",
    "                         batch_size, clip)\n",
    "            print_loss += loss\n",
    "```\n",
    "\n",
    "12. On every iteration, we also make sure we print our progress so far, keeping track of how many iterations we have completed and what our loss was for each epoch:\n",
    "\n",
    "```python\n",
    "            if iteration % print_every == 0:\n",
    "                print_loss_avg = print_loss / print_every\n",
    "                print(\"Iteration: {}; Percent done: {:.1f}%;\\\n",
    "                Mean loss: {:.4f}\".format(iteration,\n",
    "                                      iteration / n_iteration \\\n",
    "                                      * 100, print_loss_avg))\n",
    "                print_loss = 0\n",
    "```\n",
    "\n",
    "13. For the sake of completion, we also need to save our model state after every few epochs. This allows us to revisit any historical models we have trained; for example, if our model were to begin overfitting, we could revert back to an earlier iteration:\n",
    "\n",
    "```python\n",
    "            if (iteration % save_every == 0):\n",
    "                directory = os.path.join(save_dir, model_name,\\\n",
    "                                     corpus_name, '{}-{}_{}'.\\\n",
    "                                     format(encoder_n_layers,\\\n",
    "                                    decoder_n_layers, \\\n",
    "                                     hidden_size))\n",
    "                if not os.path.exists(directory):\n",
    "                        os.makedirs(directory)\n",
    "                        torch.save({\n",
    "                            'iteration': iteration,\n",
    "                            'en': encoder.state_dict(),\n",
    "                            'de': decoder.state_dict(),\n",
    "                            'en_opt': encoder_optimizer.state_dict(),\n",
    "                            'de_opt': decoder_optimizer.state_dict(),\n",
    "                            'loss': loss,\n",
    "                            'voc_dict': voc.__dict__,\n",
    "                            'embedding': embedding.state_dict()\n",
    "                        }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "```\n",
    "\n",
    "Now that we have completed all the necessary steps to begin training our model, we need to create functions to allow us to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the comlete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1 \n",
    "def NLLMaskLoss(inp, target, mask):\n",
    "    TotalN = mask.sum()\n",
    "    CELoss = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = CELoss.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, TotalN.item()\n",
    "\n",
    "# 2\n",
    "def train(input_variable, lengths, target_variable,\\\n",
    "        mask, max_target_len, encoder, decoder,\\\n",
    "        embedding, encoder_optimizer,\\\n",
    "        decoder_optimizer, batch_size, clip,\\\n",
    "        max_length=max_length):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_variable = input_variable.to(device)\n",
    "    # TODO: Error after setting the variable to gpu\n",
    "    lengths = lengths.to(device).cpu()\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "    # 3\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "    # 4\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # 5\n",
    "    use_TF = True if random.random() < teacher_forcing_ratio else False\n",
    "    # 6\n",
    "    if use_TF:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            mask_loss, nTotal = NLLMaskLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # 7\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            mask_loss, nTotal = NLLMaskLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # 8\n",
    "    loss.backward()\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return sum(print_losses) / n_totals\n",
    "\n",
    "# 9\n",
    "def trainIters(model_name, voc, pairs, encoder, decoder,\\\n",
    "                encoder_optimizer, decoder_optimizer,\\\n",
    "                embedding, encoder_n_layers, \\\n",
    "                decoder_n_layers, save_dir, n_iteration,\\\n",
    "                batch_size, print_every, save_every, \\\n",
    "                clip, corpus_name, loadFilename):\n",
    "    training_batches = [batch2Train(voc, [random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)]\n",
    "    # 10\n",
    "    print('Starting ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "    # 11\n",
    "    print(\"Beginning Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "            training_batch = training_batches[iteration - 1]\n",
    "            input_variable, lengths, target_variable, mask, \\\n",
    "                max_target_len = training_batch\n",
    "            loss = train(input_variable, lengths,\\\n",
    "                target_variable, mask, max_target_len,\\\n",
    "                encoder, decoder, embedding, \\\n",
    "                encoder_optimizer, decoder_optimizer,\\\n",
    "                batch_size, clip)\n",
    "            print_loss += loss\n",
    "            # 12\n",
    "            if iteration % print_every == 0:\n",
    "                print_loss_avg = print_loss / print_every\n",
    "                print(\"Iteration: {}; Percent done: {:.1f}%;\\\n",
    "                    Mean loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "                print_loss = 0\n",
    "            # 13\n",
    "            if (iteration % save_every == 0):\n",
    "                directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.\\\n",
    "                    format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "                if not os.path.exists(directory):\n",
    "                    os.makedirs(directory)\n",
    "                    torch.save({\n",
    "                    'iteration': iteration,\n",
    "                    'en': encoder.state_dict(),\n",
    "                    'de': decoder.state_dict(),\n",
    "                    'en_opt': encoder_optimizer.state_dict(),\n",
    "                    'de_opt': decoder_optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    'voc_dict': voc.__dict__,\n",
    "                    'embedding': embedding.state_dict()}, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Defining the evaluating process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a chatbot is slightly different from evaluating other sequence-to-sequence models. In our text translation task, an English sentence will have one direct translation into German. While there may be multiple correct translations, for the most part, there is a single correct translation from one language into another.\n",
    "\n",
    "For chatbots, there are multiple different valid outputs. Take the following three lines from some conversations with a chatbot:\n",
    "\n",
    "**Input**: \"Hello\"\n",
    "\n",
    "**Outpu**t: \"Hello\"\n",
    "\n",
    "**Input**: \"Hello\"\n",
    "\n",
    "**Outpu**t: \"Hello. How are you?\"\n",
    "\n",
    "**Input**: \"Hello\"\n",
    "\n",
    "**Outpu**t: \"What do you want?\"\n",
    "\n",
    "Here, we have three different responses, each one equally valid as a response. Therefore, at each stage of our conversation with our chatbot, there will be no single \"correct\" response. So, evaluation is much more difficult. The most intuitive way of testing whether a chatbot produces a valid output is by having a conversation with it! This means we need to set up our chatbot in a way that enables us to have a conversation with it to determine whether it is working well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We will start by defining a class that will **allow us to decode the encoded input and produce text**. We do this by using what is known as a `greedy encoder`. This simply means that **at each step of the decoder, our model takes the word with the highest predicted probability as the output**. We start by initializing the `GreedyEncoder()` class with our pretrained encoder and decoder:\n",
    "\n",
    "```python\n",
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "```\n",
    "\n",
    "2. Next, define a forward pass for our decoder. We pass the input through our encoder to get our encoder's output and hidden state. We take the encoder's final hidden layer to be the first hidden input to the decoder:\n",
    "   \n",
    "```python\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
    "```\n",
    "\n",
    "3. Then, create the decoder input with SOS tokens and initialize the tensors to append decoded words to (initialize as a single zero value):\n",
    "```python\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "```\n",
    "\n",
    "4. After that, iterate through the sequence, decoding one word at a time. We perform a forward pass through the encoder and add a `max` function to obtain the highest-scoring predicted word and its score, which we then append to the `all_tokens` and `all_scores` variables. Finally, we take this predicted token and use it as the next input to our decoder. After the whole sequence has been iterated over, we return the complete predicted sentence:\n",
    "\n",
    "```python\n",
    "        for _ in range(max_length):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        return all_tokens, all_scores\n",
    "```\n",
    "\n",
    "All the pieces are beginning to come together. We have the defined training and evaluation functions, so the final step is to write a function that will actually take our input as text, pass it to our model, and obtain a response from the model. This will be the \"interface\" of our chatbot, where we actually have our conversation.\n",
    "\n",
    "5. We first define an `evaluate()` function, which takes our input function and returns the predicted output words. We start by transforming our input sentence into indices using our vocabulary. We then obtain a tensor of the lengths of each of these sentences and transpose it:\n",
    "\n",
    "```python\n",
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_lenth=max_lenght):\n",
    "    indices = [indexFromSentence(voc, sentence)]\n",
    "    lenghts = torch.tensor([len(indexes) for indexes in indices])\n",
    "    input_batch = torch.LongTensor(indices).transpose(0, 1)\n",
    "```\n",
    "\n",
    "6. Then, we assign our lengths and input tensors to the relevant devices. Next, run the inputs through the searcher (`GreedySearchDecoder`) to obtain the word indices of the predicted output. Finally, we transform these word indices back into word tokens before returning them as the function output:\n",
    "\n",
    "```python\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "```\n",
    "\n",
    "7. Finally, we create a `runchatbot()` function, which acts as the interface with our chatbot. This function takes human-typed input and prints the chatbot's response. We create this function as a `while` loop that continues until we terminate the function or type quit as our input:\n",
    "\n",
    "```python\n",
    "def runChatbot(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(True):\n",
    "        try:\n",
    "            input_sentence = input('> ')\n",
    "            if input_sentence == 'quit': break\n",
    "\n",
    "```\n",
    "\n",
    "8. We then take the typed input and normalize it, before passing the normalized input to our `evaluate()` function, which returns the predicted words from the chatbot:\n",
    "\n",
    "```python\n",
    "            input_sentence = cleanString(input_sentence)\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "```\n",
    "\n",
    "9. Finally, we take these output words and format them, ignoring the EOS and padding tokens, before printing the chatbot's response. Because this is a while loop, this allows us to continue the conversation with the chatbot indefinitely:\n",
    "\n",
    "```python\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Response:', ' '.join(output_words))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    # 2\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
    "        # 3\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # 4\n",
    "        for _ in range(max_length):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        return all_tokens, all_scores\n",
    "\n",
    "# 5\n",
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_lenth=max_length):\n",
    "    indices = [indexFromSentence(voc, sentence)]\n",
    "    lenghts = torch.tensor([len(indexes) for indexes in indices])\n",
    "    input_batch = torch.LongTensor(indices).transpose(0, 1)\n",
    "    # 6\n",
    "    input_batch = input_batch.to(device)\n",
    "    # TODO: Error after setting the variable to gpu\n",
    "    lengths = lenghts.to(device).cpu()\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "# 7\n",
    "def runChatbot(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            input_sentence = input('> ')\n",
    "            if input_sentence == 'quit': break\n",
    "            # 8\n",
    "            input_sentence = cleanStrings(input_sentence)\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # 9\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Response:', ' '.join(output_words))\n",
    "\n",
    "        except:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have defined all the necessary functions, training the model just becomes a case or initializing our hyperparameters and calling our training functions:\n",
    "\n",
    "1. We first **initialize our hyperparameters**. While **these are only suggested hyperparameters, our models have been set up in a way that will allow them to adapt to whatever hyperparameters they are passed**. It is good practice to **experiment with different hyperparameters to see which ones result in an optimal model configuration**. Here, **you could experiment with increasing the number of layers in your encoder and decoder**, increasing or decreasing the size of the hidden layers, or increasing the batch size. All of these hyperparameters will have an effect on how well your model learns, as well as a number of other factors, such as the time it takes to train the model:\n",
    "\n",
    "```python\n",
    "model_name = 'chatbot_model'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.15\n",
    "batch_size = 64\n",
    "```\n",
    "\n",
    "2. After that, we can **load our checkpoints**. If we have previously trained a model, we can load the checkpoints and model states from previous iterations. This saves us from having to retrain our model each time:\n",
    "\n",
    "```python\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "\n",
    "if loadFilename:\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "```\n",
    "\n",
    "3. After that, we can begin to build our models. We **first load our embeddings from the vocabulary**. **If we have already trained a model, we can load the trained embeddings layer**:\n",
    "\n",
    "```python\n",
    "    embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "    if loadFilename:\n",
    "        embedding.load_state_dict(embedding_sd)\n",
    "```\n",
    "\n",
    "4. We then do the same for our encoder and decoder, creating model instances using the defined hyperparameters. Again, if we have already trained a model, we simply load the trained model states into our models:\n",
    "\n",
    "```python\n",
    "        encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "        decoder = DecoderRNN(embedding, hidden_size, voc.num_words, decoder_n_layers,dropout)\n",
    "\n",
    "    if loadFilename:\n",
    "        encoder.load_state_dict(encoder_sd)\n",
    "        decoder.load_state_dict(decoder_sd)\n",
    "```\n",
    "\n",
    "5. Last but not least, we specify a device for each of our models to be trained on. Remember, this is a crucial step if you wish to use GPU training:\n",
    "\n",
    "```python\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')\n",
    "```\n",
    "\n",
    "If this has all worked correctly and your models have been created with no errors, you should see the following:\n",
    "\n",
    "![](model_built_msg.png)\n",
    "\n",
    "Now that we have created instances of both our encoder and decoders, we are ready to begin training them.\n",
    "\n",
    "We start by initializing some training hyperparameters. In the same way as our model hyperparameters, these can be adjusted to influence training time and how our model learns. Clip controls the gradient clipping and teacher forcing controls how often we use teacher forcing within our model. Notice how we use a teacher forcing ratio of 1 so that we always use teacher forcing. Lowering the teaching forcing ratio would mean our model takes much longer to converge; however, it might help our model generate correct sentences by itself better in the long run.\n",
    "\n",
    "6. We also need to define the learning rates of our models and our decoder learning ratio. You will find that your model performs better when the decoder carries out larger parameter updates during gradient descent. Therefore, we introduce a decoder learning ratio to apply a multiplier to the learning rate so that the learning rate is greater for the decoder than it is for the encoder. We also define how often our model prints and saves the results, as well as how many epochs we want our model to run for:\n",
    "\n",
    "```python\n",
    "save_dir = '../models/chatbot'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "epochs = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "```\n",
    "\n",
    "7. Next, as always when training models in PyTorch, we switch our models to training mode to allow the parameters to be updated:\n",
    "\n",
    "```python\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "```\n",
    "\n",
    "8. Next, we create optimizers for both the encoder and decoder. We initialize these as Adam optimizers, but other optimizers will work equally well. Experimenting with different optimizers may yield different levels of model performance. If you have trained a model previously, you can also load the optimizer states if required:\n",
    "\n",
    "```python\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(),lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "```\n",
    "\n",
    "9. The final step before running the training is to make sure CUDA is configured to be called if you wish to use GPU training. To do this, we simply loop through the optimizer states for both the encoder and decoder and enable CUDA across all of the states:\n",
    "\n",
    "```python\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "            for state in decoder_optimizer.state.values():\n",
    "\n",
    "    for k, v in state.items():\n",
    "\n",
    "        if isinstance(v, torch.Tensor):\n",
    "\n",
    "            state[k] = v.cuda()\n",
    "```\n",
    "10. Finally, we are ready to train our model. This can be done by simply calling the trainIters function with all the required parameters:\n",
    "\n",
    "```python\n",
    "print(\"Starting Training!\")\n",
    "\n",
    "trainIters(model_name, voc, pairs, encoder, decoder,\\\n",
    "           encoder_optimizer, decoder_optimizer, \\\n",
    "           embedding, encoder_n_layers, \\\n",
    "           decoder_n_layers, save_dir, epochs, \\\n",
    "            batch_size,print_every, save_every, \\\n",
    "            clip, corpus_name, loadFilename)\n",
    "```\n",
    "\n",
    "![](result_1.png)\n",
    "\n",
    "Your model is now training! Depending on a number of factors, such as how many epochs you have set your model to train for and whether you are using a GPU, your model may take some time to train. When it is complete, you will see the following output. If everything has worked correctly, your model's average loss will be significantly lower than when you started training, showing that your model has learned something useful:\n",
    "\n",
    "![Alt text](loss_4000_iter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Starting ...\n",
      "Beginning Training...\n",
      "Iteration: 1; Percent done: 0.0%;                    Mean loss: 9.1809\n",
      "Iteration: 2; Percent done: 0.1%;                    Mean loss: 9.0635\n",
      "Iteration: 3; Percent done: 0.1%;                    Mean loss: 8.9012\n",
      "Iteration: 4; Percent done: 0.1%;                    Mean loss: 8.6159\n",
      "Iteration: 5; Percent done: 0.1%;                    Mean loss: 8.2450\n",
      "Iteration: 6; Percent done: 0.1%;                    Mean loss: 7.7016\n",
      "Iteration: 7; Percent done: 0.2%;                    Mean loss: 6.9618\n",
      "Iteration: 8; Percent done: 0.2%;                    Mean loss: 6.7614\n",
      "Iteration: 9; Percent done: 0.2%;                    Mean loss: 6.6377\n",
      "Iteration: 10; Percent done: 0.2%;                    Mean loss: 6.4973\n",
      "Iteration: 11; Percent done: 0.3%;                    Mean loss: 5.8887\n",
      "Iteration: 12; Percent done: 0.3%;                    Mean loss: 6.0030\n",
      "Iteration: 13; Percent done: 0.3%;                    Mean loss: 5.7664\n",
      "Iteration: 14; Percent done: 0.4%;                    Mean loss: 5.9229\n",
      "Iteration: 15; Percent done: 0.4%;                    Mean loss: 5.5088\n",
      "Iteration: 16; Percent done: 0.4%;                    Mean loss: 5.6614\n",
      "Iteration: 17; Percent done: 0.4%;                    Mean loss: 5.7879\n",
      "Iteration: 18; Percent done: 0.4%;                    Mean loss: 5.2983\n",
      "Iteration: 19; Percent done: 0.5%;                    Mean loss: 5.3647\n",
      "Iteration: 20; Percent done: 0.5%;                    Mean loss: 5.3836\n",
      "Iteration: 21; Percent done: 0.5%;                    Mean loss: 5.2133\n",
      "Iteration: 22; Percent done: 0.5%;                    Mean loss: 5.1360\n",
      "Iteration: 23; Percent done: 0.6%;                    Mean loss: 5.3939\n",
      "Iteration: 24; Percent done: 0.6%;                    Mean loss: 5.2082\n",
      "Iteration: 25; Percent done: 0.6%;                    Mean loss: 5.1736\n",
      "Iteration: 26; Percent done: 0.7%;                    Mean loss: 4.8801\n",
      "Iteration: 27; Percent done: 0.7%;                    Mean loss: 4.9761\n",
      "Iteration: 28; Percent done: 0.7%;                    Mean loss: 5.1070\n",
      "Iteration: 29; Percent done: 0.7%;                    Mean loss: 4.8278\n",
      "Iteration: 30; Percent done: 0.8%;                    Mean loss: 5.1569\n",
      "Iteration: 31; Percent done: 0.8%;                    Mean loss: 4.8937\n",
      "Iteration: 32; Percent done: 0.8%;                    Mean loss: 4.9173\n",
      "Iteration: 33; Percent done: 0.8%;                    Mean loss: 4.8958\n",
      "Iteration: 34; Percent done: 0.9%;                    Mean loss: 4.7921\n",
      "Iteration: 35; Percent done: 0.9%;                    Mean loss: 4.8906\n",
      "Iteration: 36; Percent done: 0.9%;                    Mean loss: 5.1566\n",
      "Iteration: 37; Percent done: 0.9%;                    Mean loss: 4.7058\n",
      "Iteration: 38; Percent done: 0.9%;                    Mean loss: 4.9131\n",
      "Iteration: 39; Percent done: 1.0%;                    Mean loss: 5.0581\n",
      "Iteration: 40; Percent done: 1.0%;                    Mean loss: 4.9962\n",
      "Iteration: 41; Percent done: 1.0%;                    Mean loss: 5.1767\n",
      "Iteration: 42; Percent done: 1.1%;                    Mean loss: 4.8643\n",
      "Iteration: 43; Percent done: 1.1%;                    Mean loss: 4.9817\n",
      "Iteration: 44; Percent done: 1.1%;                    Mean loss: 4.9982\n",
      "Iteration: 45; Percent done: 1.1%;                    Mean loss: 4.7257\n",
      "Iteration: 46; Percent done: 1.1%;                    Mean loss: 4.8573\n",
      "Iteration: 47; Percent done: 1.2%;                    Mean loss: 4.7260\n",
      "Iteration: 48; Percent done: 1.2%;                    Mean loss: 4.7346\n",
      "Iteration: 49; Percent done: 1.2%;                    Mean loss: 4.8603\n",
      "Iteration: 50; Percent done: 1.2%;                    Mean loss: 4.7606\n",
      "Iteration: 51; Percent done: 1.3%;                    Mean loss: 4.8176\n",
      "Iteration: 52; Percent done: 1.3%;                    Mean loss: 4.8580\n",
      "Iteration: 53; Percent done: 1.3%;                    Mean loss: 4.8078\n",
      "Iteration: 54; Percent done: 1.4%;                    Mean loss: 4.6239\n",
      "Iteration: 55; Percent done: 1.4%;                    Mean loss: 4.7582\n",
      "Iteration: 56; Percent done: 1.4%;                    Mean loss: 4.8619\n",
      "Iteration: 57; Percent done: 1.4%;                    Mean loss: 4.6182\n",
      "Iteration: 58; Percent done: 1.5%;                    Mean loss: 4.7981\n",
      "Iteration: 59; Percent done: 1.5%;                    Mean loss: 4.9357\n",
      "Iteration: 60; Percent done: 1.5%;                    Mean loss: 5.2250\n",
      "Iteration: 61; Percent done: 1.5%;                    Mean loss: 4.6891\n",
      "Iteration: 62; Percent done: 1.6%;                    Mean loss: 4.7201\n",
      "Iteration: 63; Percent done: 1.6%;                    Mean loss: 4.7932\n",
      "Iteration: 64; Percent done: 1.6%;                    Mean loss: 4.5663\n",
      "Iteration: 65; Percent done: 1.6%;                    Mean loss: 4.5521\n",
      "Iteration: 66; Percent done: 1.7%;                    Mean loss: 4.7349\n",
      "Iteration: 67; Percent done: 1.7%;                    Mean loss: 4.8209\n",
      "Iteration: 68; Percent done: 1.7%;                    Mean loss: 4.7606\n",
      "Iteration: 69; Percent done: 1.7%;                    Mean loss: 4.7181\n",
      "Iteration: 70; Percent done: 1.8%;                    Mean loss: 4.6900\n",
      "Iteration: 71; Percent done: 1.8%;                    Mean loss: 4.5376\n",
      "Iteration: 72; Percent done: 1.8%;                    Mean loss: 4.9452\n",
      "Iteration: 73; Percent done: 1.8%;                    Mean loss: 4.5052\n",
      "Iteration: 74; Percent done: 1.8%;                    Mean loss: 4.7697\n",
      "Iteration: 75; Percent done: 1.9%;                    Mean loss: 4.5670\n",
      "Iteration: 76; Percent done: 1.9%;                    Mean loss: 4.6757\n",
      "Iteration: 77; Percent done: 1.9%;                    Mean loss: 4.6181\n",
      "Iteration: 78; Percent done: 1.9%;                    Mean loss: 4.7236\n",
      "Iteration: 79; Percent done: 2.0%;                    Mean loss: 4.4850\n",
      "Iteration: 80; Percent done: 2.0%;                    Mean loss: 4.3877\n",
      "Iteration: 81; Percent done: 2.0%;                    Mean loss: 4.6427\n",
      "Iteration: 82; Percent done: 2.1%;                    Mean loss: 4.6400\n",
      "Iteration: 83; Percent done: 2.1%;                    Mean loss: 4.6788\n",
      "Iteration: 84; Percent done: 2.1%;                    Mean loss: 4.6054\n",
      "Iteration: 85; Percent done: 2.1%;                    Mean loss: 4.6471\n",
      "Iteration: 86; Percent done: 2.1%;                    Mean loss: 4.8261\n",
      "Iteration: 87; Percent done: 2.2%;                    Mean loss: 4.5106\n",
      "Iteration: 88; Percent done: 2.2%;                    Mean loss: 4.4680\n",
      "Iteration: 89; Percent done: 2.2%;                    Mean loss: 4.6818\n",
      "Iteration: 90; Percent done: 2.2%;                    Mean loss: 4.6042\n",
      "Iteration: 91; Percent done: 2.3%;                    Mean loss: 4.4796\n",
      "Iteration: 92; Percent done: 2.3%;                    Mean loss: 4.5317\n",
      "Iteration: 93; Percent done: 2.3%;                    Mean loss: 4.6668\n",
      "Iteration: 94; Percent done: 2.4%;                    Mean loss: 4.7081\n",
      "Iteration: 95; Percent done: 2.4%;                    Mean loss: 4.5888\n",
      "Iteration: 96; Percent done: 2.4%;                    Mean loss: 4.6162\n",
      "Iteration: 97; Percent done: 2.4%;                    Mean loss: 4.5767\n",
      "Iteration: 98; Percent done: 2.5%;                    Mean loss: 4.6973\n",
      "Iteration: 99; Percent done: 2.5%;                    Mean loss: 4.4271\n",
      "Iteration: 100; Percent done: 2.5%;                    Mean loss: 4.4716\n",
      "Iteration: 101; Percent done: 2.5%;                    Mean loss: 4.2997\n",
      "Iteration: 102; Percent done: 2.5%;                    Mean loss: 4.6991\n",
      "Iteration: 103; Percent done: 2.6%;                    Mean loss: 4.9265\n",
      "Iteration: 104; Percent done: 2.6%;                    Mean loss: 4.4411\n",
      "Iteration: 105; Percent done: 2.6%;                    Mean loss: 4.7259\n",
      "Iteration: 106; Percent done: 2.6%;                    Mean loss: 4.6856\n",
      "Iteration: 107; Percent done: 2.7%;                    Mean loss: 4.5283\n",
      "Iteration: 108; Percent done: 2.7%;                    Mean loss: 4.4157\n",
      "Iteration: 109; Percent done: 2.7%;                    Mean loss: 4.4718\n",
      "Iteration: 110; Percent done: 2.8%;                    Mean loss: 4.5911\n",
      "Iteration: 111; Percent done: 2.8%;                    Mean loss: 4.6944\n",
      "Iteration: 112; Percent done: 2.8%;                    Mean loss: 4.6134\n",
      "Iteration: 113; Percent done: 2.8%;                    Mean loss: 4.4455\n",
      "Iteration: 114; Percent done: 2.9%;                    Mean loss: 4.6563\n",
      "Iteration: 115; Percent done: 2.9%;                    Mean loss: 4.3927\n",
      "Iteration: 116; Percent done: 2.9%;                    Mean loss: 4.5895\n",
      "Iteration: 117; Percent done: 2.9%;                    Mean loss: 4.5033\n",
      "Iteration: 118; Percent done: 2.9%;                    Mean loss: 4.6474\n",
      "Iteration: 119; Percent done: 3.0%;                    Mean loss: 4.4907\n",
      "Iteration: 120; Percent done: 3.0%;                    Mean loss: 4.6362\n",
      "Iteration: 121; Percent done: 3.0%;                    Mean loss: 4.6004\n",
      "Iteration: 122; Percent done: 3.0%;                    Mean loss: 4.4606\n",
      "Iteration: 123; Percent done: 3.1%;                    Mean loss: 4.4648\n",
      "Iteration: 124; Percent done: 3.1%;                    Mean loss: 4.4858\n",
      "Iteration: 125; Percent done: 3.1%;                    Mean loss: 4.6729\n",
      "Iteration: 126; Percent done: 3.1%;                    Mean loss: 4.5949\n",
      "Iteration: 127; Percent done: 3.2%;                    Mean loss: 4.1977\n",
      "Iteration: 128; Percent done: 3.2%;                    Mean loss: 4.7883\n",
      "Iteration: 129; Percent done: 3.2%;                    Mean loss: 4.7773\n",
      "Iteration: 130; Percent done: 3.2%;                    Mean loss: 4.2537\n",
      "Iteration: 131; Percent done: 3.3%;                    Mean loss: 4.3525\n",
      "Iteration: 132; Percent done: 3.3%;                    Mean loss: 4.7207\n",
      "Iteration: 133; Percent done: 3.3%;                    Mean loss: 4.5457\n",
      "Iteration: 134; Percent done: 3.4%;                    Mean loss: 4.3689\n",
      "Iteration: 135; Percent done: 3.4%;                    Mean loss: 4.4721\n",
      "Iteration: 136; Percent done: 3.4%;                    Mean loss: 4.4652\n",
      "Iteration: 137; Percent done: 3.4%;                    Mean loss: 4.5149\n",
      "Iteration: 138; Percent done: 3.5%;                    Mean loss: 4.7923\n",
      "Iteration: 139; Percent done: 3.5%;                    Mean loss: 4.3445\n",
      "Iteration: 140; Percent done: 3.5%;                    Mean loss: 4.4980\n",
      "Iteration: 141; Percent done: 3.5%;                    Mean loss: 4.4475\n",
      "Iteration: 142; Percent done: 3.5%;                    Mean loss: 4.5925\n",
      "Iteration: 143; Percent done: 3.6%;                    Mean loss: 4.4338\n",
      "Iteration: 144; Percent done: 3.6%;                    Mean loss: 4.7299\n",
      "Iteration: 145; Percent done: 3.6%;                    Mean loss: 4.4711\n",
      "Iteration: 146; Percent done: 3.6%;                    Mean loss: 4.6787\n",
      "Iteration: 147; Percent done: 3.7%;                    Mean loss: 4.3607\n",
      "Iteration: 148; Percent done: 3.7%;                    Mean loss: 4.4529\n",
      "Iteration: 149; Percent done: 3.7%;                    Mean loss: 4.5433\n",
      "Iteration: 150; Percent done: 3.8%;                    Mean loss: 4.5496\n",
      "Iteration: 151; Percent done: 3.8%;                    Mean loss: 4.5507\n",
      "Iteration: 152; Percent done: 3.8%;                    Mean loss: 4.7118\n",
      "Iteration: 153; Percent done: 3.8%;                    Mean loss: 4.4911\n",
      "Iteration: 154; Percent done: 3.9%;                    Mean loss: 4.4881\n",
      "Iteration: 155; Percent done: 3.9%;                    Mean loss: 4.3791\n",
      "Iteration: 156; Percent done: 3.9%;                    Mean loss: 4.5155\n",
      "Iteration: 157; Percent done: 3.9%;                    Mean loss: 4.4435\n",
      "Iteration: 158; Percent done: 4.0%;                    Mean loss: 4.3565\n",
      "Iteration: 159; Percent done: 4.0%;                    Mean loss: 4.5114\n",
      "Iteration: 160; Percent done: 4.0%;                    Mean loss: 4.8001\n",
      "Iteration: 161; Percent done: 4.0%;                    Mean loss: 4.6111\n",
      "Iteration: 162; Percent done: 4.0%;                    Mean loss: 4.3228\n",
      "Iteration: 163; Percent done: 4.1%;                    Mean loss: 4.3774\n",
      "Iteration: 164; Percent done: 4.1%;                    Mean loss: 4.2219\n",
      "Iteration: 165; Percent done: 4.1%;                    Mean loss: 4.4345\n",
      "Iteration: 166; Percent done: 4.2%;                    Mean loss: 4.4731\n",
      "Iteration: 167; Percent done: 4.2%;                    Mean loss: 4.5371\n",
      "Iteration: 168; Percent done: 4.2%;                    Mean loss: 4.4995\n",
      "Iteration: 169; Percent done: 4.2%;                    Mean loss: 4.2078\n",
      "Iteration: 170; Percent done: 4.2%;                    Mean loss: 4.3857\n",
      "Iteration: 171; Percent done: 4.3%;                    Mean loss: 4.4090\n",
      "Iteration: 172; Percent done: 4.3%;                    Mean loss: 4.4287\n",
      "Iteration: 173; Percent done: 4.3%;                    Mean loss: 4.4775\n",
      "Iteration: 174; Percent done: 4.3%;                    Mean loss: 4.5053\n",
      "Iteration: 175; Percent done: 4.4%;                    Mean loss: 4.2217\n",
      "Iteration: 176; Percent done: 4.4%;                    Mean loss: 4.3496\n",
      "Iteration: 177; Percent done: 4.4%;                    Mean loss: 4.1519\n",
      "Iteration: 178; Percent done: 4.5%;                    Mean loss: 4.5356\n",
      "Iteration: 179; Percent done: 4.5%;                    Mean loss: 4.0994\n",
      "Iteration: 180; Percent done: 4.5%;                    Mean loss: 4.4271\n",
      "Iteration: 181; Percent done: 4.5%;                    Mean loss: 4.0865\n",
      "Iteration: 182; Percent done: 4.5%;                    Mean loss: 4.3457\n",
      "Iteration: 183; Percent done: 4.6%;                    Mean loss: 4.3194\n",
      "Iteration: 184; Percent done: 4.6%;                    Mean loss: 4.1898\n",
      "Iteration: 185; Percent done: 4.6%;                    Mean loss: 4.4411\n",
      "Iteration: 186; Percent done: 4.7%;                    Mean loss: 4.4024\n",
      "Iteration: 187; Percent done: 4.7%;                    Mean loss: 4.1772\n",
      "Iteration: 188; Percent done: 4.7%;                    Mean loss: 4.3272\n",
      "Iteration: 189; Percent done: 4.7%;                    Mean loss: 4.1649\n",
      "Iteration: 190; Percent done: 4.8%;                    Mean loss: 4.1609\n",
      "Iteration: 191; Percent done: 4.8%;                    Mean loss: 4.5223\n",
      "Iteration: 192; Percent done: 4.8%;                    Mean loss: 4.4170\n",
      "Iteration: 193; Percent done: 4.8%;                    Mean loss: 4.5185\n",
      "Iteration: 194; Percent done: 4.9%;                    Mean loss: 4.2551\n",
      "Iteration: 195; Percent done: 4.9%;                    Mean loss: 4.4375\n",
      "Iteration: 196; Percent done: 4.9%;                    Mean loss: 4.3001\n",
      "Iteration: 197; Percent done: 4.9%;                    Mean loss: 4.2110\n",
      "Iteration: 198; Percent done: 5.0%;                    Mean loss: 4.5197\n",
      "Iteration: 199; Percent done: 5.0%;                    Mean loss: 4.6842\n",
      "Iteration: 200; Percent done: 5.0%;                    Mean loss: 4.4196\n",
      "Iteration: 201; Percent done: 5.0%;                    Mean loss: 4.4989\n",
      "Iteration: 202; Percent done: 5.1%;                    Mean loss: 4.4490\n",
      "Iteration: 203; Percent done: 5.1%;                    Mean loss: 4.5262\n",
      "Iteration: 204; Percent done: 5.1%;                    Mean loss: 4.1064\n",
      "Iteration: 205; Percent done: 5.1%;                    Mean loss: 4.4413\n",
      "Iteration: 206; Percent done: 5.1%;                    Mean loss: 4.3658\n",
      "Iteration: 207; Percent done: 5.2%;                    Mean loss: 4.1666\n",
      "Iteration: 208; Percent done: 5.2%;                    Mean loss: 4.0446\n",
      "Iteration: 209; Percent done: 5.2%;                    Mean loss: 4.4471\n",
      "Iteration: 210; Percent done: 5.2%;                    Mean loss: 4.3900\n",
      "Iteration: 211; Percent done: 5.3%;                    Mean loss: 4.3581\n",
      "Iteration: 212; Percent done: 5.3%;                    Mean loss: 4.2013\n",
      "Iteration: 213; Percent done: 5.3%;                    Mean loss: 4.1013\n",
      "Iteration: 214; Percent done: 5.3%;                    Mean loss: 4.4605\n",
      "Iteration: 215; Percent done: 5.4%;                    Mean loss: 4.5263\n",
      "Iteration: 216; Percent done: 5.4%;                    Mean loss: 4.1025\n",
      "Iteration: 217; Percent done: 5.4%;                    Mean loss: 4.3519\n",
      "Iteration: 218; Percent done: 5.5%;                    Mean loss: 4.1456\n",
      "Iteration: 219; Percent done: 5.5%;                    Mean loss: 4.1307\n",
      "Iteration: 220; Percent done: 5.5%;                    Mean loss: 4.1351\n",
      "Iteration: 221; Percent done: 5.5%;                    Mean loss: 4.2827\n",
      "Iteration: 222; Percent done: 5.5%;                    Mean loss: 4.3009\n",
      "Iteration: 223; Percent done: 5.6%;                    Mean loss: 4.3869\n",
      "Iteration: 224; Percent done: 5.6%;                    Mean loss: 4.5758\n",
      "Iteration: 225; Percent done: 5.6%;                    Mean loss: 4.2223\n",
      "Iteration: 226; Percent done: 5.7%;                    Mean loss: 4.2540\n",
      "Iteration: 227; Percent done: 5.7%;                    Mean loss: 4.4129\n",
      "Iteration: 228; Percent done: 5.7%;                    Mean loss: 4.4959\n",
      "Iteration: 229; Percent done: 5.7%;                    Mean loss: 4.3806\n",
      "Iteration: 230; Percent done: 5.8%;                    Mean loss: 4.1753\n",
      "Iteration: 231; Percent done: 5.8%;                    Mean loss: 3.9587\n",
      "Iteration: 232; Percent done: 5.8%;                    Mean loss: 4.3631\n",
      "Iteration: 233; Percent done: 5.8%;                    Mean loss: 4.4111\n",
      "Iteration: 234; Percent done: 5.9%;                    Mean loss: 4.4164\n",
      "Iteration: 235; Percent done: 5.9%;                    Mean loss: 4.1750\n",
      "Iteration: 236; Percent done: 5.9%;                    Mean loss: 4.2452\n",
      "Iteration: 237; Percent done: 5.9%;                    Mean loss: 4.5526\n",
      "Iteration: 238; Percent done: 5.9%;                    Mean loss: 4.1388\n",
      "Iteration: 239; Percent done: 6.0%;                    Mean loss: 4.3337\n",
      "Iteration: 240; Percent done: 6.0%;                    Mean loss: 4.1147\n",
      "Iteration: 241; Percent done: 6.0%;                    Mean loss: 4.3181\n",
      "Iteration: 242; Percent done: 6.0%;                    Mean loss: 4.3728\n",
      "Iteration: 243; Percent done: 6.1%;                    Mean loss: 4.3353\n",
      "Iteration: 244; Percent done: 6.1%;                    Mean loss: 4.2628\n",
      "Iteration: 245; Percent done: 6.1%;                    Mean loss: 4.3255\n",
      "Iteration: 246; Percent done: 6.2%;                    Mean loss: 4.1990\n",
      "Iteration: 247; Percent done: 6.2%;                    Mean loss: 4.2003\n",
      "Iteration: 248; Percent done: 6.2%;                    Mean loss: 4.2312\n",
      "Iteration: 249; Percent done: 6.2%;                    Mean loss: 4.0307\n",
      "Iteration: 250; Percent done: 6.2%;                    Mean loss: 4.4370\n",
      "Iteration: 251; Percent done: 6.3%;                    Mean loss: 3.9693\n",
      "Iteration: 252; Percent done: 6.3%;                    Mean loss: 4.4015\n",
      "Iteration: 253; Percent done: 6.3%;                    Mean loss: 4.3166\n",
      "Iteration: 254; Percent done: 6.3%;                    Mean loss: 4.0815\n",
      "Iteration: 255; Percent done: 6.4%;                    Mean loss: 4.3983\n",
      "Iteration: 256; Percent done: 6.4%;                    Mean loss: 4.2354\n",
      "Iteration: 257; Percent done: 6.4%;                    Mean loss: 4.2432\n",
      "Iteration: 258; Percent done: 6.5%;                    Mean loss: 4.1494\n",
      "Iteration: 259; Percent done: 6.5%;                    Mean loss: 4.3821\n",
      "Iteration: 260; Percent done: 6.5%;                    Mean loss: 4.4065\n",
      "Iteration: 261; Percent done: 6.5%;                    Mean loss: 4.0543\n",
      "Iteration: 262; Percent done: 6.6%;                    Mean loss: 4.3847\n",
      "Iteration: 263; Percent done: 6.6%;                    Mean loss: 4.2206\n",
      "Iteration: 264; Percent done: 6.6%;                    Mean loss: 4.1630\n",
      "Iteration: 265; Percent done: 6.6%;                    Mean loss: 4.3898\n",
      "Iteration: 266; Percent done: 6.7%;                    Mean loss: 4.2211\n",
      "Iteration: 267; Percent done: 6.7%;                    Mean loss: 4.0666\n",
      "Iteration: 268; Percent done: 6.7%;                    Mean loss: 4.2165\n",
      "Iteration: 269; Percent done: 6.7%;                    Mean loss: 4.4773\n",
      "Iteration: 270; Percent done: 6.8%;                    Mean loss: 4.1679\n",
      "Iteration: 271; Percent done: 6.8%;                    Mean loss: 4.3552\n",
      "Iteration: 272; Percent done: 6.8%;                    Mean loss: 4.1066\n",
      "Iteration: 273; Percent done: 6.8%;                    Mean loss: 4.0813\n",
      "Iteration: 274; Percent done: 6.9%;                    Mean loss: 4.3977\n",
      "Iteration: 275; Percent done: 6.9%;                    Mean loss: 4.3334\n",
      "Iteration: 276; Percent done: 6.9%;                    Mean loss: 4.4482\n",
      "Iteration: 277; Percent done: 6.9%;                    Mean loss: 4.1666\n",
      "Iteration: 278; Percent done: 7.0%;                    Mean loss: 4.4648\n",
      "Iteration: 279; Percent done: 7.0%;                    Mean loss: 4.3458\n",
      "Iteration: 280; Percent done: 7.0%;                    Mean loss: 4.0975\n",
      "Iteration: 281; Percent done: 7.0%;                    Mean loss: 4.1344\n",
      "Iteration: 282; Percent done: 7.0%;                    Mean loss: 4.0890\n",
      "Iteration: 283; Percent done: 7.1%;                    Mean loss: 4.2972\n",
      "Iteration: 284; Percent done: 7.1%;                    Mean loss: 4.2924\n",
      "Iteration: 285; Percent done: 7.1%;                    Mean loss: 4.4214\n",
      "Iteration: 286; Percent done: 7.1%;                    Mean loss: 4.1541\n",
      "Iteration: 287; Percent done: 7.2%;                    Mean loss: 4.3016\n",
      "Iteration: 288; Percent done: 7.2%;                    Mean loss: 4.3442\n",
      "Iteration: 289; Percent done: 7.2%;                    Mean loss: 4.0030\n",
      "Iteration: 290; Percent done: 7.2%;                    Mean loss: 4.0375\n",
      "Iteration: 291; Percent done: 7.3%;                    Mean loss: 4.5121\n",
      "Iteration: 292; Percent done: 7.3%;                    Mean loss: 4.0529\n",
      "Iteration: 293; Percent done: 7.3%;                    Mean loss: 4.1378\n",
      "Iteration: 294; Percent done: 7.3%;                    Mean loss: 4.2757\n",
      "Iteration: 295; Percent done: 7.4%;                    Mean loss: 4.2301\n",
      "Iteration: 296; Percent done: 7.4%;                    Mean loss: 4.1025\n",
      "Iteration: 297; Percent done: 7.4%;                    Mean loss: 4.3961\n",
      "Iteration: 298; Percent done: 7.4%;                    Mean loss: 3.8284\n",
      "Iteration: 299; Percent done: 7.5%;                    Mean loss: 4.2019\n",
      "Iteration: 300; Percent done: 7.5%;                    Mean loss: 4.0486\n",
      "Iteration: 301; Percent done: 7.5%;                    Mean loss: 4.1077\n",
      "Iteration: 302; Percent done: 7.5%;                    Mean loss: 4.2633\n",
      "Iteration: 303; Percent done: 7.6%;                    Mean loss: 4.2592\n",
      "Iteration: 304; Percent done: 7.6%;                    Mean loss: 4.2271\n",
      "Iteration: 305; Percent done: 7.6%;                    Mean loss: 4.2546\n",
      "Iteration: 306; Percent done: 7.6%;                    Mean loss: 3.9180\n",
      "Iteration: 307; Percent done: 7.7%;                    Mean loss: 4.3645\n",
      "Iteration: 308; Percent done: 7.7%;                    Mean loss: 4.3312\n",
      "Iteration: 309; Percent done: 7.7%;                    Mean loss: 4.3152\n",
      "Iteration: 310; Percent done: 7.8%;                    Mean loss: 4.0264\n",
      "Iteration: 311; Percent done: 7.8%;                    Mean loss: 4.1925\n",
      "Iteration: 312; Percent done: 7.8%;                    Mean loss: 4.1664\n",
      "Iteration: 313; Percent done: 7.8%;                    Mean loss: 3.9709\n",
      "Iteration: 314; Percent done: 7.8%;                    Mean loss: 4.2508\n",
      "Iteration: 315; Percent done: 7.9%;                    Mean loss: 4.3031\n",
      "Iteration: 316; Percent done: 7.9%;                    Mean loss: 4.3341\n",
      "Iteration: 317; Percent done: 7.9%;                    Mean loss: 3.9471\n",
      "Iteration: 318; Percent done: 8.0%;                    Mean loss: 4.0116\n",
      "Iteration: 319; Percent done: 8.0%;                    Mean loss: 4.1726\n",
      "Iteration: 320; Percent done: 8.0%;                    Mean loss: 4.0964\n",
      "Iteration: 321; Percent done: 8.0%;                    Mean loss: 4.1630\n",
      "Iteration: 322; Percent done: 8.1%;                    Mean loss: 4.0880\n",
      "Iteration: 323; Percent done: 8.1%;                    Mean loss: 4.2949\n",
      "Iteration: 324; Percent done: 8.1%;                    Mean loss: 4.1591\n",
      "Iteration: 325; Percent done: 8.1%;                    Mean loss: 4.1694\n",
      "Iteration: 326; Percent done: 8.2%;                    Mean loss: 4.3156\n",
      "Iteration: 327; Percent done: 8.2%;                    Mean loss: 4.0482\n",
      "Iteration: 328; Percent done: 8.2%;                    Mean loss: 4.2129\n",
      "Iteration: 329; Percent done: 8.2%;                    Mean loss: 4.3549\n",
      "Iteration: 330; Percent done: 8.2%;                    Mean loss: 4.0317\n",
      "Iteration: 331; Percent done: 8.3%;                    Mean loss: 4.0268\n",
      "Iteration: 332; Percent done: 8.3%;                    Mean loss: 4.0903\n",
      "Iteration: 333; Percent done: 8.3%;                    Mean loss: 4.1672\n",
      "Iteration: 334; Percent done: 8.3%;                    Mean loss: 4.1714\n",
      "Iteration: 335; Percent done: 8.4%;                    Mean loss: 4.0732\n",
      "Iteration: 336; Percent done: 8.4%;                    Mean loss: 4.3031\n",
      "Iteration: 337; Percent done: 8.4%;                    Mean loss: 4.1041\n",
      "Iteration: 338; Percent done: 8.5%;                    Mean loss: 4.1956\n",
      "Iteration: 339; Percent done: 8.5%;                    Mean loss: 4.3830\n",
      "Iteration: 340; Percent done: 8.5%;                    Mean loss: 4.2297\n",
      "Iteration: 341; Percent done: 8.5%;                    Mean loss: 4.1089\n",
      "Iteration: 342; Percent done: 8.6%;                    Mean loss: 4.2310\n",
      "Iteration: 343; Percent done: 8.6%;                    Mean loss: 4.0769\n",
      "Iteration: 344; Percent done: 8.6%;                    Mean loss: 4.1342\n",
      "Iteration: 345; Percent done: 8.6%;                    Mean loss: 4.1824\n",
      "Iteration: 346; Percent done: 8.6%;                    Mean loss: 4.0057\n",
      "Iteration: 347; Percent done: 8.7%;                    Mean loss: 4.2476\n",
      "Iteration: 348; Percent done: 8.7%;                    Mean loss: 4.1551\n",
      "Iteration: 349; Percent done: 8.7%;                    Mean loss: 4.4364\n",
      "Iteration: 350; Percent done: 8.8%;                    Mean loss: 4.1493\n",
      "Iteration: 351; Percent done: 8.8%;                    Mean loss: 4.1159\n",
      "Iteration: 352; Percent done: 8.8%;                    Mean loss: 4.3397\n",
      "Iteration: 353; Percent done: 8.8%;                    Mean loss: 4.1573\n",
      "Iteration: 354; Percent done: 8.8%;                    Mean loss: 4.1467\n",
      "Iteration: 355; Percent done: 8.9%;                    Mean loss: 4.0587\n",
      "Iteration: 356; Percent done: 8.9%;                    Mean loss: 4.1356\n",
      "Iteration: 357; Percent done: 8.9%;                    Mean loss: 4.3581\n",
      "Iteration: 358; Percent done: 8.9%;                    Mean loss: 4.0216\n",
      "Iteration: 359; Percent done: 9.0%;                    Mean loss: 3.7604\n",
      "Iteration: 360; Percent done: 9.0%;                    Mean loss: 4.0501\n",
      "Iteration: 361; Percent done: 9.0%;                    Mean loss: 4.1176\n",
      "Iteration: 362; Percent done: 9.0%;                    Mean loss: 4.0929\n",
      "Iteration: 363; Percent done: 9.1%;                    Mean loss: 3.9380\n",
      "Iteration: 364; Percent done: 9.1%;                    Mean loss: 4.0483\n",
      "Iteration: 365; Percent done: 9.1%;                    Mean loss: 3.9616\n",
      "Iteration: 366; Percent done: 9.2%;                    Mean loss: 4.2213\n",
      "Iteration: 367; Percent done: 9.2%;                    Mean loss: 4.1089\n",
      "Iteration: 368; Percent done: 9.2%;                    Mean loss: 4.4312\n",
      "Iteration: 369; Percent done: 9.2%;                    Mean loss: 4.0357\n",
      "Iteration: 370; Percent done: 9.2%;                    Mean loss: 3.9721\n",
      "Iteration: 371; Percent done: 9.3%;                    Mean loss: 4.3071\n",
      "Iteration: 372; Percent done: 9.3%;                    Mean loss: 4.1164\n",
      "Iteration: 373; Percent done: 9.3%;                    Mean loss: 4.0163\n",
      "Iteration: 374; Percent done: 9.3%;                    Mean loss: 4.1265\n",
      "Iteration: 375; Percent done: 9.4%;                    Mean loss: 4.2420\n",
      "Iteration: 376; Percent done: 9.4%;                    Mean loss: 4.2186\n",
      "Iteration: 377; Percent done: 9.4%;                    Mean loss: 4.4079\n",
      "Iteration: 378; Percent done: 9.4%;                    Mean loss: 3.9848\n",
      "Iteration: 379; Percent done: 9.5%;                    Mean loss: 4.0438\n",
      "Iteration: 380; Percent done: 9.5%;                    Mean loss: 3.9467\n",
      "Iteration: 381; Percent done: 9.5%;                    Mean loss: 4.1216\n",
      "Iteration: 382; Percent done: 9.6%;                    Mean loss: 4.1585\n",
      "Iteration: 383; Percent done: 9.6%;                    Mean loss: 3.7961\n",
      "Iteration: 384; Percent done: 9.6%;                    Mean loss: 4.0611\n",
      "Iteration: 385; Percent done: 9.6%;                    Mean loss: 4.2700\n",
      "Iteration: 386; Percent done: 9.7%;                    Mean loss: 4.1764\n",
      "Iteration: 387; Percent done: 9.7%;                    Mean loss: 4.1387\n",
      "Iteration: 388; Percent done: 9.7%;                    Mean loss: 4.2509\n",
      "Iteration: 389; Percent done: 9.7%;                    Mean loss: 4.0526\n",
      "Iteration: 390; Percent done: 9.8%;                    Mean loss: 4.1362\n",
      "Iteration: 391; Percent done: 9.8%;                    Mean loss: 4.1518\n",
      "Iteration: 392; Percent done: 9.8%;                    Mean loss: 4.3198\n",
      "Iteration: 393; Percent done: 9.8%;                    Mean loss: 3.8887\n",
      "Iteration: 394; Percent done: 9.8%;                    Mean loss: 4.2731\n",
      "Iteration: 395; Percent done: 9.9%;                    Mean loss: 3.9660\n",
      "Iteration: 396; Percent done: 9.9%;                    Mean loss: 4.0541\n",
      "Iteration: 397; Percent done: 9.9%;                    Mean loss: 4.0521\n",
      "Iteration: 398; Percent done: 10.0%;                    Mean loss: 4.0000\n",
      "Iteration: 399; Percent done: 10.0%;                    Mean loss: 3.9820\n",
      "Iteration: 400; Percent done: 10.0%;                    Mean loss: 4.1587\n",
      "Iteration: 401; Percent done: 10.0%;                    Mean loss: 4.1623\n",
      "Iteration: 402; Percent done: 10.1%;                    Mean loss: 4.0184\n",
      "Iteration: 403; Percent done: 10.1%;                    Mean loss: 4.1642\n",
      "Iteration: 404; Percent done: 10.1%;                    Mean loss: 3.8323\n",
      "Iteration: 405; Percent done: 10.1%;                    Mean loss: 4.0544\n",
      "Iteration: 406; Percent done: 10.2%;                    Mean loss: 4.0483\n",
      "Iteration: 407; Percent done: 10.2%;                    Mean loss: 3.6025\n",
      "Iteration: 408; Percent done: 10.2%;                    Mean loss: 4.1041\n",
      "Iteration: 409; Percent done: 10.2%;                    Mean loss: 4.3289\n",
      "Iteration: 410; Percent done: 10.2%;                    Mean loss: 4.1951\n",
      "Iteration: 411; Percent done: 10.3%;                    Mean loss: 4.0261\n",
      "Iteration: 412; Percent done: 10.3%;                    Mean loss: 3.9271\n",
      "Iteration: 413; Percent done: 10.3%;                    Mean loss: 4.4182\n",
      "Iteration: 414; Percent done: 10.3%;                    Mean loss: 3.8958\n",
      "Iteration: 415; Percent done: 10.4%;                    Mean loss: 3.8245\n",
      "Iteration: 416; Percent done: 10.4%;                    Mean loss: 3.9143\n",
      "Iteration: 417; Percent done: 10.4%;                    Mean loss: 4.2059\n",
      "Iteration: 418; Percent done: 10.4%;                    Mean loss: 4.0751\n",
      "Iteration: 419; Percent done: 10.5%;                    Mean loss: 4.0522\n",
      "Iteration: 420; Percent done: 10.5%;                    Mean loss: 3.9773\n",
      "Iteration: 421; Percent done: 10.5%;                    Mean loss: 4.0929\n",
      "Iteration: 422; Percent done: 10.5%;                    Mean loss: 4.0680\n",
      "Iteration: 423; Percent done: 10.6%;                    Mean loss: 4.0483\n",
      "Iteration: 424; Percent done: 10.6%;                    Mean loss: 3.9506\n",
      "Iteration: 425; Percent done: 10.6%;                    Mean loss: 4.0654\n",
      "Iteration: 426; Percent done: 10.7%;                    Mean loss: 4.3238\n",
      "Iteration: 427; Percent done: 10.7%;                    Mean loss: 4.0818\n",
      "Iteration: 428; Percent done: 10.7%;                    Mean loss: 4.3265\n",
      "Iteration: 429; Percent done: 10.7%;                    Mean loss: 3.9084\n",
      "Iteration: 430; Percent done: 10.8%;                    Mean loss: 3.9715\n",
      "Iteration: 431; Percent done: 10.8%;                    Mean loss: 4.0738\n",
      "Iteration: 432; Percent done: 10.8%;                    Mean loss: 4.2301\n",
      "Iteration: 433; Percent done: 10.8%;                    Mean loss: 4.1711\n",
      "Iteration: 434; Percent done: 10.8%;                    Mean loss: 4.1593\n",
      "Iteration: 435; Percent done: 10.9%;                    Mean loss: 4.2415\n",
      "Iteration: 436; Percent done: 10.9%;                    Mean loss: 4.3035\n",
      "Iteration: 437; Percent done: 10.9%;                    Mean loss: 4.4177\n",
      "Iteration: 438; Percent done: 10.9%;                    Mean loss: 4.0694\n",
      "Iteration: 439; Percent done: 11.0%;                    Mean loss: 3.8284\n",
      "Iteration: 440; Percent done: 11.0%;                    Mean loss: 4.0642\n",
      "Iteration: 441; Percent done: 11.0%;                    Mean loss: 3.9181\n",
      "Iteration: 442; Percent done: 11.1%;                    Mean loss: 3.7781\n",
      "Iteration: 443; Percent done: 11.1%;                    Mean loss: 4.0836\n",
      "Iteration: 444; Percent done: 11.1%;                    Mean loss: 4.2678\n",
      "Iteration: 445; Percent done: 11.1%;                    Mean loss: 4.0121\n",
      "Iteration: 446; Percent done: 11.2%;                    Mean loss: 4.0822\n",
      "Iteration: 447; Percent done: 11.2%;                    Mean loss: 4.1885\n",
      "Iteration: 448; Percent done: 11.2%;                    Mean loss: 4.0164\n",
      "Iteration: 449; Percent done: 11.2%;                    Mean loss: 3.9513\n",
      "Iteration: 450; Percent done: 11.2%;                    Mean loss: 4.0183\n",
      "Iteration: 451; Percent done: 11.3%;                    Mean loss: 4.1664\n",
      "Iteration: 452; Percent done: 11.3%;                    Mean loss: 4.0588\n",
      "Iteration: 453; Percent done: 11.3%;                    Mean loss: 4.0778\n",
      "Iteration: 454; Percent done: 11.3%;                    Mean loss: 3.8424\n",
      "Iteration: 455; Percent done: 11.4%;                    Mean loss: 4.0652\n",
      "Iteration: 456; Percent done: 11.4%;                    Mean loss: 3.8220\n",
      "Iteration: 457; Percent done: 11.4%;                    Mean loss: 4.0143\n",
      "Iteration: 458; Percent done: 11.5%;                    Mean loss: 4.1591\n",
      "Iteration: 459; Percent done: 11.5%;                    Mean loss: 4.1352\n",
      "Iteration: 460; Percent done: 11.5%;                    Mean loss: 4.3306\n",
      "Iteration: 461; Percent done: 11.5%;                    Mean loss: 3.6941\n",
      "Iteration: 462; Percent done: 11.6%;                    Mean loss: 4.0634\n",
      "Iteration: 463; Percent done: 11.6%;                    Mean loss: 4.2390\n",
      "Iteration: 464; Percent done: 11.6%;                    Mean loss: 4.1625\n",
      "Iteration: 465; Percent done: 11.6%;                    Mean loss: 4.1474\n",
      "Iteration: 466; Percent done: 11.7%;                    Mean loss: 4.1233\n",
      "Iteration: 467; Percent done: 11.7%;                    Mean loss: 4.1057\n",
      "Iteration: 468; Percent done: 11.7%;                    Mean loss: 3.9069\n",
      "Iteration: 469; Percent done: 11.7%;                    Mean loss: 3.8473\n",
      "Iteration: 470; Percent done: 11.8%;                    Mean loss: 3.9564\n",
      "Iteration: 471; Percent done: 11.8%;                    Mean loss: 4.1795\n",
      "Iteration: 472; Percent done: 11.8%;                    Mean loss: 4.1093\n",
      "Iteration: 473; Percent done: 11.8%;                    Mean loss: 4.2262\n",
      "Iteration: 474; Percent done: 11.8%;                    Mean loss: 4.1859\n",
      "Iteration: 475; Percent done: 11.9%;                    Mean loss: 3.9603\n",
      "Iteration: 476; Percent done: 11.9%;                    Mean loss: 4.0058\n",
      "Iteration: 477; Percent done: 11.9%;                    Mean loss: 3.9493\n",
      "Iteration: 478; Percent done: 11.9%;                    Mean loss: 3.8975\n",
      "Iteration: 479; Percent done: 12.0%;                    Mean loss: 4.0660\n",
      "Iteration: 480; Percent done: 12.0%;                    Mean loss: 3.9784\n",
      "Iteration: 481; Percent done: 12.0%;                    Mean loss: 3.8894\n",
      "Iteration: 482; Percent done: 12.0%;                    Mean loss: 3.8886\n",
      "Iteration: 483; Percent done: 12.1%;                    Mean loss: 3.9136\n",
      "Iteration: 484; Percent done: 12.1%;                    Mean loss: 4.2511\n",
      "Iteration: 485; Percent done: 12.1%;                    Mean loss: 4.0058\n",
      "Iteration: 486; Percent done: 12.2%;                    Mean loss: 3.9166\n",
      "Iteration: 487; Percent done: 12.2%;                    Mean loss: 4.2341\n",
      "Iteration: 488; Percent done: 12.2%;                    Mean loss: 3.9270\n",
      "Iteration: 489; Percent done: 12.2%;                    Mean loss: 4.1100\n",
      "Iteration: 490; Percent done: 12.2%;                    Mean loss: 4.0932\n",
      "Iteration: 491; Percent done: 12.3%;                    Mean loss: 4.3193\n",
      "Iteration: 492; Percent done: 12.3%;                    Mean loss: 4.0461\n",
      "Iteration: 493; Percent done: 12.3%;                    Mean loss: 3.9484\n",
      "Iteration: 494; Percent done: 12.3%;                    Mean loss: 3.9524\n",
      "Iteration: 495; Percent done: 12.4%;                    Mean loss: 4.0058\n",
      "Iteration: 496; Percent done: 12.4%;                    Mean loss: 3.8952\n",
      "Iteration: 497; Percent done: 12.4%;                    Mean loss: 4.1294\n",
      "Iteration: 498; Percent done: 12.4%;                    Mean loss: 4.0797\n",
      "Iteration: 499; Percent done: 12.5%;                    Mean loss: 4.1672\n",
      "Iteration: 500; Percent done: 12.5%;                    Mean loss: 3.9681\n",
      "Iteration: 501; Percent done: 12.5%;                    Mean loss: 4.0879\n",
      "Iteration: 502; Percent done: 12.6%;                    Mean loss: 3.7765\n",
      "Iteration: 503; Percent done: 12.6%;                    Mean loss: 4.0698\n",
      "Iteration: 504; Percent done: 12.6%;                    Mean loss: 4.0914\n",
      "Iteration: 505; Percent done: 12.6%;                    Mean loss: 3.9389\n",
      "Iteration: 506; Percent done: 12.7%;                    Mean loss: 4.0280\n",
      "Iteration: 507; Percent done: 12.7%;                    Mean loss: 3.8299\n",
      "Iteration: 508; Percent done: 12.7%;                    Mean loss: 3.9094\n",
      "Iteration: 509; Percent done: 12.7%;                    Mean loss: 3.9767\n",
      "Iteration: 510; Percent done: 12.8%;                    Mean loss: 4.0350\n",
      "Iteration: 511; Percent done: 12.8%;                    Mean loss: 3.9595\n",
      "Iteration: 512; Percent done: 12.8%;                    Mean loss: 4.0863\n",
      "Iteration: 513; Percent done: 12.8%;                    Mean loss: 4.0429\n",
      "Iteration: 514; Percent done: 12.8%;                    Mean loss: 4.0570\n",
      "Iteration: 515; Percent done: 12.9%;                    Mean loss: 4.0609\n",
      "Iteration: 516; Percent done: 12.9%;                    Mean loss: 3.8220\n",
      "Iteration: 517; Percent done: 12.9%;                    Mean loss: 3.8880\n",
      "Iteration: 518; Percent done: 13.0%;                    Mean loss: 4.1273\n",
      "Iteration: 519; Percent done: 13.0%;                    Mean loss: 4.0422\n",
      "Iteration: 520; Percent done: 13.0%;                    Mean loss: 4.0041\n",
      "Iteration: 521; Percent done: 13.0%;                    Mean loss: 4.1101\n",
      "Iteration: 522; Percent done: 13.1%;                    Mean loss: 3.9513\n",
      "Iteration: 523; Percent done: 13.1%;                    Mean loss: 3.9466\n",
      "Iteration: 524; Percent done: 13.1%;                    Mean loss: 4.1430\n",
      "Iteration: 525; Percent done: 13.1%;                    Mean loss: 4.0790\n",
      "Iteration: 526; Percent done: 13.2%;                    Mean loss: 3.9944\n",
      "Iteration: 527; Percent done: 13.2%;                    Mean loss: 3.8018\n",
      "Iteration: 528; Percent done: 13.2%;                    Mean loss: 3.7968\n",
      "Iteration: 529; Percent done: 13.2%;                    Mean loss: 3.7672\n",
      "Iteration: 530; Percent done: 13.2%;                    Mean loss: 4.3185\n",
      "Iteration: 531; Percent done: 13.3%;                    Mean loss: 4.1042\n",
      "Iteration: 532; Percent done: 13.3%;                    Mean loss: 3.9473\n",
      "Iteration: 533; Percent done: 13.3%;                    Mean loss: 3.5612\n",
      "Iteration: 534; Percent done: 13.4%;                    Mean loss: 3.9775\n",
      "Iteration: 535; Percent done: 13.4%;                    Mean loss: 3.5590\n",
      "Iteration: 536; Percent done: 13.4%;                    Mean loss: 3.9533\n",
      "Iteration: 537; Percent done: 13.4%;                    Mean loss: 4.1709\n",
      "Iteration: 538; Percent done: 13.5%;                    Mean loss: 4.1516\n",
      "Iteration: 539; Percent done: 13.5%;                    Mean loss: 4.2195\n",
      "Iteration: 540; Percent done: 13.5%;                    Mean loss: 4.0184\n",
      "Iteration: 541; Percent done: 13.5%;                    Mean loss: 4.0124\n",
      "Iteration: 542; Percent done: 13.6%;                    Mean loss: 3.7721\n",
      "Iteration: 543; Percent done: 13.6%;                    Mean loss: 4.0702\n",
      "Iteration: 544; Percent done: 13.6%;                    Mean loss: 4.0370\n",
      "Iteration: 545; Percent done: 13.6%;                    Mean loss: 4.1349\n",
      "Iteration: 546; Percent done: 13.7%;                    Mean loss: 3.8722\n",
      "Iteration: 547; Percent done: 13.7%;                    Mean loss: 4.0377\n",
      "Iteration: 548; Percent done: 13.7%;                    Mean loss: 3.8744\n",
      "Iteration: 549; Percent done: 13.7%;                    Mean loss: 3.9970\n",
      "Iteration: 550; Percent done: 13.8%;                    Mean loss: 4.1032\n",
      "Iteration: 551; Percent done: 13.8%;                    Mean loss: 3.9928\n",
      "Iteration: 552; Percent done: 13.8%;                    Mean loss: 3.7839\n",
      "Iteration: 553; Percent done: 13.8%;                    Mean loss: 4.0775\n",
      "Iteration: 554; Percent done: 13.9%;                    Mean loss: 3.8890\n",
      "Iteration: 555; Percent done: 13.9%;                    Mean loss: 4.0949\n",
      "Iteration: 556; Percent done: 13.9%;                    Mean loss: 4.0896\n",
      "Iteration: 557; Percent done: 13.9%;                    Mean loss: 4.0327\n",
      "Iteration: 558; Percent done: 14.0%;                    Mean loss: 3.8279\n",
      "Iteration: 559; Percent done: 14.0%;                    Mean loss: 4.1313\n",
      "Iteration: 560; Percent done: 14.0%;                    Mean loss: 4.1081\n",
      "Iteration: 561; Percent done: 14.0%;                    Mean loss: 3.9924\n",
      "Iteration: 562; Percent done: 14.1%;                    Mean loss: 4.0528\n",
      "Iteration: 563; Percent done: 14.1%;                    Mean loss: 4.2632\n",
      "Iteration: 564; Percent done: 14.1%;                    Mean loss: 3.9297\n",
      "Iteration: 565; Percent done: 14.1%;                    Mean loss: 3.7895\n",
      "Iteration: 566; Percent done: 14.1%;                    Mean loss: 4.0848\n",
      "Iteration: 567; Percent done: 14.2%;                    Mean loss: 3.9770\n",
      "Iteration: 568; Percent done: 14.2%;                    Mean loss: 3.9209\n",
      "Iteration: 569; Percent done: 14.2%;                    Mean loss: 3.8667\n",
      "Iteration: 570; Percent done: 14.2%;                    Mean loss: 3.8598\n",
      "Iteration: 571; Percent done: 14.3%;                    Mean loss: 4.0495\n",
      "Iteration: 572; Percent done: 14.3%;                    Mean loss: 4.1109\n",
      "Iteration: 573; Percent done: 14.3%;                    Mean loss: 3.7399\n",
      "Iteration: 574; Percent done: 14.3%;                    Mean loss: 4.0899\n",
      "Iteration: 575; Percent done: 14.4%;                    Mean loss: 3.9520\n",
      "Iteration: 576; Percent done: 14.4%;                    Mean loss: 4.0254\n",
      "Iteration: 577; Percent done: 14.4%;                    Mean loss: 3.7203\n",
      "Iteration: 578; Percent done: 14.4%;                    Mean loss: 3.8991\n",
      "Iteration: 579; Percent done: 14.5%;                    Mean loss: 3.7223\n",
      "Iteration: 580; Percent done: 14.5%;                    Mean loss: 3.9774\n",
      "Iteration: 581; Percent done: 14.5%;                    Mean loss: 3.8977\n",
      "Iteration: 582; Percent done: 14.5%;                    Mean loss: 3.9829\n",
      "Iteration: 583; Percent done: 14.6%;                    Mean loss: 4.0833\n",
      "Iteration: 584; Percent done: 14.6%;                    Mean loss: 4.0430\n",
      "Iteration: 585; Percent done: 14.6%;                    Mean loss: 4.0565\n",
      "Iteration: 586; Percent done: 14.6%;                    Mean loss: 4.0032\n",
      "Iteration: 587; Percent done: 14.7%;                    Mean loss: 3.8664\n",
      "Iteration: 588; Percent done: 14.7%;                    Mean loss: 4.1070\n",
      "Iteration: 589; Percent done: 14.7%;                    Mean loss: 3.8045\n",
      "Iteration: 590; Percent done: 14.8%;                    Mean loss: 3.9190\n",
      "Iteration: 591; Percent done: 14.8%;                    Mean loss: 3.9620\n",
      "Iteration: 592; Percent done: 14.8%;                    Mean loss: 4.0020\n",
      "Iteration: 593; Percent done: 14.8%;                    Mean loss: 3.9824\n",
      "Iteration: 594; Percent done: 14.8%;                    Mean loss: 3.6911\n",
      "Iteration: 595; Percent done: 14.9%;                    Mean loss: 3.9273\n",
      "Iteration: 596; Percent done: 14.9%;                    Mean loss: 3.7152\n",
      "Iteration: 597; Percent done: 14.9%;                    Mean loss: 3.7037\n",
      "Iteration: 598; Percent done: 14.9%;                    Mean loss: 3.9200\n",
      "Iteration: 599; Percent done: 15.0%;                    Mean loss: 4.2899\n",
      "Iteration: 600; Percent done: 15.0%;                    Mean loss: 4.0182\n",
      "Iteration: 601; Percent done: 15.0%;                    Mean loss: 3.8926\n",
      "Iteration: 602; Percent done: 15.0%;                    Mean loss: 4.0795\n",
      "Iteration: 603; Percent done: 15.1%;                    Mean loss: 4.0133\n",
      "Iteration: 604; Percent done: 15.1%;                    Mean loss: 3.7877\n",
      "Iteration: 605; Percent done: 15.1%;                    Mean loss: 3.9772\n",
      "Iteration: 606; Percent done: 15.2%;                    Mean loss: 3.8893\n",
      "Iteration: 607; Percent done: 15.2%;                    Mean loss: 3.8331\n",
      "Iteration: 608; Percent done: 15.2%;                    Mean loss: 3.8770\n",
      "Iteration: 609; Percent done: 15.2%;                    Mean loss: 4.1141\n",
      "Iteration: 610; Percent done: 15.2%;                    Mean loss: 4.0346\n",
      "Iteration: 611; Percent done: 15.3%;                    Mean loss: 3.7642\n",
      "Iteration: 612; Percent done: 15.3%;                    Mean loss: 3.9659\n",
      "Iteration: 613; Percent done: 15.3%;                    Mean loss: 3.7043\n",
      "Iteration: 614; Percent done: 15.3%;                    Mean loss: 3.9187\n",
      "Iteration: 615; Percent done: 15.4%;                    Mean loss: 3.9059\n",
      "Iteration: 616; Percent done: 15.4%;                    Mean loss: 3.6740\n",
      "Iteration: 617; Percent done: 15.4%;                    Mean loss: 4.0861\n",
      "Iteration: 618; Percent done: 15.4%;                    Mean loss: 4.0159\n",
      "Iteration: 619; Percent done: 15.5%;                    Mean loss: 3.6077\n",
      "Iteration: 620; Percent done: 15.5%;                    Mean loss: 3.9528\n",
      "Iteration: 621; Percent done: 15.5%;                    Mean loss: 3.8326\n",
      "Iteration: 622; Percent done: 15.6%;                    Mean loss: 3.7555\n",
      "Iteration: 623; Percent done: 15.6%;                    Mean loss: 3.8450\n",
      "Iteration: 624; Percent done: 15.6%;                    Mean loss: 3.8962\n",
      "Iteration: 625; Percent done: 15.6%;                    Mean loss: 4.0318\n",
      "Iteration: 626; Percent done: 15.7%;                    Mean loss: 4.1536\n",
      "Iteration: 627; Percent done: 15.7%;                    Mean loss: 3.9116\n",
      "Iteration: 628; Percent done: 15.7%;                    Mean loss: 3.8167\n",
      "Iteration: 629; Percent done: 15.7%;                    Mean loss: 4.0189\n",
      "Iteration: 630; Percent done: 15.8%;                    Mean loss: 3.9767\n",
      "Iteration: 631; Percent done: 15.8%;                    Mean loss: 3.4133\n",
      "Iteration: 632; Percent done: 15.8%;                    Mean loss: 4.1635\n",
      "Iteration: 633; Percent done: 15.8%;                    Mean loss: 3.9278\n",
      "Iteration: 634; Percent done: 15.8%;                    Mean loss: 3.6560\n",
      "Iteration: 635; Percent done: 15.9%;                    Mean loss: 3.7362\n",
      "Iteration: 636; Percent done: 15.9%;                    Mean loss: 3.7693\n",
      "Iteration: 637; Percent done: 15.9%;                    Mean loss: 4.0296\n",
      "Iteration: 638; Percent done: 16.0%;                    Mean loss: 3.7218\n",
      "Iteration: 639; Percent done: 16.0%;                    Mean loss: 3.8650\n",
      "Iteration: 640; Percent done: 16.0%;                    Mean loss: 4.0281\n",
      "Iteration: 641; Percent done: 16.0%;                    Mean loss: 3.7402\n",
      "Iteration: 642; Percent done: 16.1%;                    Mean loss: 3.7492\n",
      "Iteration: 643; Percent done: 16.1%;                    Mean loss: 3.9919\n",
      "Iteration: 644; Percent done: 16.1%;                    Mean loss: 3.8172\n",
      "Iteration: 645; Percent done: 16.1%;                    Mean loss: 3.5254\n",
      "Iteration: 646; Percent done: 16.2%;                    Mean loss: 4.2010\n",
      "Iteration: 647; Percent done: 16.2%;                    Mean loss: 3.7286\n",
      "Iteration: 648; Percent done: 16.2%;                    Mean loss: 4.1368\n",
      "Iteration: 649; Percent done: 16.2%;                    Mean loss: 4.0585\n",
      "Iteration: 650; Percent done: 16.2%;                    Mean loss: 3.8044\n",
      "Iteration: 651; Percent done: 16.3%;                    Mean loss: 4.1956\n",
      "Iteration: 652; Percent done: 16.3%;                    Mean loss: 4.1154\n",
      "Iteration: 653; Percent done: 16.3%;                    Mean loss: 3.7348\n",
      "Iteration: 654; Percent done: 16.4%;                    Mean loss: 3.8966\n",
      "Iteration: 655; Percent done: 16.4%;                    Mean loss: 3.8149\n",
      "Iteration: 656; Percent done: 16.4%;                    Mean loss: 3.8727\n",
      "Iteration: 657; Percent done: 16.4%;                    Mean loss: 3.8093\n",
      "Iteration: 658; Percent done: 16.4%;                    Mean loss: 3.8724\n",
      "Iteration: 659; Percent done: 16.5%;                    Mean loss: 3.8107\n",
      "Iteration: 660; Percent done: 16.5%;                    Mean loss: 3.8797\n",
      "Iteration: 661; Percent done: 16.5%;                    Mean loss: 4.1119\n",
      "Iteration: 662; Percent done: 16.6%;                    Mean loss: 3.6979\n",
      "Iteration: 663; Percent done: 16.6%;                    Mean loss: 4.0353\n",
      "Iteration: 664; Percent done: 16.6%;                    Mean loss: 4.0150\n",
      "Iteration: 665; Percent done: 16.6%;                    Mean loss: 3.9325\n",
      "Iteration: 666; Percent done: 16.7%;                    Mean loss: 3.8025\n",
      "Iteration: 667; Percent done: 16.7%;                    Mean loss: 3.6520\n",
      "Iteration: 668; Percent done: 16.7%;                    Mean loss: 3.8548\n",
      "Iteration: 669; Percent done: 16.7%;                    Mean loss: 3.7833\n",
      "Iteration: 670; Percent done: 16.8%;                    Mean loss: 3.8950\n",
      "Iteration: 671; Percent done: 16.8%;                    Mean loss: 3.9847\n",
      "Iteration: 672; Percent done: 16.8%;                    Mean loss: 4.2511\n",
      "Iteration: 673; Percent done: 16.8%;                    Mean loss: 3.9005\n",
      "Iteration: 674; Percent done: 16.9%;                    Mean loss: 3.6559\n",
      "Iteration: 675; Percent done: 16.9%;                    Mean loss: 3.9119\n",
      "Iteration: 676; Percent done: 16.9%;                    Mean loss: 4.1299\n",
      "Iteration: 677; Percent done: 16.9%;                    Mean loss: 3.7004\n",
      "Iteration: 678; Percent done: 17.0%;                    Mean loss: 3.7123\n",
      "Iteration: 679; Percent done: 17.0%;                    Mean loss: 3.8673\n",
      "Iteration: 680; Percent done: 17.0%;                    Mean loss: 3.9484\n",
      "Iteration: 681; Percent done: 17.0%;                    Mean loss: 3.9294\n",
      "Iteration: 682; Percent done: 17.1%;                    Mean loss: 3.7942\n",
      "Iteration: 683; Percent done: 17.1%;                    Mean loss: 4.0455\n",
      "Iteration: 684; Percent done: 17.1%;                    Mean loss: 3.8799\n",
      "Iteration: 685; Percent done: 17.1%;                    Mean loss: 3.8559\n",
      "Iteration: 686; Percent done: 17.2%;                    Mean loss: 4.0700\n",
      "Iteration: 687; Percent done: 17.2%;                    Mean loss: 3.6520\n",
      "Iteration: 688; Percent done: 17.2%;                    Mean loss: 3.8897\n",
      "Iteration: 689; Percent done: 17.2%;                    Mean loss: 3.9509\n",
      "Iteration: 690; Percent done: 17.2%;                    Mean loss: 3.9387\n",
      "Iteration: 691; Percent done: 17.3%;                    Mean loss: 3.6833\n",
      "Iteration: 692; Percent done: 17.3%;                    Mean loss: 4.0179\n",
      "Iteration: 693; Percent done: 17.3%;                    Mean loss: 3.8728\n",
      "Iteration: 694; Percent done: 17.3%;                    Mean loss: 3.6431\n",
      "Iteration: 695; Percent done: 17.4%;                    Mean loss: 3.7268\n",
      "Iteration: 696; Percent done: 17.4%;                    Mean loss: 3.5393\n",
      "Iteration: 697; Percent done: 17.4%;                    Mean loss: 3.7824\n",
      "Iteration: 698; Percent done: 17.4%;                    Mean loss: 3.7343\n",
      "Iteration: 699; Percent done: 17.5%;                    Mean loss: 3.7269\n",
      "Iteration: 700; Percent done: 17.5%;                    Mean loss: 3.6755\n",
      "Iteration: 701; Percent done: 17.5%;                    Mean loss: 4.0017\n",
      "Iteration: 702; Percent done: 17.5%;                    Mean loss: 3.9594\n",
      "Iteration: 703; Percent done: 17.6%;                    Mean loss: 4.0137\n",
      "Iteration: 704; Percent done: 17.6%;                    Mean loss: 3.8639\n",
      "Iteration: 705; Percent done: 17.6%;                    Mean loss: 3.7448\n",
      "Iteration: 706; Percent done: 17.6%;                    Mean loss: 3.7759\n",
      "Iteration: 707; Percent done: 17.7%;                    Mean loss: 3.9600\n",
      "Iteration: 708; Percent done: 17.7%;                    Mean loss: 3.8098\n",
      "Iteration: 709; Percent done: 17.7%;                    Mean loss: 3.9132\n",
      "Iteration: 710; Percent done: 17.8%;                    Mean loss: 3.7527\n",
      "Iteration: 711; Percent done: 17.8%;                    Mean loss: 3.7873\n",
      "Iteration: 712; Percent done: 17.8%;                    Mean loss: 3.8136\n",
      "Iteration: 713; Percent done: 17.8%;                    Mean loss: 3.6502\n",
      "Iteration: 714; Percent done: 17.8%;                    Mean loss: 3.8928\n",
      "Iteration: 715; Percent done: 17.9%;                    Mean loss: 3.6183\n",
      "Iteration: 716; Percent done: 17.9%;                    Mean loss: 3.7728\n",
      "Iteration: 717; Percent done: 17.9%;                    Mean loss: 3.7581\n",
      "Iteration: 718; Percent done: 17.9%;                    Mean loss: 3.6175\n",
      "Iteration: 719; Percent done: 18.0%;                    Mean loss: 3.8929\n",
      "Iteration: 720; Percent done: 18.0%;                    Mean loss: 4.0091\n",
      "Iteration: 721; Percent done: 18.0%;                    Mean loss: 3.8317\n",
      "Iteration: 722; Percent done: 18.1%;                    Mean loss: 3.6424\n",
      "Iteration: 723; Percent done: 18.1%;                    Mean loss: 3.8307\n",
      "Iteration: 724; Percent done: 18.1%;                    Mean loss: 3.7956\n",
      "Iteration: 725; Percent done: 18.1%;                    Mean loss: 3.7418\n",
      "Iteration: 726; Percent done: 18.1%;                    Mean loss: 3.9021\n",
      "Iteration: 727; Percent done: 18.2%;                    Mean loss: 4.0643\n",
      "Iteration: 728; Percent done: 18.2%;                    Mean loss: 3.7317\n",
      "Iteration: 729; Percent done: 18.2%;                    Mean loss: 3.6751\n",
      "Iteration: 730; Percent done: 18.2%;                    Mean loss: 3.7980\n",
      "Iteration: 731; Percent done: 18.3%;                    Mean loss: 3.9136\n",
      "Iteration: 732; Percent done: 18.3%;                    Mean loss: 3.9349\n",
      "Iteration: 733; Percent done: 18.3%;                    Mean loss: 3.8486\n",
      "Iteration: 734; Percent done: 18.4%;                    Mean loss: 3.7469\n",
      "Iteration: 735; Percent done: 18.4%;                    Mean loss: 3.8490\n",
      "Iteration: 736; Percent done: 18.4%;                    Mean loss: 3.6435\n",
      "Iteration: 737; Percent done: 18.4%;                    Mean loss: 3.6994\n",
      "Iteration: 738; Percent done: 18.4%;                    Mean loss: 3.9257\n",
      "Iteration: 739; Percent done: 18.5%;                    Mean loss: 3.7170\n",
      "Iteration: 740; Percent done: 18.5%;                    Mean loss: 4.1385\n",
      "Iteration: 741; Percent done: 18.5%;                    Mean loss: 3.6193\n",
      "Iteration: 742; Percent done: 18.6%;                    Mean loss: 4.1444\n",
      "Iteration: 743; Percent done: 18.6%;                    Mean loss: 3.8629\n",
      "Iteration: 744; Percent done: 18.6%;                    Mean loss: 3.7965\n",
      "Iteration: 745; Percent done: 18.6%;                    Mean loss: 3.6772\n",
      "Iteration: 746; Percent done: 18.6%;                    Mean loss: 3.7400\n",
      "Iteration: 747; Percent done: 18.7%;                    Mean loss: 3.5737\n",
      "Iteration: 748; Percent done: 18.7%;                    Mean loss: 3.9866\n",
      "Iteration: 749; Percent done: 18.7%;                    Mean loss: 3.4693\n",
      "Iteration: 750; Percent done: 18.8%;                    Mean loss: 3.9832\n",
      "Iteration: 751; Percent done: 18.8%;                    Mean loss: 3.7828\n",
      "Iteration: 752; Percent done: 18.8%;                    Mean loss: 3.9651\n",
      "Iteration: 753; Percent done: 18.8%;                    Mean loss: 3.5498\n",
      "Iteration: 754; Percent done: 18.9%;                    Mean loss: 3.8174\n",
      "Iteration: 755; Percent done: 18.9%;                    Mean loss: 4.0035\n",
      "Iteration: 756; Percent done: 18.9%;                    Mean loss: 4.1309\n",
      "Iteration: 757; Percent done: 18.9%;                    Mean loss: 3.7746\n",
      "Iteration: 758; Percent done: 18.9%;                    Mean loss: 4.0655\n",
      "Iteration: 759; Percent done: 19.0%;                    Mean loss: 3.7873\n",
      "Iteration: 760; Percent done: 19.0%;                    Mean loss: 3.9491\n",
      "Iteration: 761; Percent done: 19.0%;                    Mean loss: 3.7939\n",
      "Iteration: 762; Percent done: 19.1%;                    Mean loss: 4.0717\n",
      "Iteration: 763; Percent done: 19.1%;                    Mean loss: 3.9000\n",
      "Iteration: 764; Percent done: 19.1%;                    Mean loss: 3.9169\n",
      "Iteration: 765; Percent done: 19.1%;                    Mean loss: 3.9613\n",
      "Iteration: 766; Percent done: 19.1%;                    Mean loss: 3.9622\n",
      "Iteration: 767; Percent done: 19.2%;                    Mean loss: 3.8396\n",
      "Iteration: 768; Percent done: 19.2%;                    Mean loss: 3.6195\n",
      "Iteration: 769; Percent done: 19.2%;                    Mean loss: 3.7873\n",
      "Iteration: 770; Percent done: 19.2%;                    Mean loss: 3.7642\n",
      "Iteration: 771; Percent done: 19.3%;                    Mean loss: 3.8069\n",
      "Iteration: 772; Percent done: 19.3%;                    Mean loss: 3.7140\n",
      "Iteration: 773; Percent done: 19.3%;                    Mean loss: 3.7923\n",
      "Iteration: 774; Percent done: 19.4%;                    Mean loss: 3.7423\n",
      "Iteration: 775; Percent done: 19.4%;                    Mean loss: 4.0277\n",
      "Iteration: 776; Percent done: 19.4%;                    Mean loss: 3.7518\n",
      "Iteration: 777; Percent done: 19.4%;                    Mean loss: 4.0972\n",
      "Iteration: 778; Percent done: 19.4%;                    Mean loss: 3.7113\n",
      "Iteration: 779; Percent done: 19.5%;                    Mean loss: 3.9331\n",
      "Iteration: 780; Percent done: 19.5%;                    Mean loss: 3.7170\n",
      "Iteration: 781; Percent done: 19.5%;                    Mean loss: 3.7901\n",
      "Iteration: 782; Percent done: 19.6%;                    Mean loss: 3.9876\n",
      "Iteration: 783; Percent done: 19.6%;                    Mean loss: 3.8901\n",
      "Iteration: 784; Percent done: 19.6%;                    Mean loss: 3.7055\n",
      "Iteration: 785; Percent done: 19.6%;                    Mean loss: 3.6602\n",
      "Iteration: 786; Percent done: 19.7%;                    Mean loss: 3.8546\n",
      "Iteration: 787; Percent done: 19.7%;                    Mean loss: 3.7795\n",
      "Iteration: 788; Percent done: 19.7%;                    Mean loss: 3.6531\n",
      "Iteration: 789; Percent done: 19.7%;                    Mean loss: 3.5927\n",
      "Iteration: 790; Percent done: 19.8%;                    Mean loss: 3.8511\n",
      "Iteration: 791; Percent done: 19.8%;                    Mean loss: 3.8515\n",
      "Iteration: 792; Percent done: 19.8%;                    Mean loss: 3.5641\n",
      "Iteration: 793; Percent done: 19.8%;                    Mean loss: 3.6641\n",
      "Iteration: 794; Percent done: 19.9%;                    Mean loss: 3.9085\n",
      "Iteration: 795; Percent done: 19.9%;                    Mean loss: 3.9749\n",
      "Iteration: 796; Percent done: 19.9%;                    Mean loss: 3.6616\n",
      "Iteration: 797; Percent done: 19.9%;                    Mean loss: 3.9984\n",
      "Iteration: 798; Percent done: 20.0%;                    Mean loss: 3.6605\n",
      "Iteration: 799; Percent done: 20.0%;                    Mean loss: 3.9371\n",
      "Iteration: 800; Percent done: 20.0%;                    Mean loss: 3.8981\n",
      "Iteration: 801; Percent done: 20.0%;                    Mean loss: 3.6546\n",
      "Iteration: 802; Percent done: 20.1%;                    Mean loss: 3.7697\n",
      "Iteration: 803; Percent done: 20.1%;                    Mean loss: 3.6795\n",
      "Iteration: 804; Percent done: 20.1%;                    Mean loss: 3.7873\n",
      "Iteration: 805; Percent done: 20.1%;                    Mean loss: 3.8509\n",
      "Iteration: 806; Percent done: 20.2%;                    Mean loss: 3.8232\n",
      "Iteration: 807; Percent done: 20.2%;                    Mean loss: 3.9553\n",
      "Iteration: 808; Percent done: 20.2%;                    Mean loss: 3.9986\n",
      "Iteration: 809; Percent done: 20.2%;                    Mean loss: 3.8230\n",
      "Iteration: 810; Percent done: 20.2%;                    Mean loss: 3.6129\n",
      "Iteration: 811; Percent done: 20.3%;                    Mean loss: 3.8093\n",
      "Iteration: 812; Percent done: 20.3%;                    Mean loss: 3.8946\n",
      "Iteration: 813; Percent done: 20.3%;                    Mean loss: 3.7719\n",
      "Iteration: 814; Percent done: 20.3%;                    Mean loss: 3.9534\n",
      "Iteration: 815; Percent done: 20.4%;                    Mean loss: 3.8784\n",
      "Iteration: 816; Percent done: 20.4%;                    Mean loss: 3.9206\n",
      "Iteration: 817; Percent done: 20.4%;                    Mean loss: 3.6507\n",
      "Iteration: 818; Percent done: 20.4%;                    Mean loss: 3.6316\n",
      "Iteration: 819; Percent done: 20.5%;                    Mean loss: 3.7073\n",
      "Iteration: 820; Percent done: 20.5%;                    Mean loss: 3.7755\n",
      "Iteration: 821; Percent done: 20.5%;                    Mean loss: 3.8976\n",
      "Iteration: 822; Percent done: 20.5%;                    Mean loss: 3.8247\n",
      "Iteration: 823; Percent done: 20.6%;                    Mean loss: 3.5552\n",
      "Iteration: 824; Percent done: 20.6%;                    Mean loss: 3.7510\n",
      "Iteration: 825; Percent done: 20.6%;                    Mean loss: 3.7745\n",
      "Iteration: 826; Percent done: 20.6%;                    Mean loss: 3.9003\n",
      "Iteration: 827; Percent done: 20.7%;                    Mean loss: 3.8354\n",
      "Iteration: 828; Percent done: 20.7%;                    Mean loss: 3.7013\n",
      "Iteration: 829; Percent done: 20.7%;                    Mean loss: 3.7435\n",
      "Iteration: 830; Percent done: 20.8%;                    Mean loss: 3.7508\n",
      "Iteration: 831; Percent done: 20.8%;                    Mean loss: 3.9434\n",
      "Iteration: 832; Percent done: 20.8%;                    Mean loss: 3.6096\n",
      "Iteration: 833; Percent done: 20.8%;                    Mean loss: 3.5873\n",
      "Iteration: 834; Percent done: 20.8%;                    Mean loss: 3.7471\n",
      "Iteration: 835; Percent done: 20.9%;                    Mean loss: 3.5682\n",
      "Iteration: 836; Percent done: 20.9%;                    Mean loss: 3.5026\n",
      "Iteration: 837; Percent done: 20.9%;                    Mean loss: 3.6949\n",
      "Iteration: 838; Percent done: 20.9%;                    Mean loss: 3.8676\n",
      "Iteration: 839; Percent done: 21.0%;                    Mean loss: 3.7501\n",
      "Iteration: 840; Percent done: 21.0%;                    Mean loss: 3.9132\n",
      "Iteration: 841; Percent done: 21.0%;                    Mean loss: 3.6156\n",
      "Iteration: 842; Percent done: 21.1%;                    Mean loss: 3.9071\n",
      "Iteration: 843; Percent done: 21.1%;                    Mean loss: 3.6588\n",
      "Iteration: 844; Percent done: 21.1%;                    Mean loss: 3.9886\n",
      "Iteration: 845; Percent done: 21.1%;                    Mean loss: 3.7513\n",
      "Iteration: 846; Percent done: 21.1%;                    Mean loss: 3.8850\n",
      "Iteration: 847; Percent done: 21.2%;                    Mean loss: 3.6749\n",
      "Iteration: 848; Percent done: 21.2%;                    Mean loss: 3.5873\n",
      "Iteration: 849; Percent done: 21.2%;                    Mean loss: 3.7868\n",
      "Iteration: 850; Percent done: 21.2%;                    Mean loss: 3.6483\n",
      "Iteration: 851; Percent done: 21.3%;                    Mean loss: 3.7550\n",
      "Iteration: 852; Percent done: 21.3%;                    Mean loss: 3.5608\n",
      "Iteration: 853; Percent done: 21.3%;                    Mean loss: 3.8225\n",
      "Iteration: 854; Percent done: 21.3%;                    Mean loss: 3.9044\n",
      "Iteration: 855; Percent done: 21.4%;                    Mean loss: 3.5415\n",
      "Iteration: 856; Percent done: 21.4%;                    Mean loss: 3.7650\n",
      "Iteration: 857; Percent done: 21.4%;                    Mean loss: 3.7235\n",
      "Iteration: 858; Percent done: 21.4%;                    Mean loss: 3.6140\n",
      "Iteration: 859; Percent done: 21.5%;                    Mean loss: 3.4964\n",
      "Iteration: 860; Percent done: 21.5%;                    Mean loss: 3.8244\n",
      "Iteration: 861; Percent done: 21.5%;                    Mean loss: 3.7337\n",
      "Iteration: 862; Percent done: 21.6%;                    Mean loss: 3.5774\n",
      "Iteration: 863; Percent done: 21.6%;                    Mean loss: 3.6220\n",
      "Iteration: 864; Percent done: 21.6%;                    Mean loss: 3.6150\n",
      "Iteration: 865; Percent done: 21.6%;                    Mean loss: 3.5048\n",
      "Iteration: 866; Percent done: 21.6%;                    Mean loss: 3.8098\n",
      "Iteration: 867; Percent done: 21.7%;                    Mean loss: 3.8750\n",
      "Iteration: 868; Percent done: 21.7%;                    Mean loss: 3.8384\n",
      "Iteration: 869; Percent done: 21.7%;                    Mean loss: 3.8330\n",
      "Iteration: 870; Percent done: 21.8%;                    Mean loss: 3.8552\n",
      "Iteration: 871; Percent done: 21.8%;                    Mean loss: 3.8851\n",
      "Iteration: 872; Percent done: 21.8%;                    Mean loss: 3.8532\n",
      "Iteration: 873; Percent done: 21.8%;                    Mean loss: 3.8414\n",
      "Iteration: 874; Percent done: 21.9%;                    Mean loss: 3.9114\n",
      "Iteration: 875; Percent done: 21.9%;                    Mean loss: 3.7898\n",
      "Iteration: 876; Percent done: 21.9%;                    Mean loss: 3.5002\n",
      "Iteration: 877; Percent done: 21.9%;                    Mean loss: 3.6256\n",
      "Iteration: 878; Percent done: 21.9%;                    Mean loss: 3.6289\n",
      "Iteration: 879; Percent done: 22.0%;                    Mean loss: 3.6689\n",
      "Iteration: 880; Percent done: 22.0%;                    Mean loss: 3.8183\n",
      "Iteration: 881; Percent done: 22.0%;                    Mean loss: 3.6754\n",
      "Iteration: 882; Percent done: 22.1%;                    Mean loss: 3.7700\n",
      "Iteration: 883; Percent done: 22.1%;                    Mean loss: 3.6151\n",
      "Iteration: 884; Percent done: 22.1%;                    Mean loss: 3.7036\n",
      "Iteration: 885; Percent done: 22.1%;                    Mean loss: 3.8395\n",
      "Iteration: 886; Percent done: 22.1%;                    Mean loss: 3.6855\n",
      "Iteration: 887; Percent done: 22.2%;                    Mean loss: 3.7076\n",
      "Iteration: 888; Percent done: 22.2%;                    Mean loss: 3.8851\n",
      "Iteration: 889; Percent done: 22.2%;                    Mean loss: 3.8758\n",
      "Iteration: 890; Percent done: 22.2%;                    Mean loss: 3.6441\n",
      "Iteration: 891; Percent done: 22.3%;                    Mean loss: 3.7964\n",
      "Iteration: 892; Percent done: 22.3%;                    Mean loss: 3.8997\n",
      "Iteration: 893; Percent done: 22.3%;                    Mean loss: 3.6306\n",
      "Iteration: 894; Percent done: 22.4%;                    Mean loss: 3.9850\n",
      "Iteration: 895; Percent done: 22.4%;                    Mean loss: 3.6730\n",
      "Iteration: 896; Percent done: 22.4%;                    Mean loss: 3.7433\n",
      "Iteration: 897; Percent done: 22.4%;                    Mean loss: 3.6515\n",
      "Iteration: 898; Percent done: 22.4%;                    Mean loss: 3.7080\n",
      "Iteration: 899; Percent done: 22.5%;                    Mean loss: 3.7909\n",
      "Iteration: 900; Percent done: 22.5%;                    Mean loss: 3.9662\n",
      "Iteration: 901; Percent done: 22.5%;                    Mean loss: 3.9114\n",
      "Iteration: 902; Percent done: 22.6%;                    Mean loss: 3.7993\n",
      "Iteration: 903; Percent done: 22.6%;                    Mean loss: 3.6807\n",
      "Iteration: 904; Percent done: 22.6%;                    Mean loss: 3.7666\n",
      "Iteration: 905; Percent done: 22.6%;                    Mean loss: 3.6039\n",
      "Iteration: 906; Percent done: 22.7%;                    Mean loss: 3.8919\n",
      "Iteration: 907; Percent done: 22.7%;                    Mean loss: 3.7135\n",
      "Iteration: 908; Percent done: 22.7%;                    Mean loss: 3.6225\n",
      "Iteration: 909; Percent done: 22.7%;                    Mean loss: 4.0145\n",
      "Iteration: 910; Percent done: 22.8%;                    Mean loss: 3.7163\n",
      "Iteration: 911; Percent done: 22.8%;                    Mean loss: 3.8116\n",
      "Iteration: 912; Percent done: 22.8%;                    Mean loss: 3.7303\n",
      "Iteration: 913; Percent done: 22.8%;                    Mean loss: 3.6820\n",
      "Iteration: 914; Percent done: 22.9%;                    Mean loss: 4.1016\n",
      "Iteration: 915; Percent done: 22.9%;                    Mean loss: 3.7633\n",
      "Iteration: 916; Percent done: 22.9%;                    Mean loss: 3.3755\n",
      "Iteration: 917; Percent done: 22.9%;                    Mean loss: 3.6549\n",
      "Iteration: 918; Percent done: 22.9%;                    Mean loss: 3.5919\n",
      "Iteration: 919; Percent done: 23.0%;                    Mean loss: 3.7692\n",
      "Iteration: 920; Percent done: 23.0%;                    Mean loss: 3.6614\n",
      "Iteration: 921; Percent done: 23.0%;                    Mean loss: 3.5412\n",
      "Iteration: 922; Percent done: 23.1%;                    Mean loss: 3.7309\n",
      "Iteration: 923; Percent done: 23.1%;                    Mean loss: 3.8115\n",
      "Iteration: 924; Percent done: 23.1%;                    Mean loss: 3.7101\n",
      "Iteration: 925; Percent done: 23.1%;                    Mean loss: 3.6559\n",
      "Iteration: 926; Percent done: 23.2%;                    Mean loss: 3.5252\n",
      "Iteration: 927; Percent done: 23.2%;                    Mean loss: 3.7158\n",
      "Iteration: 928; Percent done: 23.2%;                    Mean loss: 3.5052\n",
      "Iteration: 929; Percent done: 23.2%;                    Mean loss: 3.5531\n",
      "Iteration: 930; Percent done: 23.2%;                    Mean loss: 3.8968\n",
      "Iteration: 931; Percent done: 23.3%;                    Mean loss: 3.7029\n",
      "Iteration: 932; Percent done: 23.3%;                    Mean loss: 3.7578\n",
      "Iteration: 933; Percent done: 23.3%;                    Mean loss: 3.6970\n",
      "Iteration: 934; Percent done: 23.4%;                    Mean loss: 3.8460\n",
      "Iteration: 935; Percent done: 23.4%;                    Mean loss: 3.5907\n",
      "Iteration: 936; Percent done: 23.4%;                    Mean loss: 3.7141\n",
      "Iteration: 937; Percent done: 23.4%;                    Mean loss: 3.6722\n",
      "Iteration: 938; Percent done: 23.4%;                    Mean loss: 4.0285\n",
      "Iteration: 939; Percent done: 23.5%;                    Mean loss: 3.8338\n",
      "Iteration: 940; Percent done: 23.5%;                    Mean loss: 3.7806\n",
      "Iteration: 941; Percent done: 23.5%;                    Mean loss: 3.8926\n",
      "Iteration: 942; Percent done: 23.5%;                    Mean loss: 3.4742\n",
      "Iteration: 943; Percent done: 23.6%;                    Mean loss: 3.6091\n",
      "Iteration: 944; Percent done: 23.6%;                    Mean loss: 3.6628\n",
      "Iteration: 945; Percent done: 23.6%;                    Mean loss: 3.6593\n",
      "Iteration: 946; Percent done: 23.6%;                    Mean loss: 3.9393\n",
      "Iteration: 947; Percent done: 23.7%;                    Mean loss: 3.3957\n",
      "Iteration: 948; Percent done: 23.7%;                    Mean loss: 3.8590\n",
      "Iteration: 949; Percent done: 23.7%;                    Mean loss: 3.9303\n",
      "Iteration: 950; Percent done: 23.8%;                    Mean loss: 3.5718\n",
      "Iteration: 951; Percent done: 23.8%;                    Mean loss: 3.7143\n",
      "Iteration: 952; Percent done: 23.8%;                    Mean loss: 3.7652\n",
      "Iteration: 953; Percent done: 23.8%;                    Mean loss: 3.7019\n",
      "Iteration: 954; Percent done: 23.8%;                    Mean loss: 3.7607\n",
      "Iteration: 955; Percent done: 23.9%;                    Mean loss: 3.7307\n",
      "Iteration: 956; Percent done: 23.9%;                    Mean loss: 3.6106\n",
      "Iteration: 957; Percent done: 23.9%;                    Mean loss: 3.7207\n",
      "Iteration: 958; Percent done: 23.9%;                    Mean loss: 3.8465\n",
      "Iteration: 959; Percent done: 24.0%;                    Mean loss: 3.9623\n",
      "Iteration: 960; Percent done: 24.0%;                    Mean loss: 3.7548\n",
      "Iteration: 961; Percent done: 24.0%;                    Mean loss: 3.6736\n",
      "Iteration: 962; Percent done: 24.1%;                    Mean loss: 3.8670\n",
      "Iteration: 963; Percent done: 24.1%;                    Mean loss: 3.6758\n",
      "Iteration: 964; Percent done: 24.1%;                    Mean loss: 3.7325\n",
      "Iteration: 965; Percent done: 24.1%;                    Mean loss: 3.6410\n",
      "Iteration: 966; Percent done: 24.1%;                    Mean loss: 3.8588\n",
      "Iteration: 967; Percent done: 24.2%;                    Mean loss: 3.6917\n",
      "Iteration: 968; Percent done: 24.2%;                    Mean loss: 3.5658\n",
      "Iteration: 969; Percent done: 24.2%;                    Mean loss: 3.6489\n",
      "Iteration: 970; Percent done: 24.2%;                    Mean loss: 3.6418\n",
      "Iteration: 971; Percent done: 24.3%;                    Mean loss: 3.5078\n",
      "Iteration: 972; Percent done: 24.3%;                    Mean loss: 3.8035\n",
      "Iteration: 973; Percent done: 24.3%;                    Mean loss: 3.7553\n",
      "Iteration: 974; Percent done: 24.3%;                    Mean loss: 3.7698\n",
      "Iteration: 975; Percent done: 24.4%;                    Mean loss: 3.5377\n",
      "Iteration: 976; Percent done: 24.4%;                    Mean loss: 3.6869\n",
      "Iteration: 977; Percent done: 24.4%;                    Mean loss: 3.8366\n",
      "Iteration: 978; Percent done: 24.4%;                    Mean loss: 3.8582\n",
      "Iteration: 979; Percent done: 24.5%;                    Mean loss: 3.5002\n",
      "Iteration: 980; Percent done: 24.5%;                    Mean loss: 3.5534\n",
      "Iteration: 981; Percent done: 24.5%;                    Mean loss: 3.6196\n",
      "Iteration: 982; Percent done: 24.6%;                    Mean loss: 3.7568\n",
      "Iteration: 983; Percent done: 24.6%;                    Mean loss: 3.6426\n",
      "Iteration: 984; Percent done: 24.6%;                    Mean loss: 3.5939\n",
      "Iteration: 985; Percent done: 24.6%;                    Mean loss: 3.5752\n",
      "Iteration: 986; Percent done: 24.6%;                    Mean loss: 3.6593\n",
      "Iteration: 987; Percent done: 24.7%;                    Mean loss: 3.9740\n",
      "Iteration: 988; Percent done: 24.7%;                    Mean loss: 3.5634\n",
      "Iteration: 989; Percent done: 24.7%;                    Mean loss: 3.7649\n",
      "Iteration: 990; Percent done: 24.8%;                    Mean loss: 3.8007\n",
      "Iteration: 991; Percent done: 24.8%;                    Mean loss: 3.6574\n",
      "Iteration: 992; Percent done: 24.8%;                    Mean loss: 3.4783\n",
      "Iteration: 993; Percent done: 24.8%;                    Mean loss: 3.3765\n",
      "Iteration: 994; Percent done: 24.9%;                    Mean loss: 3.7225\n",
      "Iteration: 995; Percent done: 24.9%;                    Mean loss: 3.9448\n",
      "Iteration: 996; Percent done: 24.9%;                    Mean loss: 3.4792\n",
      "Iteration: 997; Percent done: 24.9%;                    Mean loss: 3.7390\n",
      "Iteration: 998; Percent done: 24.9%;                    Mean loss: 3.6343\n",
      "Iteration: 999; Percent done: 25.0%;                    Mean loss: 3.7195\n",
      "Iteration: 1000; Percent done: 25.0%;                    Mean loss: 3.7375\n",
      "Iteration: 1001; Percent done: 25.0%;                    Mean loss: 3.8804\n",
      "Iteration: 1002; Percent done: 25.1%;                    Mean loss: 3.6533\n",
      "Iteration: 1003; Percent done: 25.1%;                    Mean loss: 3.8431\n",
      "Iteration: 1004; Percent done: 25.1%;                    Mean loss: 3.6178\n",
      "Iteration: 1005; Percent done: 25.1%;                    Mean loss: 4.0373\n",
      "Iteration: 1006; Percent done: 25.1%;                    Mean loss: 3.6350\n",
      "Iteration: 1007; Percent done: 25.2%;                    Mean loss: 3.5903\n",
      "Iteration: 1008; Percent done: 25.2%;                    Mean loss: 3.8275\n",
      "Iteration: 1009; Percent done: 25.2%;                    Mean loss: 3.8135\n",
      "Iteration: 1010; Percent done: 25.2%;                    Mean loss: 3.6871\n",
      "Iteration: 1011; Percent done: 25.3%;                    Mean loss: 3.7242\n",
      "Iteration: 1012; Percent done: 25.3%;                    Mean loss: 3.7984\n",
      "Iteration: 1013; Percent done: 25.3%;                    Mean loss: 3.4977\n",
      "Iteration: 1014; Percent done: 25.4%;                    Mean loss: 3.5753\n",
      "Iteration: 1015; Percent done: 25.4%;                    Mean loss: 3.6165\n",
      "Iteration: 1016; Percent done: 25.4%;                    Mean loss: 3.5854\n",
      "Iteration: 1017; Percent done: 25.4%;                    Mean loss: 3.5974\n",
      "Iteration: 1018; Percent done: 25.4%;                    Mean loss: 3.5009\n",
      "Iteration: 1019; Percent done: 25.5%;                    Mean loss: 3.5348\n",
      "Iteration: 1020; Percent done: 25.5%;                    Mean loss: 3.6502\n",
      "Iteration: 1021; Percent done: 25.5%;                    Mean loss: 3.7146\n",
      "Iteration: 1022; Percent done: 25.6%;                    Mean loss: 3.5417\n",
      "Iteration: 1023; Percent done: 25.6%;                    Mean loss: 3.7169\n",
      "Iteration: 1024; Percent done: 25.6%;                    Mean loss: 3.5062\n",
      "Iteration: 1025; Percent done: 25.6%;                    Mean loss: 3.6700\n",
      "Iteration: 1026; Percent done: 25.7%;                    Mean loss: 3.9153\n",
      "Iteration: 1027; Percent done: 25.7%;                    Mean loss: 3.7344\n",
      "Iteration: 1028; Percent done: 25.7%;                    Mean loss: 3.5761\n",
      "Iteration: 1029; Percent done: 25.7%;                    Mean loss: 3.7770\n",
      "Iteration: 1030; Percent done: 25.8%;                    Mean loss: 3.6094\n",
      "Iteration: 1031; Percent done: 25.8%;                    Mean loss: 3.4477\n",
      "Iteration: 1032; Percent done: 25.8%;                    Mean loss: 3.4965\n",
      "Iteration: 1033; Percent done: 25.8%;                    Mean loss: 3.3911\n",
      "Iteration: 1034; Percent done: 25.9%;                    Mean loss: 3.7820\n",
      "Iteration: 1035; Percent done: 25.9%;                    Mean loss: 3.6639\n",
      "Iteration: 1036; Percent done: 25.9%;                    Mean loss: 3.7356\n",
      "Iteration: 1037; Percent done: 25.9%;                    Mean loss: 3.7293\n",
      "Iteration: 1038; Percent done: 25.9%;                    Mean loss: 3.6317\n",
      "Iteration: 1039; Percent done: 26.0%;                    Mean loss: 3.6674\n",
      "Iteration: 1040; Percent done: 26.0%;                    Mean loss: 3.3141\n",
      "Iteration: 1041; Percent done: 26.0%;                    Mean loss: 3.5661\n",
      "Iteration: 1042; Percent done: 26.1%;                    Mean loss: 3.7821\n",
      "Iteration: 1043; Percent done: 26.1%;                    Mean loss: 3.6793\n",
      "Iteration: 1044; Percent done: 26.1%;                    Mean loss: 3.8152\n",
      "Iteration: 1045; Percent done: 26.1%;                    Mean loss: 3.8882\n",
      "Iteration: 1046; Percent done: 26.2%;                    Mean loss: 3.8561\n",
      "Iteration: 1047; Percent done: 26.2%;                    Mean loss: 3.5670\n",
      "Iteration: 1048; Percent done: 26.2%;                    Mean loss: 3.9002\n",
      "Iteration: 1049; Percent done: 26.2%;                    Mean loss: 3.8832\n",
      "Iteration: 1050; Percent done: 26.2%;                    Mean loss: 3.5852\n",
      "Iteration: 1051; Percent done: 26.3%;                    Mean loss: 3.5455\n",
      "Iteration: 1052; Percent done: 26.3%;                    Mean loss: 3.5094\n",
      "Iteration: 1053; Percent done: 26.3%;                    Mean loss: 3.6942\n",
      "Iteration: 1054; Percent done: 26.4%;                    Mean loss: 3.7232\n",
      "Iteration: 1055; Percent done: 26.4%;                    Mean loss: 3.5982\n",
      "Iteration: 1056; Percent done: 26.4%;                    Mean loss: 3.5971\n",
      "Iteration: 1057; Percent done: 26.4%;                    Mean loss: 3.5821\n",
      "Iteration: 1058; Percent done: 26.5%;                    Mean loss: 3.5972\n",
      "Iteration: 1059; Percent done: 26.5%;                    Mean loss: 3.7774\n",
      "Iteration: 1060; Percent done: 26.5%;                    Mean loss: 3.7918\n",
      "Iteration: 1061; Percent done: 26.5%;                    Mean loss: 3.6770\n",
      "Iteration: 1062; Percent done: 26.6%;                    Mean loss: 3.5661\n",
      "Iteration: 1063; Percent done: 26.6%;                    Mean loss: 3.6263\n",
      "Iteration: 1064; Percent done: 26.6%;                    Mean loss: 3.6488\n",
      "Iteration: 1065; Percent done: 26.6%;                    Mean loss: 3.4766\n",
      "Iteration: 1066; Percent done: 26.7%;                    Mean loss: 3.2874\n",
      "Iteration: 1067; Percent done: 26.7%;                    Mean loss: 3.6708\n",
      "Iteration: 1068; Percent done: 26.7%;                    Mean loss: 4.0080\n",
      "Iteration: 1069; Percent done: 26.7%;                    Mean loss: 3.9044\n",
      "Iteration: 1070; Percent done: 26.8%;                    Mean loss: 3.6876\n",
      "Iteration: 1071; Percent done: 26.8%;                    Mean loss: 3.7339\n",
      "Iteration: 1072; Percent done: 26.8%;                    Mean loss: 3.7769\n",
      "Iteration: 1073; Percent done: 26.8%;                    Mean loss: 3.8131\n",
      "Iteration: 1074; Percent done: 26.9%;                    Mean loss: 3.7955\n",
      "Iteration: 1075; Percent done: 26.9%;                    Mean loss: 3.4946\n",
      "Iteration: 1076; Percent done: 26.9%;                    Mean loss: 3.6667\n",
      "Iteration: 1077; Percent done: 26.9%;                    Mean loss: 3.6887\n",
      "Iteration: 1078; Percent done: 27.0%;                    Mean loss: 3.4460\n",
      "Iteration: 1079; Percent done: 27.0%;                    Mean loss: 3.3225\n",
      "Iteration: 1080; Percent done: 27.0%;                    Mean loss: 3.6209\n",
      "Iteration: 1081; Percent done: 27.0%;                    Mean loss: 3.6234\n",
      "Iteration: 1082; Percent done: 27.1%;                    Mean loss: 3.7195\n",
      "Iteration: 1083; Percent done: 27.1%;                    Mean loss: 3.7061\n",
      "Iteration: 1084; Percent done: 27.1%;                    Mean loss: 4.0428\n",
      "Iteration: 1085; Percent done: 27.1%;                    Mean loss: 3.4037\n",
      "Iteration: 1086; Percent done: 27.2%;                    Mean loss: 3.6123\n",
      "Iteration: 1087; Percent done: 27.2%;                    Mean loss: 3.8126\n",
      "Iteration: 1088; Percent done: 27.2%;                    Mean loss: 3.5616\n",
      "Iteration: 1089; Percent done: 27.2%;                    Mean loss: 3.8019\n",
      "Iteration: 1090; Percent done: 27.3%;                    Mean loss: 3.6826\n",
      "Iteration: 1091; Percent done: 27.3%;                    Mean loss: 3.6670\n",
      "Iteration: 1092; Percent done: 27.3%;                    Mean loss: 3.7327\n",
      "Iteration: 1093; Percent done: 27.3%;                    Mean loss: 3.8321\n",
      "Iteration: 1094; Percent done: 27.4%;                    Mean loss: 3.6914\n",
      "Iteration: 1095; Percent done: 27.4%;                    Mean loss: 3.3631\n",
      "Iteration: 1096; Percent done: 27.4%;                    Mean loss: 3.4647\n",
      "Iteration: 1097; Percent done: 27.4%;                    Mean loss: 3.4705\n",
      "Iteration: 1098; Percent done: 27.5%;                    Mean loss: 3.6817\n",
      "Iteration: 1099; Percent done: 27.5%;                    Mean loss: 3.7839\n",
      "Iteration: 1100; Percent done: 27.5%;                    Mean loss: 3.5714\n",
      "Iteration: 1101; Percent done: 27.5%;                    Mean loss: 3.7267\n",
      "Iteration: 1102; Percent done: 27.6%;                    Mean loss: 3.6666\n",
      "Iteration: 1103; Percent done: 27.6%;                    Mean loss: 3.6637\n",
      "Iteration: 1104; Percent done: 27.6%;                    Mean loss: 3.7957\n",
      "Iteration: 1105; Percent done: 27.6%;                    Mean loss: 3.6290\n",
      "Iteration: 1106; Percent done: 27.7%;                    Mean loss: 3.6387\n",
      "Iteration: 1107; Percent done: 27.7%;                    Mean loss: 3.7988\n",
      "Iteration: 1108; Percent done: 27.7%;                    Mean loss: 3.9576\n",
      "Iteration: 1109; Percent done: 27.7%;                    Mean loss: 3.4703\n",
      "Iteration: 1110; Percent done: 27.8%;                    Mean loss: 3.6676\n",
      "Iteration: 1111; Percent done: 27.8%;                    Mean loss: 3.9680\n",
      "Iteration: 1112; Percent done: 27.8%;                    Mean loss: 3.5156\n",
      "Iteration: 1113; Percent done: 27.8%;                    Mean loss: 3.7700\n",
      "Iteration: 1114; Percent done: 27.9%;                    Mean loss: 3.7626\n",
      "Iteration: 1115; Percent done: 27.9%;                    Mean loss: 3.4908\n",
      "Iteration: 1116; Percent done: 27.9%;                    Mean loss: 3.4944\n",
      "Iteration: 1117; Percent done: 27.9%;                    Mean loss: 3.7859\n",
      "Iteration: 1118; Percent done: 28.0%;                    Mean loss: 3.6739\n",
      "Iteration: 1119; Percent done: 28.0%;                    Mean loss: 3.5122\n",
      "Iteration: 1120; Percent done: 28.0%;                    Mean loss: 3.5941\n",
      "Iteration: 1121; Percent done: 28.0%;                    Mean loss: 3.6612\n",
      "Iteration: 1122; Percent done: 28.1%;                    Mean loss: 3.5024\n",
      "Iteration: 1123; Percent done: 28.1%;                    Mean loss: 4.0350\n",
      "Iteration: 1124; Percent done: 28.1%;                    Mean loss: 3.6766\n",
      "Iteration: 1125; Percent done: 28.1%;                    Mean loss: 3.4339\n",
      "Iteration: 1126; Percent done: 28.1%;                    Mean loss: 3.4974\n",
      "Iteration: 1127; Percent done: 28.2%;                    Mean loss: 3.7345\n",
      "Iteration: 1128; Percent done: 28.2%;                    Mean loss: 3.5914\n",
      "Iteration: 1129; Percent done: 28.2%;                    Mean loss: 3.2148\n",
      "Iteration: 1130; Percent done: 28.2%;                    Mean loss: 3.4381\n",
      "Iteration: 1131; Percent done: 28.3%;                    Mean loss: 3.6886\n",
      "Iteration: 1132; Percent done: 28.3%;                    Mean loss: 3.8084\n",
      "Iteration: 1133; Percent done: 28.3%;                    Mean loss: 3.5488\n",
      "Iteration: 1134; Percent done: 28.3%;                    Mean loss: 3.5782\n",
      "Iteration: 1135; Percent done: 28.4%;                    Mean loss: 3.5733\n",
      "Iteration: 1136; Percent done: 28.4%;                    Mean loss: 3.3870\n",
      "Iteration: 1137; Percent done: 28.4%;                    Mean loss: 3.6598\n",
      "Iteration: 1138; Percent done: 28.4%;                    Mean loss: 3.4698\n",
      "Iteration: 1139; Percent done: 28.5%;                    Mean loss: 3.7483\n",
      "Iteration: 1140; Percent done: 28.5%;                    Mean loss: 3.8932\n",
      "Iteration: 1141; Percent done: 28.5%;                    Mean loss: 3.6568\n",
      "Iteration: 1142; Percent done: 28.5%;                    Mean loss: 3.5869\n",
      "Iteration: 1143; Percent done: 28.6%;                    Mean loss: 3.5616\n",
      "Iteration: 1144; Percent done: 28.6%;                    Mean loss: 3.4898\n",
      "Iteration: 1145; Percent done: 28.6%;                    Mean loss: 3.5526\n",
      "Iteration: 1146; Percent done: 28.6%;                    Mean loss: 3.5003\n",
      "Iteration: 1147; Percent done: 28.7%;                    Mean loss: 3.6049\n",
      "Iteration: 1148; Percent done: 28.7%;                    Mean loss: 3.5315\n",
      "Iteration: 1149; Percent done: 28.7%;                    Mean loss: 3.6582\n",
      "Iteration: 1150; Percent done: 28.7%;                    Mean loss: 3.6378\n",
      "Iteration: 1151; Percent done: 28.8%;                    Mean loss: 3.6088\n",
      "Iteration: 1152; Percent done: 28.8%;                    Mean loss: 3.4253\n",
      "Iteration: 1153; Percent done: 28.8%;                    Mean loss: 3.3397\n",
      "Iteration: 1154; Percent done: 28.8%;                    Mean loss: 3.6072\n",
      "Iteration: 1155; Percent done: 28.9%;                    Mean loss: 3.3870\n",
      "Iteration: 1156; Percent done: 28.9%;                    Mean loss: 3.4309\n",
      "Iteration: 1157; Percent done: 28.9%;                    Mean loss: 3.8783\n",
      "Iteration: 1158; Percent done: 28.9%;                    Mean loss: 3.3676\n",
      "Iteration: 1159; Percent done: 29.0%;                    Mean loss: 3.5300\n",
      "Iteration: 1160; Percent done: 29.0%;                    Mean loss: 3.5825\n",
      "Iteration: 1161; Percent done: 29.0%;                    Mean loss: 4.0011\n",
      "Iteration: 1162; Percent done: 29.0%;                    Mean loss: 3.7881\n",
      "Iteration: 1163; Percent done: 29.1%;                    Mean loss: 3.5577\n",
      "Iteration: 1164; Percent done: 29.1%;                    Mean loss: 3.6481\n",
      "Iteration: 1165; Percent done: 29.1%;                    Mean loss: 3.6290\n",
      "Iteration: 1166; Percent done: 29.1%;                    Mean loss: 3.2596\n",
      "Iteration: 1167; Percent done: 29.2%;                    Mean loss: 3.7063\n",
      "Iteration: 1168; Percent done: 29.2%;                    Mean loss: 3.6029\n",
      "Iteration: 1169; Percent done: 29.2%;                    Mean loss: 3.6235\n",
      "Iteration: 1170; Percent done: 29.2%;                    Mean loss: 3.7565\n",
      "Iteration: 1171; Percent done: 29.3%;                    Mean loss: 3.3680\n",
      "Iteration: 1172; Percent done: 29.3%;                    Mean loss: 3.7595\n",
      "Iteration: 1173; Percent done: 29.3%;                    Mean loss: 3.7221\n",
      "Iteration: 1174; Percent done: 29.3%;                    Mean loss: 3.6495\n",
      "Iteration: 1175; Percent done: 29.4%;                    Mean loss: 3.3666\n",
      "Iteration: 1176; Percent done: 29.4%;                    Mean loss: 3.5036\n",
      "Iteration: 1177; Percent done: 29.4%;                    Mean loss: 3.5367\n",
      "Iteration: 1178; Percent done: 29.4%;                    Mean loss: 3.4261\n",
      "Iteration: 1179; Percent done: 29.5%;                    Mean loss: 3.6602\n",
      "Iteration: 1180; Percent done: 29.5%;                    Mean loss: 3.6175\n",
      "Iteration: 1181; Percent done: 29.5%;                    Mean loss: 3.5171\n",
      "Iteration: 1182; Percent done: 29.5%;                    Mean loss: 3.3078\n",
      "Iteration: 1183; Percent done: 29.6%;                    Mean loss: 3.1911\n",
      "Iteration: 1184; Percent done: 29.6%;                    Mean loss: 3.6271\n",
      "Iteration: 1185; Percent done: 29.6%;                    Mean loss: 3.7838\n",
      "Iteration: 1186; Percent done: 29.6%;                    Mean loss: 3.6031\n",
      "Iteration: 1187; Percent done: 29.7%;                    Mean loss: 3.4731\n",
      "Iteration: 1188; Percent done: 29.7%;                    Mean loss: 3.7008\n",
      "Iteration: 1189; Percent done: 29.7%;                    Mean loss: 3.6009\n",
      "Iteration: 1190; Percent done: 29.8%;                    Mean loss: 3.4688\n",
      "Iteration: 1191; Percent done: 29.8%;                    Mean loss: 3.6908\n",
      "Iteration: 1192; Percent done: 29.8%;                    Mean loss: 3.5559\n",
      "Iteration: 1193; Percent done: 29.8%;                    Mean loss: 3.6141\n",
      "Iteration: 1194; Percent done: 29.8%;                    Mean loss: 3.5267\n",
      "Iteration: 1195; Percent done: 29.9%;                    Mean loss: 3.4328\n",
      "Iteration: 1196; Percent done: 29.9%;                    Mean loss: 3.6063\n",
      "Iteration: 1197; Percent done: 29.9%;                    Mean loss: 3.2955\n",
      "Iteration: 1198; Percent done: 29.9%;                    Mean loss: 3.7394\n",
      "Iteration: 1199; Percent done: 30.0%;                    Mean loss: 3.5403\n",
      "Iteration: 1200; Percent done: 30.0%;                    Mean loss: 3.3968\n",
      "Iteration: 1201; Percent done: 30.0%;                    Mean loss: 3.4335\n",
      "Iteration: 1202; Percent done: 30.0%;                    Mean loss: 3.4372\n",
      "Iteration: 1203; Percent done: 30.1%;                    Mean loss: 3.8304\n",
      "Iteration: 1204; Percent done: 30.1%;                    Mean loss: 3.4046\n",
      "Iteration: 1205; Percent done: 30.1%;                    Mean loss: 3.5520\n",
      "Iteration: 1206; Percent done: 30.1%;                    Mean loss: 3.5463\n",
      "Iteration: 1207; Percent done: 30.2%;                    Mean loss: 3.5982\n",
      "Iteration: 1208; Percent done: 30.2%;                    Mean loss: 3.6579\n",
      "Iteration: 1209; Percent done: 30.2%;                    Mean loss: 3.7221\n",
      "Iteration: 1210; Percent done: 30.2%;                    Mean loss: 3.5892\n",
      "Iteration: 1211; Percent done: 30.3%;                    Mean loss: 3.4735\n",
      "Iteration: 1212; Percent done: 30.3%;                    Mean loss: 3.5225\n",
      "Iteration: 1213; Percent done: 30.3%;                    Mean loss: 3.7136\n",
      "Iteration: 1214; Percent done: 30.3%;                    Mean loss: 3.6959\n",
      "Iteration: 1215; Percent done: 30.4%;                    Mean loss: 3.4580\n",
      "Iteration: 1216; Percent done: 30.4%;                    Mean loss: 3.5807\n",
      "Iteration: 1217; Percent done: 30.4%;                    Mean loss: 3.7976\n",
      "Iteration: 1218; Percent done: 30.4%;                    Mean loss: 3.6207\n",
      "Iteration: 1219; Percent done: 30.5%;                    Mean loss: 3.7252\n",
      "Iteration: 1220; Percent done: 30.5%;                    Mean loss: 3.4622\n",
      "Iteration: 1221; Percent done: 30.5%;                    Mean loss: 3.4430\n",
      "Iteration: 1222; Percent done: 30.6%;                    Mean loss: 3.4547\n",
      "Iteration: 1223; Percent done: 30.6%;                    Mean loss: 3.7391\n",
      "Iteration: 1224; Percent done: 30.6%;                    Mean loss: 3.6062\n",
      "Iteration: 1225; Percent done: 30.6%;                    Mean loss: 3.5474\n",
      "Iteration: 1226; Percent done: 30.6%;                    Mean loss: 3.6662\n",
      "Iteration: 1227; Percent done: 30.7%;                    Mean loss: 3.5494\n",
      "Iteration: 1228; Percent done: 30.7%;                    Mean loss: 3.3645\n",
      "Iteration: 1229; Percent done: 30.7%;                    Mean loss: 3.5516\n",
      "Iteration: 1230; Percent done: 30.8%;                    Mean loss: 3.5822\n",
      "Iteration: 1231; Percent done: 30.8%;                    Mean loss: 3.4094\n",
      "Iteration: 1232; Percent done: 30.8%;                    Mean loss: 3.5159\n",
      "Iteration: 1233; Percent done: 30.8%;                    Mean loss: 3.6597\n",
      "Iteration: 1234; Percent done: 30.9%;                    Mean loss: 3.7717\n",
      "Iteration: 1235; Percent done: 30.9%;                    Mean loss: 3.4349\n",
      "Iteration: 1236; Percent done: 30.9%;                    Mean loss: 3.5167\n",
      "Iteration: 1237; Percent done: 30.9%;                    Mean loss: 3.4473\n",
      "Iteration: 1238; Percent done: 30.9%;                    Mean loss: 3.7315\n",
      "Iteration: 1239; Percent done: 31.0%;                    Mean loss: 3.2146\n",
      "Iteration: 1240; Percent done: 31.0%;                    Mean loss: 3.3943\n",
      "Iteration: 1241; Percent done: 31.0%;                    Mean loss: 3.3151\n",
      "Iteration: 1242; Percent done: 31.1%;                    Mean loss: 3.6323\n",
      "Iteration: 1243; Percent done: 31.1%;                    Mean loss: 3.5042\n",
      "Iteration: 1244; Percent done: 31.1%;                    Mean loss: 3.2554\n",
      "Iteration: 1245; Percent done: 31.1%;                    Mean loss: 3.8370\n",
      "Iteration: 1246; Percent done: 31.1%;                    Mean loss: 3.7457\n",
      "Iteration: 1247; Percent done: 31.2%;                    Mean loss: 3.5063\n",
      "Iteration: 1248; Percent done: 31.2%;                    Mean loss: 3.6396\n",
      "Iteration: 1249; Percent done: 31.2%;                    Mean loss: 3.5890\n",
      "Iteration: 1250; Percent done: 31.2%;                    Mean loss: 3.6131\n",
      "Iteration: 1251; Percent done: 31.3%;                    Mean loss: 3.7012\n",
      "Iteration: 1252; Percent done: 31.3%;                    Mean loss: 3.6950\n",
      "Iteration: 1253; Percent done: 31.3%;                    Mean loss: 3.8805\n",
      "Iteration: 1254; Percent done: 31.4%;                    Mean loss: 3.8203\n",
      "Iteration: 1255; Percent done: 31.4%;                    Mean loss: 3.5874\n",
      "Iteration: 1256; Percent done: 31.4%;                    Mean loss: 3.6360\n",
      "Iteration: 1257; Percent done: 31.4%;                    Mean loss: 3.5128\n",
      "Iteration: 1258; Percent done: 31.4%;                    Mean loss: 3.6568\n",
      "Iteration: 1259; Percent done: 31.5%;                    Mean loss: 3.6405\n",
      "Iteration: 1260; Percent done: 31.5%;                    Mean loss: 3.8443\n",
      "Iteration: 1261; Percent done: 31.5%;                    Mean loss: 3.9087\n",
      "Iteration: 1262; Percent done: 31.6%;                    Mean loss: 3.6857\n",
      "Iteration: 1263; Percent done: 31.6%;                    Mean loss: 3.7630\n",
      "Iteration: 1264; Percent done: 31.6%;                    Mean loss: 3.6011\n",
      "Iteration: 1265; Percent done: 31.6%;                    Mean loss: 3.4873\n",
      "Iteration: 1266; Percent done: 31.6%;                    Mean loss: 3.4364\n",
      "Iteration: 1267; Percent done: 31.7%;                    Mean loss: 3.3866\n",
      "Iteration: 1268; Percent done: 31.7%;                    Mean loss: 3.4828\n",
      "Iteration: 1269; Percent done: 31.7%;                    Mean loss: 3.9292\n",
      "Iteration: 1270; Percent done: 31.8%;                    Mean loss: 3.3175\n",
      "Iteration: 1271; Percent done: 31.8%;                    Mean loss: 3.7791\n",
      "Iteration: 1272; Percent done: 31.8%;                    Mean loss: 3.6423\n",
      "Iteration: 1273; Percent done: 31.8%;                    Mean loss: 3.5591\n",
      "Iteration: 1274; Percent done: 31.9%;                    Mean loss: 3.5594\n",
      "Iteration: 1275; Percent done: 31.9%;                    Mean loss: 3.8061\n",
      "Iteration: 1276; Percent done: 31.9%;                    Mean loss: 3.5874\n",
      "Iteration: 1277; Percent done: 31.9%;                    Mean loss: 3.4386\n",
      "Iteration: 1278; Percent done: 31.9%;                    Mean loss: 3.5968\n",
      "Iteration: 1279; Percent done: 32.0%;                    Mean loss: 3.5753\n",
      "Iteration: 1280; Percent done: 32.0%;                    Mean loss: 3.8575\n",
      "Iteration: 1281; Percent done: 32.0%;                    Mean loss: 3.7628\n",
      "Iteration: 1282; Percent done: 32.0%;                    Mean loss: 3.3682\n",
      "Iteration: 1283; Percent done: 32.1%;                    Mean loss: 3.5438\n",
      "Iteration: 1284; Percent done: 32.1%;                    Mean loss: 3.2731\n",
      "Iteration: 1285; Percent done: 32.1%;                    Mean loss: 3.3969\n",
      "Iteration: 1286; Percent done: 32.1%;                    Mean loss: 3.6021\n",
      "Iteration: 1287; Percent done: 32.2%;                    Mean loss: 3.5433\n",
      "Iteration: 1288; Percent done: 32.2%;                    Mean loss: 3.6025\n",
      "Iteration: 1289; Percent done: 32.2%;                    Mean loss: 3.5504\n",
      "Iteration: 1290; Percent done: 32.2%;                    Mean loss: 3.4593\n",
      "Iteration: 1291; Percent done: 32.3%;                    Mean loss: 3.3961\n",
      "Iteration: 1292; Percent done: 32.3%;                    Mean loss: 3.5191\n",
      "Iteration: 1293; Percent done: 32.3%;                    Mean loss: 3.4178\n",
      "Iteration: 1294; Percent done: 32.4%;                    Mean loss: 3.6593\n",
      "Iteration: 1295; Percent done: 32.4%;                    Mean loss: 3.5309\n",
      "Iteration: 1296; Percent done: 32.4%;                    Mean loss: 3.4645\n",
      "Iteration: 1297; Percent done: 32.4%;                    Mean loss: 3.6543\n",
      "Iteration: 1298; Percent done: 32.5%;                    Mean loss: 3.4717\n",
      "Iteration: 1299; Percent done: 32.5%;                    Mean loss: 3.3450\n",
      "Iteration: 1300; Percent done: 32.5%;                    Mean loss: 3.6963\n",
      "Iteration: 1301; Percent done: 32.5%;                    Mean loss: 3.5136\n",
      "Iteration: 1302; Percent done: 32.6%;                    Mean loss: 3.3017\n",
      "Iteration: 1303; Percent done: 32.6%;                    Mean loss: 3.5618\n",
      "Iteration: 1304; Percent done: 32.6%;                    Mean loss: 3.5308\n",
      "Iteration: 1305; Percent done: 32.6%;                    Mean loss: 3.7078\n",
      "Iteration: 1306; Percent done: 32.6%;                    Mean loss: 3.7602\n",
      "Iteration: 1307; Percent done: 32.7%;                    Mean loss: 3.4599\n",
      "Iteration: 1308; Percent done: 32.7%;                    Mean loss: 3.4705\n",
      "Iteration: 1309; Percent done: 32.7%;                    Mean loss: 3.3745\n",
      "Iteration: 1310; Percent done: 32.8%;                    Mean loss: 3.4562\n",
      "Iteration: 1311; Percent done: 32.8%;                    Mean loss: 3.4977\n",
      "Iteration: 1312; Percent done: 32.8%;                    Mean loss: 3.6677\n",
      "Iteration: 1313; Percent done: 32.8%;                    Mean loss: 3.6561\n",
      "Iteration: 1314; Percent done: 32.9%;                    Mean loss: 3.3852\n",
      "Iteration: 1315; Percent done: 32.9%;                    Mean loss: 3.7833\n",
      "Iteration: 1316; Percent done: 32.9%;                    Mean loss: 3.7581\n",
      "Iteration: 1317; Percent done: 32.9%;                    Mean loss: 3.5235\n",
      "Iteration: 1318; Percent done: 33.0%;                    Mean loss: 3.6072\n",
      "Iteration: 1319; Percent done: 33.0%;                    Mean loss: 3.4554\n",
      "Iteration: 1320; Percent done: 33.0%;                    Mean loss: 3.3889\n",
      "Iteration: 1321; Percent done: 33.0%;                    Mean loss: 3.4605\n",
      "Iteration: 1322; Percent done: 33.1%;                    Mean loss: 3.1053\n",
      "Iteration: 1323; Percent done: 33.1%;                    Mean loss: 3.7468\n",
      "Iteration: 1324; Percent done: 33.1%;                    Mean loss: 3.4296\n",
      "Iteration: 1325; Percent done: 33.1%;                    Mean loss: 3.5934\n",
      "Iteration: 1326; Percent done: 33.1%;                    Mean loss: 3.7588\n",
      "Iteration: 1327; Percent done: 33.2%;                    Mean loss: 3.6259\n",
      "Iteration: 1328; Percent done: 33.2%;                    Mean loss: 3.6887\n",
      "Iteration: 1329; Percent done: 33.2%;                    Mean loss: 3.5020\n",
      "Iteration: 1330; Percent done: 33.2%;                    Mean loss: 3.4240\n",
      "Iteration: 1331; Percent done: 33.3%;                    Mean loss: 3.5697\n",
      "Iteration: 1332; Percent done: 33.3%;                    Mean loss: 3.5950\n",
      "Iteration: 1333; Percent done: 33.3%;                    Mean loss: 3.7287\n",
      "Iteration: 1334; Percent done: 33.4%;                    Mean loss: 3.6309\n",
      "Iteration: 1335; Percent done: 33.4%;                    Mean loss: 3.6489\n",
      "Iteration: 1336; Percent done: 33.4%;                    Mean loss: 3.4450\n",
      "Iteration: 1337; Percent done: 33.4%;                    Mean loss: 3.6202\n",
      "Iteration: 1338; Percent done: 33.5%;                    Mean loss: 3.5076\n",
      "Iteration: 1339; Percent done: 33.5%;                    Mean loss: 3.5131\n",
      "Iteration: 1340; Percent done: 33.5%;                    Mean loss: 3.4880\n",
      "Iteration: 1341; Percent done: 33.5%;                    Mean loss: 3.6446\n",
      "Iteration: 1342; Percent done: 33.6%;                    Mean loss: 3.5783\n",
      "Iteration: 1343; Percent done: 33.6%;                    Mean loss: 3.5489\n",
      "Iteration: 1344; Percent done: 33.6%;                    Mean loss: 3.3964\n",
      "Iteration: 1345; Percent done: 33.6%;                    Mean loss: 3.3303\n",
      "Iteration: 1346; Percent done: 33.7%;                    Mean loss: 3.5431\n",
      "Iteration: 1347; Percent done: 33.7%;                    Mean loss: 3.5416\n",
      "Iteration: 1348; Percent done: 33.7%;                    Mean loss: 3.4890\n",
      "Iteration: 1349; Percent done: 33.7%;                    Mean loss: 3.5707\n",
      "Iteration: 1350; Percent done: 33.8%;                    Mean loss: 3.4552\n",
      "Iteration: 1351; Percent done: 33.8%;                    Mean loss: 3.7489\n",
      "Iteration: 1352; Percent done: 33.8%;                    Mean loss: 3.7811\n",
      "Iteration: 1353; Percent done: 33.8%;                    Mean loss: 3.7337\n",
      "Iteration: 1354; Percent done: 33.9%;                    Mean loss: 3.6698\n",
      "Iteration: 1355; Percent done: 33.9%;                    Mean loss: 3.5196\n",
      "Iteration: 1356; Percent done: 33.9%;                    Mean loss: 3.4538\n",
      "Iteration: 1357; Percent done: 33.9%;                    Mean loss: 3.6805\n",
      "Iteration: 1358; Percent done: 34.0%;                    Mean loss: 3.6097\n",
      "Iteration: 1359; Percent done: 34.0%;                    Mean loss: 3.7937\n",
      "Iteration: 1360; Percent done: 34.0%;                    Mean loss: 3.5505\n",
      "Iteration: 1361; Percent done: 34.0%;                    Mean loss: 3.5654\n",
      "Iteration: 1362; Percent done: 34.1%;                    Mean loss: 3.4348\n",
      "Iteration: 1363; Percent done: 34.1%;                    Mean loss: 3.5777\n",
      "Iteration: 1364; Percent done: 34.1%;                    Mean loss: 3.6551\n",
      "Iteration: 1365; Percent done: 34.1%;                    Mean loss: 3.5939\n",
      "Iteration: 1366; Percent done: 34.2%;                    Mean loss: 3.6847\n",
      "Iteration: 1367; Percent done: 34.2%;                    Mean loss: 3.2762\n",
      "Iteration: 1368; Percent done: 34.2%;                    Mean loss: 3.8186\n",
      "Iteration: 1369; Percent done: 34.2%;                    Mean loss: 3.5783\n",
      "Iteration: 1370; Percent done: 34.2%;                    Mean loss: 3.3222\n",
      "Iteration: 1371; Percent done: 34.3%;                    Mean loss: 3.4089\n",
      "Iteration: 1372; Percent done: 34.3%;                    Mean loss: 3.6214\n",
      "Iteration: 1373; Percent done: 34.3%;                    Mean loss: 3.5854\n",
      "Iteration: 1374; Percent done: 34.4%;                    Mean loss: 3.6157\n",
      "Iteration: 1375; Percent done: 34.4%;                    Mean loss: 3.5433\n",
      "Iteration: 1376; Percent done: 34.4%;                    Mean loss: 3.7158\n",
      "Iteration: 1377; Percent done: 34.4%;                    Mean loss: 3.7276\n",
      "Iteration: 1378; Percent done: 34.4%;                    Mean loss: 3.5513\n",
      "Iteration: 1379; Percent done: 34.5%;                    Mean loss: 3.6689\n",
      "Iteration: 1380; Percent done: 34.5%;                    Mean loss: 3.8716\n",
      "Iteration: 1381; Percent done: 34.5%;                    Mean loss: 3.4850\n",
      "Iteration: 1382; Percent done: 34.5%;                    Mean loss: 3.4651\n",
      "Iteration: 1383; Percent done: 34.6%;                    Mean loss: 3.4983\n",
      "Iteration: 1384; Percent done: 34.6%;                    Mean loss: 3.3480\n",
      "Iteration: 1385; Percent done: 34.6%;                    Mean loss: 3.4572\n",
      "Iteration: 1386; Percent done: 34.6%;                    Mean loss: 3.5498\n",
      "Iteration: 1387; Percent done: 34.7%;                    Mean loss: 3.6291\n",
      "Iteration: 1388; Percent done: 34.7%;                    Mean loss: 3.3957\n",
      "Iteration: 1389; Percent done: 34.7%;                    Mean loss: 3.4419\n",
      "Iteration: 1390; Percent done: 34.8%;                    Mean loss: 3.5126\n",
      "Iteration: 1391; Percent done: 34.8%;                    Mean loss: 3.5723\n",
      "Iteration: 1392; Percent done: 34.8%;                    Mean loss: 3.4661\n",
      "Iteration: 1393; Percent done: 34.8%;                    Mean loss: 3.3248\n",
      "Iteration: 1394; Percent done: 34.8%;                    Mean loss: 3.3492\n",
      "Iteration: 1395; Percent done: 34.9%;                    Mean loss: 3.3314\n",
      "Iteration: 1396; Percent done: 34.9%;                    Mean loss: 3.3407\n",
      "Iteration: 1397; Percent done: 34.9%;                    Mean loss: 3.5425\n",
      "Iteration: 1398; Percent done: 34.9%;                    Mean loss: 3.5712\n",
      "Iteration: 1399; Percent done: 35.0%;                    Mean loss: 3.4390\n",
      "Iteration: 1400; Percent done: 35.0%;                    Mean loss: 3.7492\n",
      "Iteration: 1401; Percent done: 35.0%;                    Mean loss: 3.6361\n",
      "Iteration: 1402; Percent done: 35.0%;                    Mean loss: 3.5033\n",
      "Iteration: 1403; Percent done: 35.1%;                    Mean loss: 3.3725\n",
      "Iteration: 1404; Percent done: 35.1%;                    Mean loss: 3.6009\n",
      "Iteration: 1405; Percent done: 35.1%;                    Mean loss: 3.6518\n",
      "Iteration: 1406; Percent done: 35.1%;                    Mean loss: 3.4291\n",
      "Iteration: 1407; Percent done: 35.2%;                    Mean loss: 3.5491\n",
      "Iteration: 1408; Percent done: 35.2%;                    Mean loss: 3.3158\n",
      "Iteration: 1409; Percent done: 35.2%;                    Mean loss: 3.4464\n",
      "Iteration: 1410; Percent done: 35.2%;                    Mean loss: 3.6172\n",
      "Iteration: 1411; Percent done: 35.3%;                    Mean loss: 3.8765\n",
      "Iteration: 1412; Percent done: 35.3%;                    Mean loss: 3.2882\n",
      "Iteration: 1413; Percent done: 35.3%;                    Mean loss: 3.5438\n",
      "Iteration: 1414; Percent done: 35.4%;                    Mean loss: 3.6454\n",
      "Iteration: 1415; Percent done: 35.4%;                    Mean loss: 3.4040\n",
      "Iteration: 1416; Percent done: 35.4%;                    Mean loss: 3.3559\n",
      "Iteration: 1417; Percent done: 35.4%;                    Mean loss: 3.3802\n",
      "Iteration: 1418; Percent done: 35.4%;                    Mean loss: 3.4823\n",
      "Iteration: 1419; Percent done: 35.5%;                    Mean loss: 3.4588\n",
      "Iteration: 1420; Percent done: 35.5%;                    Mean loss: 3.4274\n",
      "Iteration: 1421; Percent done: 35.5%;                    Mean loss: 3.5553\n",
      "Iteration: 1422; Percent done: 35.5%;                    Mean loss: 3.5597\n",
      "Iteration: 1423; Percent done: 35.6%;                    Mean loss: 3.6817\n",
      "Iteration: 1424; Percent done: 35.6%;                    Mean loss: 3.6697\n",
      "Iteration: 1425; Percent done: 35.6%;                    Mean loss: 3.5726\n",
      "Iteration: 1426; Percent done: 35.6%;                    Mean loss: 3.5917\n",
      "Iteration: 1427; Percent done: 35.7%;                    Mean loss: 3.4379\n",
      "Iteration: 1428; Percent done: 35.7%;                    Mean loss: 3.6422\n",
      "Iteration: 1429; Percent done: 35.7%;                    Mean loss: 3.7915\n",
      "Iteration: 1430; Percent done: 35.8%;                    Mean loss: 3.4627\n",
      "Iteration: 1431; Percent done: 35.8%;                    Mean loss: 3.5807\n",
      "Iteration: 1432; Percent done: 35.8%;                    Mean loss: 3.3737\n",
      "Iteration: 1433; Percent done: 35.8%;                    Mean loss: 3.6691\n",
      "Iteration: 1434; Percent done: 35.9%;                    Mean loss: 3.3739\n",
      "Iteration: 1435; Percent done: 35.9%;                    Mean loss: 3.7474\n",
      "Iteration: 1436; Percent done: 35.9%;                    Mean loss: 3.8745\n",
      "Iteration: 1437; Percent done: 35.9%;                    Mean loss: 3.5393\n",
      "Iteration: 1438; Percent done: 35.9%;                    Mean loss: 3.5777\n",
      "Iteration: 1439; Percent done: 36.0%;                    Mean loss: 3.5024\n",
      "Iteration: 1440; Percent done: 36.0%;                    Mean loss: 3.4249\n",
      "Iteration: 1441; Percent done: 36.0%;                    Mean loss: 3.7186\n",
      "Iteration: 1442; Percent done: 36.0%;                    Mean loss: 3.6583\n",
      "Iteration: 1443; Percent done: 36.1%;                    Mean loss: 3.6764\n",
      "Iteration: 1444; Percent done: 36.1%;                    Mean loss: 3.4522\n",
      "Iteration: 1445; Percent done: 36.1%;                    Mean loss: 3.4414\n",
      "Iteration: 1446; Percent done: 36.1%;                    Mean loss: 3.6676\n",
      "Iteration: 1447; Percent done: 36.2%;                    Mean loss: 3.4075\n",
      "Iteration: 1448; Percent done: 36.2%;                    Mean loss: 3.6570\n",
      "Iteration: 1449; Percent done: 36.2%;                    Mean loss: 3.6411\n",
      "Iteration: 1450; Percent done: 36.2%;                    Mean loss: 3.5464\n",
      "Iteration: 1451; Percent done: 36.3%;                    Mean loss: 3.5793\n",
      "Iteration: 1452; Percent done: 36.3%;                    Mean loss: 3.3913\n",
      "Iteration: 1453; Percent done: 36.3%;                    Mean loss: 3.4514\n",
      "Iteration: 1454; Percent done: 36.4%;                    Mean loss: 3.4540\n",
      "Iteration: 1455; Percent done: 36.4%;                    Mean loss: 3.3777\n",
      "Iteration: 1456; Percent done: 36.4%;                    Mean loss: 3.7104\n",
      "Iteration: 1457; Percent done: 36.4%;                    Mean loss: 3.2867\n",
      "Iteration: 1458; Percent done: 36.4%;                    Mean loss: 3.2863\n",
      "Iteration: 1459; Percent done: 36.5%;                    Mean loss: 3.3574\n",
      "Iteration: 1460; Percent done: 36.5%;                    Mean loss: 3.4208\n",
      "Iteration: 1461; Percent done: 36.5%;                    Mean loss: 3.4459\n",
      "Iteration: 1462; Percent done: 36.5%;                    Mean loss: 3.4668\n",
      "Iteration: 1463; Percent done: 36.6%;                    Mean loss: 3.5766\n",
      "Iteration: 1464; Percent done: 36.6%;                    Mean loss: 3.3995\n",
      "Iteration: 1465; Percent done: 36.6%;                    Mean loss: 3.4561\n",
      "Iteration: 1466; Percent done: 36.6%;                    Mean loss: 3.4309\n",
      "Iteration: 1467; Percent done: 36.7%;                    Mean loss: 3.5950\n",
      "Iteration: 1468; Percent done: 36.7%;                    Mean loss: 3.5549\n",
      "Iteration: 1469; Percent done: 36.7%;                    Mean loss: 3.7510\n",
      "Iteration: 1470; Percent done: 36.8%;                    Mean loss: 3.4328\n",
      "Iteration: 1471; Percent done: 36.8%;                    Mean loss: 3.7003\n",
      "Iteration: 1472; Percent done: 36.8%;                    Mean loss: 3.6271\n",
      "Iteration: 1473; Percent done: 36.8%;                    Mean loss: 3.5226\n",
      "Iteration: 1474; Percent done: 36.9%;                    Mean loss: 3.5447\n",
      "Iteration: 1475; Percent done: 36.9%;                    Mean loss: 3.5038\n",
      "Iteration: 1476; Percent done: 36.9%;                    Mean loss: 3.4015\n",
      "Iteration: 1477; Percent done: 36.9%;                    Mean loss: 3.5263\n",
      "Iteration: 1478; Percent done: 37.0%;                    Mean loss: 3.2033\n",
      "Iteration: 1479; Percent done: 37.0%;                    Mean loss: 3.5814\n",
      "Iteration: 1480; Percent done: 37.0%;                    Mean loss: 3.4670\n",
      "Iteration: 1481; Percent done: 37.0%;                    Mean loss: 3.4694\n",
      "Iteration: 1482; Percent done: 37.0%;                    Mean loss: 3.7168\n",
      "Iteration: 1483; Percent done: 37.1%;                    Mean loss: 3.7282\n",
      "Iteration: 1484; Percent done: 37.1%;                    Mean loss: 3.8740\n",
      "Iteration: 1485; Percent done: 37.1%;                    Mean loss: 3.6201\n",
      "Iteration: 1486; Percent done: 37.1%;                    Mean loss: 3.5202\n",
      "Iteration: 1487; Percent done: 37.2%;                    Mean loss: 3.4799\n",
      "Iteration: 1488; Percent done: 37.2%;                    Mean loss: 3.5655\n",
      "Iteration: 1489; Percent done: 37.2%;                    Mean loss: 3.6369\n",
      "Iteration: 1490; Percent done: 37.2%;                    Mean loss: 3.5433\n",
      "Iteration: 1491; Percent done: 37.3%;                    Mean loss: 3.3319\n",
      "Iteration: 1492; Percent done: 37.3%;                    Mean loss: 3.4208\n",
      "Iteration: 1493; Percent done: 37.3%;                    Mean loss: 3.5636\n",
      "Iteration: 1494; Percent done: 37.4%;                    Mean loss: 3.4642\n",
      "Iteration: 1495; Percent done: 37.4%;                    Mean loss: 3.5124\n",
      "Iteration: 1496; Percent done: 37.4%;                    Mean loss: 3.5408\n",
      "Iteration: 1497; Percent done: 37.4%;                    Mean loss: 3.5182\n",
      "Iteration: 1498; Percent done: 37.5%;                    Mean loss: 3.5415\n",
      "Iteration: 1499; Percent done: 37.5%;                    Mean loss: 3.4723\n",
      "Iteration: 1500; Percent done: 37.5%;                    Mean loss: 3.3355\n",
      "Iteration: 1501; Percent done: 37.5%;                    Mean loss: 3.2386\n",
      "Iteration: 1502; Percent done: 37.5%;                    Mean loss: 3.6393\n",
      "Iteration: 1503; Percent done: 37.6%;                    Mean loss: 3.6921\n",
      "Iteration: 1504; Percent done: 37.6%;                    Mean loss: 3.6830\n",
      "Iteration: 1505; Percent done: 37.6%;                    Mean loss: 3.6724\n",
      "Iteration: 1506; Percent done: 37.6%;                    Mean loss: 3.6686\n",
      "Iteration: 1507; Percent done: 37.7%;                    Mean loss: 3.4369\n",
      "Iteration: 1508; Percent done: 37.7%;                    Mean loss: 3.1924\n",
      "Iteration: 1509; Percent done: 37.7%;                    Mean loss: 3.4518\n",
      "Iteration: 1510; Percent done: 37.8%;                    Mean loss: 3.7354\n",
      "Iteration: 1511; Percent done: 37.8%;                    Mean loss: 3.6381\n",
      "Iteration: 1512; Percent done: 37.8%;                    Mean loss: 3.4759\n",
      "Iteration: 1513; Percent done: 37.8%;                    Mean loss: 3.4919\n",
      "Iteration: 1514; Percent done: 37.9%;                    Mean loss: 3.4910\n",
      "Iteration: 1515; Percent done: 37.9%;                    Mean loss: 3.5112\n",
      "Iteration: 1516; Percent done: 37.9%;                    Mean loss: 3.6417\n",
      "Iteration: 1517; Percent done: 37.9%;                    Mean loss: 3.3911\n",
      "Iteration: 1518; Percent done: 38.0%;                    Mean loss: 3.1216\n",
      "Iteration: 1519; Percent done: 38.0%;                    Mean loss: 3.5419\n",
      "Iteration: 1520; Percent done: 38.0%;                    Mean loss: 3.4205\n",
      "Iteration: 1521; Percent done: 38.0%;                    Mean loss: 3.7077\n",
      "Iteration: 1522; Percent done: 38.0%;                    Mean loss: 3.4470\n",
      "Iteration: 1523; Percent done: 38.1%;                    Mean loss: 3.6013\n",
      "Iteration: 1524; Percent done: 38.1%;                    Mean loss: 3.6925\n",
      "Iteration: 1525; Percent done: 38.1%;                    Mean loss: 3.5217\n",
      "Iteration: 1526; Percent done: 38.1%;                    Mean loss: 3.1334\n",
      "Iteration: 1527; Percent done: 38.2%;                    Mean loss: 3.9359\n",
      "Iteration: 1528; Percent done: 38.2%;                    Mean loss: 3.4847\n",
      "Iteration: 1529; Percent done: 38.2%;                    Mean loss: 3.4653\n",
      "Iteration: 1530; Percent done: 38.2%;                    Mean loss: 3.3566\n",
      "Iteration: 1531; Percent done: 38.3%;                    Mean loss: 3.2676\n",
      "Iteration: 1532; Percent done: 38.3%;                    Mean loss: 3.3635\n",
      "Iteration: 1533; Percent done: 38.3%;                    Mean loss: 3.3809\n",
      "Iteration: 1534; Percent done: 38.4%;                    Mean loss: 3.3387\n",
      "Iteration: 1535; Percent done: 38.4%;                    Mean loss: 3.4270\n",
      "Iteration: 1536; Percent done: 38.4%;                    Mean loss: 3.3539\n",
      "Iteration: 1537; Percent done: 38.4%;                    Mean loss: 3.3310\n",
      "Iteration: 1538; Percent done: 38.5%;                    Mean loss: 3.3938\n",
      "Iteration: 1539; Percent done: 38.5%;                    Mean loss: 3.5611\n",
      "Iteration: 1540; Percent done: 38.5%;                    Mean loss: 3.5169\n",
      "Iteration: 1541; Percent done: 38.5%;                    Mean loss: 3.4693\n",
      "Iteration: 1542; Percent done: 38.6%;                    Mean loss: 3.2623\n",
      "Iteration: 1543; Percent done: 38.6%;                    Mean loss: 3.5708\n",
      "Iteration: 1544; Percent done: 38.6%;                    Mean loss: 3.5831\n",
      "Iteration: 1545; Percent done: 38.6%;                    Mean loss: 3.4119\n",
      "Iteration: 1546; Percent done: 38.6%;                    Mean loss: 3.4739\n",
      "Iteration: 1547; Percent done: 38.7%;                    Mean loss: 3.2961\n",
      "Iteration: 1548; Percent done: 38.7%;                    Mean loss: 3.6286\n",
      "Iteration: 1549; Percent done: 38.7%;                    Mean loss: 3.2084\n",
      "Iteration: 1550; Percent done: 38.8%;                    Mean loss: 3.5419\n",
      "Iteration: 1551; Percent done: 38.8%;                    Mean loss: 3.6291\n",
      "Iteration: 1552; Percent done: 38.8%;                    Mean loss: 3.4001\n",
      "Iteration: 1553; Percent done: 38.8%;                    Mean loss: 3.2390\n",
      "Iteration: 1554; Percent done: 38.9%;                    Mean loss: 3.3644\n",
      "Iteration: 1555; Percent done: 38.9%;                    Mean loss: 3.3391\n",
      "Iteration: 1556; Percent done: 38.9%;                    Mean loss: 3.5054\n",
      "Iteration: 1557; Percent done: 38.9%;                    Mean loss: 3.5066\n",
      "Iteration: 1558; Percent done: 39.0%;                    Mean loss: 3.3799\n",
      "Iteration: 1559; Percent done: 39.0%;                    Mean loss: 3.4375\n",
      "Iteration: 1560; Percent done: 39.0%;                    Mean loss: 3.7340\n",
      "Iteration: 1561; Percent done: 39.0%;                    Mean loss: 3.4986\n",
      "Iteration: 1562; Percent done: 39.1%;                    Mean loss: 3.6308\n",
      "Iteration: 1563; Percent done: 39.1%;                    Mean loss: 3.2988\n",
      "Iteration: 1564; Percent done: 39.1%;                    Mean loss: 3.4790\n",
      "Iteration: 1565; Percent done: 39.1%;                    Mean loss: 3.3883\n",
      "Iteration: 1566; Percent done: 39.1%;                    Mean loss: 3.3634\n",
      "Iteration: 1567; Percent done: 39.2%;                    Mean loss: 3.5463\n",
      "Iteration: 1568; Percent done: 39.2%;                    Mean loss: 3.3265\n",
      "Iteration: 1569; Percent done: 39.2%;                    Mean loss: 3.4129\n",
      "Iteration: 1570; Percent done: 39.2%;                    Mean loss: 3.5553\n",
      "Iteration: 1571; Percent done: 39.3%;                    Mean loss: 3.5746\n",
      "Iteration: 1572; Percent done: 39.3%;                    Mean loss: 3.5121\n",
      "Iteration: 1573; Percent done: 39.3%;                    Mean loss: 3.1398\n",
      "Iteration: 1574; Percent done: 39.4%;                    Mean loss: 3.3523\n",
      "Iteration: 1575; Percent done: 39.4%;                    Mean loss: 3.6235\n",
      "Iteration: 1576; Percent done: 39.4%;                    Mean loss: 3.7522\n",
      "Iteration: 1577; Percent done: 39.4%;                    Mean loss: 3.6867\n",
      "Iteration: 1578; Percent done: 39.5%;                    Mean loss: 3.3932\n",
      "Iteration: 1579; Percent done: 39.5%;                    Mean loss: 3.3850\n",
      "Iteration: 1580; Percent done: 39.5%;                    Mean loss: 3.4244\n",
      "Iteration: 1581; Percent done: 39.5%;                    Mean loss: 3.9491\n",
      "Iteration: 1582; Percent done: 39.6%;                    Mean loss: 3.5434\n",
      "Iteration: 1583; Percent done: 39.6%;                    Mean loss: 3.3962\n",
      "Iteration: 1584; Percent done: 39.6%;                    Mean loss: 3.5701\n",
      "Iteration: 1585; Percent done: 39.6%;                    Mean loss: 3.3733\n",
      "Iteration: 1586; Percent done: 39.6%;                    Mean loss: 3.3022\n",
      "Iteration: 1587; Percent done: 39.7%;                    Mean loss: 3.5863\n",
      "Iteration: 1588; Percent done: 39.7%;                    Mean loss: 3.7733\n",
      "Iteration: 1589; Percent done: 39.7%;                    Mean loss: 3.3978\n",
      "Iteration: 1590; Percent done: 39.8%;                    Mean loss: 3.4067\n",
      "Iteration: 1591; Percent done: 39.8%;                    Mean loss: 3.4181\n",
      "Iteration: 1592; Percent done: 39.8%;                    Mean loss: 3.4878\n",
      "Iteration: 1593; Percent done: 39.8%;                    Mean loss: 3.3990\n",
      "Iteration: 1594; Percent done: 39.9%;                    Mean loss: 2.9664\n",
      "Iteration: 1595; Percent done: 39.9%;                    Mean loss: 3.3057\n",
      "Iteration: 1596; Percent done: 39.9%;                    Mean loss: 3.5929\n",
      "Iteration: 1597; Percent done: 39.9%;                    Mean loss: 3.2530\n",
      "Iteration: 1598; Percent done: 40.0%;                    Mean loss: 3.4510\n",
      "Iteration: 1599; Percent done: 40.0%;                    Mean loss: 3.3606\n",
      "Iteration: 1600; Percent done: 40.0%;                    Mean loss: 3.5560\n",
      "Iteration: 1601; Percent done: 40.0%;                    Mean loss: 3.4569\n",
      "Iteration: 1602; Percent done: 40.1%;                    Mean loss: 3.6764\n",
      "Iteration: 1603; Percent done: 40.1%;                    Mean loss: 3.4741\n",
      "Iteration: 1604; Percent done: 40.1%;                    Mean loss: 3.2816\n",
      "Iteration: 1605; Percent done: 40.1%;                    Mean loss: 3.4478\n",
      "Iteration: 1606; Percent done: 40.2%;                    Mean loss: 3.5805\n",
      "Iteration: 1607; Percent done: 40.2%;                    Mean loss: 3.5413\n",
      "Iteration: 1608; Percent done: 40.2%;                    Mean loss: 3.6667\n",
      "Iteration: 1609; Percent done: 40.2%;                    Mean loss: 3.5096\n",
      "Iteration: 1610; Percent done: 40.2%;                    Mean loss: 3.3598\n",
      "Iteration: 1611; Percent done: 40.3%;                    Mean loss: 3.4350\n",
      "Iteration: 1612; Percent done: 40.3%;                    Mean loss: 3.2855\n",
      "Iteration: 1613; Percent done: 40.3%;                    Mean loss: 3.3367\n",
      "Iteration: 1614; Percent done: 40.4%;                    Mean loss: 3.4380\n",
      "Iteration: 1615; Percent done: 40.4%;                    Mean loss: 3.4088\n",
      "Iteration: 1616; Percent done: 40.4%;                    Mean loss: 3.4138\n",
      "Iteration: 1617; Percent done: 40.4%;                    Mean loss: 3.8083\n",
      "Iteration: 1618; Percent done: 40.5%;                    Mean loss: 3.5131\n",
      "Iteration: 1619; Percent done: 40.5%;                    Mean loss: 3.8195\n",
      "Iteration: 1620; Percent done: 40.5%;                    Mean loss: 3.2416\n",
      "Iteration: 1621; Percent done: 40.5%;                    Mean loss: 3.2811\n",
      "Iteration: 1622; Percent done: 40.6%;                    Mean loss: 3.5052\n",
      "Iteration: 1623; Percent done: 40.6%;                    Mean loss: 3.5305\n",
      "Iteration: 1624; Percent done: 40.6%;                    Mean loss: 3.2921\n",
      "Iteration: 1625; Percent done: 40.6%;                    Mean loss: 3.6060\n",
      "Iteration: 1626; Percent done: 40.6%;                    Mean loss: 3.5734\n",
      "Iteration: 1627; Percent done: 40.7%;                    Mean loss: 3.6501\n",
      "Iteration: 1628; Percent done: 40.7%;                    Mean loss: 3.4989\n",
      "Iteration: 1629; Percent done: 40.7%;                    Mean loss: 3.3966\n",
      "Iteration: 1630; Percent done: 40.8%;                    Mean loss: 3.5157\n",
      "Iteration: 1631; Percent done: 40.8%;                    Mean loss: 3.4704\n",
      "Iteration: 1632; Percent done: 40.8%;                    Mean loss: 3.7124\n",
      "Iteration: 1633; Percent done: 40.8%;                    Mean loss: 3.4550\n",
      "Iteration: 1634; Percent done: 40.8%;                    Mean loss: 3.6341\n",
      "Iteration: 1635; Percent done: 40.9%;                    Mean loss: 3.1162\n",
      "Iteration: 1636; Percent done: 40.9%;                    Mean loss: 3.6951\n",
      "Iteration: 1637; Percent done: 40.9%;                    Mean loss: 3.5581\n",
      "Iteration: 1638; Percent done: 40.9%;                    Mean loss: 3.3950\n",
      "Iteration: 1639; Percent done: 41.0%;                    Mean loss: 3.7595\n",
      "Iteration: 1640; Percent done: 41.0%;                    Mean loss: 3.6059\n",
      "Iteration: 1641; Percent done: 41.0%;                    Mean loss: 3.4578\n",
      "Iteration: 1642; Percent done: 41.0%;                    Mean loss: 3.3724\n",
      "Iteration: 1643; Percent done: 41.1%;                    Mean loss: 3.5496\n",
      "Iteration: 1644; Percent done: 41.1%;                    Mean loss: 3.5099\n",
      "Iteration: 1645; Percent done: 41.1%;                    Mean loss: 3.2346\n",
      "Iteration: 1646; Percent done: 41.1%;                    Mean loss: 3.7328\n",
      "Iteration: 1647; Percent done: 41.2%;                    Mean loss: 3.4237\n",
      "Iteration: 1648; Percent done: 41.2%;                    Mean loss: 3.5124\n",
      "Iteration: 1649; Percent done: 41.2%;                    Mean loss: 3.5552\n",
      "Iteration: 1650; Percent done: 41.2%;                    Mean loss: 3.2761\n",
      "Iteration: 1651; Percent done: 41.3%;                    Mean loss: 3.3726\n",
      "Iteration: 1652; Percent done: 41.3%;                    Mean loss: 3.4338\n",
      "Iteration: 1653; Percent done: 41.3%;                    Mean loss: 3.4758\n",
      "Iteration: 1654; Percent done: 41.3%;                    Mean loss: 3.3925\n",
      "Iteration: 1655; Percent done: 41.4%;                    Mean loss: 3.5979\n",
      "Iteration: 1656; Percent done: 41.4%;                    Mean loss: 3.4707\n",
      "Iteration: 1657; Percent done: 41.4%;                    Mean loss: 3.5602\n",
      "Iteration: 1658; Percent done: 41.4%;                    Mean loss: 3.6325\n",
      "Iteration: 1659; Percent done: 41.5%;                    Mean loss: 3.4127\n",
      "Iteration: 1660; Percent done: 41.5%;                    Mean loss: 3.4928\n",
      "Iteration: 1661; Percent done: 41.5%;                    Mean loss: 3.5972\n",
      "Iteration: 1662; Percent done: 41.5%;                    Mean loss: 3.4694\n",
      "Iteration: 1663; Percent done: 41.6%;                    Mean loss: 3.6022\n",
      "Iteration: 1664; Percent done: 41.6%;                    Mean loss: 3.3721\n",
      "Iteration: 1665; Percent done: 41.6%;                    Mean loss: 3.3683\n",
      "Iteration: 1666; Percent done: 41.6%;                    Mean loss: 3.3923\n",
      "Iteration: 1667; Percent done: 41.7%;                    Mean loss: 3.4332\n",
      "Iteration: 1668; Percent done: 41.7%;                    Mean loss: 3.5179\n",
      "Iteration: 1669; Percent done: 41.7%;                    Mean loss: 3.4343\n",
      "Iteration: 1670; Percent done: 41.8%;                    Mean loss: 3.4437\n",
      "Iteration: 1671; Percent done: 41.8%;                    Mean loss: 3.6960\n",
      "Iteration: 1672; Percent done: 41.8%;                    Mean loss: 3.5760\n",
      "Iteration: 1673; Percent done: 41.8%;                    Mean loss: 3.6429\n",
      "Iteration: 1674; Percent done: 41.9%;                    Mean loss: 3.6252\n",
      "Iteration: 1675; Percent done: 41.9%;                    Mean loss: 3.1939\n",
      "Iteration: 1676; Percent done: 41.9%;                    Mean loss: 3.4788\n",
      "Iteration: 1677; Percent done: 41.9%;                    Mean loss: 3.6301\n",
      "Iteration: 1678; Percent done: 41.9%;                    Mean loss: 3.2835\n",
      "Iteration: 1679; Percent done: 42.0%;                    Mean loss: 3.4942\n",
      "Iteration: 1680; Percent done: 42.0%;                    Mean loss: 3.3449\n",
      "Iteration: 1681; Percent done: 42.0%;                    Mean loss: 3.4081\n",
      "Iteration: 1682; Percent done: 42.0%;                    Mean loss: 3.4689\n",
      "Iteration: 1683; Percent done: 42.1%;                    Mean loss: 3.3307\n",
      "Iteration: 1684; Percent done: 42.1%;                    Mean loss: 3.2884\n",
      "Iteration: 1685; Percent done: 42.1%;                    Mean loss: 3.7628\n",
      "Iteration: 1686; Percent done: 42.1%;                    Mean loss: 3.3398\n",
      "Iteration: 1687; Percent done: 42.2%;                    Mean loss: 3.4139\n",
      "Iteration: 1688; Percent done: 42.2%;                    Mean loss: 3.4645\n",
      "Iteration: 1689; Percent done: 42.2%;                    Mean loss: 3.6518\n",
      "Iteration: 1690; Percent done: 42.2%;                    Mean loss: 3.4668\n",
      "Iteration: 1691; Percent done: 42.3%;                    Mean loss: 3.3326\n",
      "Iteration: 1692; Percent done: 42.3%;                    Mean loss: 3.5235\n",
      "Iteration: 1693; Percent done: 42.3%;                    Mean loss: 3.3898\n",
      "Iteration: 1694; Percent done: 42.4%;                    Mean loss: 3.4365\n",
      "Iteration: 1695; Percent done: 42.4%;                    Mean loss: 3.2847\n",
      "Iteration: 1696; Percent done: 42.4%;                    Mean loss: 3.4136\n",
      "Iteration: 1697; Percent done: 42.4%;                    Mean loss: 3.0904\n",
      "Iteration: 1698; Percent done: 42.4%;                    Mean loss: 3.2324\n",
      "Iteration: 1699; Percent done: 42.5%;                    Mean loss: 3.2582\n",
      "Iteration: 1700; Percent done: 42.5%;                    Mean loss: 3.4830\n",
      "Iteration: 1701; Percent done: 42.5%;                    Mean loss: 3.7528\n",
      "Iteration: 1702; Percent done: 42.5%;                    Mean loss: 3.3071\n",
      "Iteration: 1703; Percent done: 42.6%;                    Mean loss: 3.5383\n",
      "Iteration: 1704; Percent done: 42.6%;                    Mean loss: 3.6589\n",
      "Iteration: 1705; Percent done: 42.6%;                    Mean loss: 3.2248\n",
      "Iteration: 1706; Percent done: 42.6%;                    Mean loss: 3.2711\n",
      "Iteration: 1707; Percent done: 42.7%;                    Mean loss: 3.4483\n",
      "Iteration: 1708; Percent done: 42.7%;                    Mean loss: 3.3261\n",
      "Iteration: 1709; Percent done: 42.7%;                    Mean loss: 3.3125\n",
      "Iteration: 1710; Percent done: 42.8%;                    Mean loss: 3.6533\n",
      "Iteration: 1711; Percent done: 42.8%;                    Mean loss: 3.2299\n",
      "Iteration: 1712; Percent done: 42.8%;                    Mean loss: 3.3889\n",
      "Iteration: 1713; Percent done: 42.8%;                    Mean loss: 3.6084\n",
      "Iteration: 1714; Percent done: 42.9%;                    Mean loss: 3.4128\n",
      "Iteration: 1715; Percent done: 42.9%;                    Mean loss: 3.6508\n",
      "Iteration: 1716; Percent done: 42.9%;                    Mean loss: 3.5557\n",
      "Iteration: 1717; Percent done: 42.9%;                    Mean loss: 3.6201\n",
      "Iteration: 1718; Percent done: 43.0%;                    Mean loss: 3.3915\n",
      "Iteration: 1719; Percent done: 43.0%;                    Mean loss: 3.2050\n",
      "Iteration: 1720; Percent done: 43.0%;                    Mean loss: 3.3640\n",
      "Iteration: 1721; Percent done: 43.0%;                    Mean loss: 3.4232\n",
      "Iteration: 1722; Percent done: 43.0%;                    Mean loss: 3.4716\n",
      "Iteration: 1723; Percent done: 43.1%;                    Mean loss: 3.6521\n",
      "Iteration: 1724; Percent done: 43.1%;                    Mean loss: 3.5897\n",
      "Iteration: 1725; Percent done: 43.1%;                    Mean loss: 3.2889\n",
      "Iteration: 1726; Percent done: 43.1%;                    Mean loss: 3.4467\n",
      "Iteration: 1727; Percent done: 43.2%;                    Mean loss: 3.5083\n",
      "Iteration: 1728; Percent done: 43.2%;                    Mean loss: 3.4181\n",
      "Iteration: 1729; Percent done: 43.2%;                    Mean loss: 3.4762\n",
      "Iteration: 1730; Percent done: 43.2%;                    Mean loss: 3.5081\n",
      "Iteration: 1731; Percent done: 43.3%;                    Mean loss: 3.4082\n",
      "Iteration: 1732; Percent done: 43.3%;                    Mean loss: 3.5503\n",
      "Iteration: 1733; Percent done: 43.3%;                    Mean loss: 3.2511\n",
      "Iteration: 1734; Percent done: 43.4%;                    Mean loss: 3.3587\n",
      "Iteration: 1735; Percent done: 43.4%;                    Mean loss: 3.3449\n",
      "Iteration: 1736; Percent done: 43.4%;                    Mean loss: 3.6488\n",
      "Iteration: 1737; Percent done: 43.4%;                    Mean loss: 3.3550\n",
      "Iteration: 1738; Percent done: 43.5%;                    Mean loss: 3.2453\n",
      "Iteration: 1739; Percent done: 43.5%;                    Mean loss: 3.3280\n",
      "Iteration: 1740; Percent done: 43.5%;                    Mean loss: 3.3031\n",
      "Iteration: 1741; Percent done: 43.5%;                    Mean loss: 3.3352\n",
      "Iteration: 1742; Percent done: 43.5%;                    Mean loss: 3.2033\n",
      "Iteration: 1743; Percent done: 43.6%;                    Mean loss: 3.3268\n",
      "Iteration: 1744; Percent done: 43.6%;                    Mean loss: 3.5138\n",
      "Iteration: 1745; Percent done: 43.6%;                    Mean loss: 3.6865\n",
      "Iteration: 1746; Percent done: 43.6%;                    Mean loss: 3.5069\n",
      "Iteration: 1747; Percent done: 43.7%;                    Mean loss: 3.3433\n",
      "Iteration: 1748; Percent done: 43.7%;                    Mean loss: 3.3221\n",
      "Iteration: 1749; Percent done: 43.7%;                    Mean loss: 3.0781\n",
      "Iteration: 1750; Percent done: 43.8%;                    Mean loss: 3.7329\n",
      "Iteration: 1751; Percent done: 43.8%;                    Mean loss: 3.2444\n",
      "Iteration: 1752; Percent done: 43.8%;                    Mean loss: 3.1285\n",
      "Iteration: 1753; Percent done: 43.8%;                    Mean loss: 3.2450\n",
      "Iteration: 1754; Percent done: 43.9%;                    Mean loss: 3.4116\n",
      "Iteration: 1755; Percent done: 43.9%;                    Mean loss: 3.2489\n",
      "Iteration: 1756; Percent done: 43.9%;                    Mean loss: 3.4244\n",
      "Iteration: 1757; Percent done: 43.9%;                    Mean loss: 3.4078\n",
      "Iteration: 1758; Percent done: 44.0%;                    Mean loss: 3.2206\n",
      "Iteration: 1759; Percent done: 44.0%;                    Mean loss: 3.2227\n",
      "Iteration: 1760; Percent done: 44.0%;                    Mean loss: 3.5611\n",
      "Iteration: 1761; Percent done: 44.0%;                    Mean loss: 3.4017\n",
      "Iteration: 1762; Percent done: 44.0%;                    Mean loss: 3.4469\n",
      "Iteration: 1763; Percent done: 44.1%;                    Mean loss: 3.4329\n",
      "Iteration: 1764; Percent done: 44.1%;                    Mean loss: 3.4679\n",
      "Iteration: 1765; Percent done: 44.1%;                    Mean loss: 3.4701\n",
      "Iteration: 1766; Percent done: 44.1%;                    Mean loss: 3.5474\n",
      "Iteration: 1767; Percent done: 44.2%;                    Mean loss: 3.1682\n",
      "Iteration: 1768; Percent done: 44.2%;                    Mean loss: 3.2652\n",
      "Iteration: 1769; Percent done: 44.2%;                    Mean loss: 3.3168\n",
      "Iteration: 1770; Percent done: 44.2%;                    Mean loss: 3.6187\n",
      "Iteration: 1771; Percent done: 44.3%;                    Mean loss: 3.2730\n",
      "Iteration: 1772; Percent done: 44.3%;                    Mean loss: 3.2805\n",
      "Iteration: 1773; Percent done: 44.3%;                    Mean loss: 3.5992\n",
      "Iteration: 1774; Percent done: 44.4%;                    Mean loss: 3.3528\n",
      "Iteration: 1775; Percent done: 44.4%;                    Mean loss: 3.3363\n",
      "Iteration: 1776; Percent done: 44.4%;                    Mean loss: 3.6062\n",
      "Iteration: 1777; Percent done: 44.4%;                    Mean loss: 3.4171\n",
      "Iteration: 1778; Percent done: 44.5%;                    Mean loss: 3.5673\n",
      "Iteration: 1779; Percent done: 44.5%;                    Mean loss: 3.3149\n",
      "Iteration: 1780; Percent done: 44.5%;                    Mean loss: 3.1163\n",
      "Iteration: 1781; Percent done: 44.5%;                    Mean loss: 3.3339\n",
      "Iteration: 1782; Percent done: 44.5%;                    Mean loss: 3.3445\n",
      "Iteration: 1783; Percent done: 44.6%;                    Mean loss: 3.4348\n",
      "Iteration: 1784; Percent done: 44.6%;                    Mean loss: 3.4920\n",
      "Iteration: 1785; Percent done: 44.6%;                    Mean loss: 3.3966\n",
      "Iteration: 1786; Percent done: 44.6%;                    Mean loss: 3.5048\n",
      "Iteration: 1787; Percent done: 44.7%;                    Mean loss: 3.3592\n",
      "Iteration: 1788; Percent done: 44.7%;                    Mean loss: 3.2366\n",
      "Iteration: 1789; Percent done: 44.7%;                    Mean loss: 3.4536\n",
      "Iteration: 1790; Percent done: 44.8%;                    Mean loss: 3.4427\n",
      "Iteration: 1791; Percent done: 44.8%;                    Mean loss: 3.5352\n",
      "Iteration: 1792; Percent done: 44.8%;                    Mean loss: 3.5860\n",
      "Iteration: 1793; Percent done: 44.8%;                    Mean loss: 3.4769\n",
      "Iteration: 1794; Percent done: 44.9%;                    Mean loss: 3.5277\n",
      "Iteration: 1795; Percent done: 44.9%;                    Mean loss: 3.2626\n",
      "Iteration: 1796; Percent done: 44.9%;                    Mean loss: 3.3822\n",
      "Iteration: 1797; Percent done: 44.9%;                    Mean loss: 3.4419\n",
      "Iteration: 1798; Percent done: 45.0%;                    Mean loss: 3.4387\n",
      "Iteration: 1799; Percent done: 45.0%;                    Mean loss: 3.4340\n",
      "Iteration: 1800; Percent done: 45.0%;                    Mean loss: 3.5966\n",
      "Iteration: 1801; Percent done: 45.0%;                    Mean loss: 3.3793\n",
      "Iteration: 1802; Percent done: 45.1%;                    Mean loss: 3.2982\n",
      "Iteration: 1803; Percent done: 45.1%;                    Mean loss: 3.3903\n",
      "Iteration: 1804; Percent done: 45.1%;                    Mean loss: 3.5362\n",
      "Iteration: 1805; Percent done: 45.1%;                    Mean loss: 3.3785\n",
      "Iteration: 1806; Percent done: 45.1%;                    Mean loss: 3.2588\n",
      "Iteration: 1807; Percent done: 45.2%;                    Mean loss: 3.4685\n",
      "Iteration: 1808; Percent done: 45.2%;                    Mean loss: 3.4740\n",
      "Iteration: 1809; Percent done: 45.2%;                    Mean loss: 3.4228\n",
      "Iteration: 1810; Percent done: 45.2%;                    Mean loss: 3.5774\n",
      "Iteration: 1811; Percent done: 45.3%;                    Mean loss: 3.3287\n",
      "Iteration: 1812; Percent done: 45.3%;                    Mean loss: 3.3276\n",
      "Iteration: 1813; Percent done: 45.3%;                    Mean loss: 3.4107\n",
      "Iteration: 1814; Percent done: 45.4%;                    Mean loss: 3.5442\n",
      "Iteration: 1815; Percent done: 45.4%;                    Mean loss: 3.4510\n",
      "Iteration: 1816; Percent done: 45.4%;                    Mean loss: 3.2779\n",
      "Iteration: 1817; Percent done: 45.4%;                    Mean loss: 3.6326\n",
      "Iteration: 1818; Percent done: 45.5%;                    Mean loss: 3.4906\n",
      "Iteration: 1819; Percent done: 45.5%;                    Mean loss: 3.2879\n",
      "Iteration: 1820; Percent done: 45.5%;                    Mean loss: 3.3571\n",
      "Iteration: 1821; Percent done: 45.5%;                    Mean loss: 3.4487\n",
      "Iteration: 1822; Percent done: 45.6%;                    Mean loss: 3.4075\n",
      "Iteration: 1823; Percent done: 45.6%;                    Mean loss: 3.5021\n",
      "Iteration: 1824; Percent done: 45.6%;                    Mean loss: 3.3990\n",
      "Iteration: 1825; Percent done: 45.6%;                    Mean loss: 3.4532\n",
      "Iteration: 1826; Percent done: 45.6%;                    Mean loss: 3.6005\n",
      "Iteration: 1827; Percent done: 45.7%;                    Mean loss: 3.4831\n",
      "Iteration: 1828; Percent done: 45.7%;                    Mean loss: 3.1901\n",
      "Iteration: 1829; Percent done: 45.7%;                    Mean loss: 3.4944\n",
      "Iteration: 1830; Percent done: 45.8%;                    Mean loss: 3.0516\n",
      "Iteration: 1831; Percent done: 45.8%;                    Mean loss: 3.3880\n",
      "Iteration: 1832; Percent done: 45.8%;                    Mean loss: 3.5767\n",
      "Iteration: 1833; Percent done: 45.8%;                    Mean loss: 3.4181\n",
      "Iteration: 1834; Percent done: 45.9%;                    Mean loss: 3.5383\n",
      "Iteration: 1835; Percent done: 45.9%;                    Mean loss: 3.6707\n",
      "Iteration: 1836; Percent done: 45.9%;                    Mean loss: 3.1718\n",
      "Iteration: 1837; Percent done: 45.9%;                    Mean loss: 3.7023\n",
      "Iteration: 1838; Percent done: 46.0%;                    Mean loss: 3.4902\n",
      "Iteration: 1839; Percent done: 46.0%;                    Mean loss: 3.2463\n",
      "Iteration: 1840; Percent done: 46.0%;                    Mean loss: 3.4950\n",
      "Iteration: 1841; Percent done: 46.0%;                    Mean loss: 3.2428\n",
      "Iteration: 1842; Percent done: 46.1%;                    Mean loss: 3.4609\n",
      "Iteration: 1843; Percent done: 46.1%;                    Mean loss: 3.2481\n",
      "Iteration: 1844; Percent done: 46.1%;                    Mean loss: 3.3679\n",
      "Iteration: 1845; Percent done: 46.1%;                    Mean loss: 3.3636\n",
      "Iteration: 1846; Percent done: 46.2%;                    Mean loss: 3.4078\n",
      "Iteration: 1847; Percent done: 46.2%;                    Mean loss: 3.5411\n",
      "Iteration: 1848; Percent done: 46.2%;                    Mean loss: 3.4940\n",
      "Iteration: 1849; Percent done: 46.2%;                    Mean loss: 3.6095\n",
      "Iteration: 1850; Percent done: 46.2%;                    Mean loss: 3.4688\n",
      "Iteration: 1851; Percent done: 46.3%;                    Mean loss: 3.3928\n",
      "Iteration: 1852; Percent done: 46.3%;                    Mean loss: 3.2277\n",
      "Iteration: 1853; Percent done: 46.3%;                    Mean loss: 3.4066\n",
      "Iteration: 1854; Percent done: 46.4%;                    Mean loss: 3.3014\n",
      "Iteration: 1855; Percent done: 46.4%;                    Mean loss: 3.4325\n",
      "Iteration: 1856; Percent done: 46.4%;                    Mean loss: 3.5483\n",
      "Iteration: 1857; Percent done: 46.4%;                    Mean loss: 3.2564\n",
      "Iteration: 1858; Percent done: 46.5%;                    Mean loss: 3.3210\n",
      "Iteration: 1859; Percent done: 46.5%;                    Mean loss: 3.3814\n",
      "Iteration: 1860; Percent done: 46.5%;                    Mean loss: 3.5533\n",
      "Iteration: 1861; Percent done: 46.5%;                    Mean loss: 3.4167\n",
      "Iteration: 1862; Percent done: 46.6%;                    Mean loss: 3.3400\n",
      "Iteration: 1863; Percent done: 46.6%;                    Mean loss: 3.6864\n",
      "Iteration: 1864; Percent done: 46.6%;                    Mean loss: 3.1442\n",
      "Iteration: 1865; Percent done: 46.6%;                    Mean loss: 3.2265\n",
      "Iteration: 1866; Percent done: 46.7%;                    Mean loss: 3.3782\n",
      "Iteration: 1867; Percent done: 46.7%;                    Mean loss: 3.3834\n",
      "Iteration: 1868; Percent done: 46.7%;                    Mean loss: 3.2039\n",
      "Iteration: 1869; Percent done: 46.7%;                    Mean loss: 3.4114\n",
      "Iteration: 1870; Percent done: 46.8%;                    Mean loss: 3.5467\n",
      "Iteration: 1871; Percent done: 46.8%;                    Mean loss: 3.2092\n",
      "Iteration: 1872; Percent done: 46.8%;                    Mean loss: 3.2024\n",
      "Iteration: 1873; Percent done: 46.8%;                    Mean loss: 3.2861\n",
      "Iteration: 1874; Percent done: 46.9%;                    Mean loss: 3.3528\n",
      "Iteration: 1875; Percent done: 46.9%;                    Mean loss: 3.2881\n",
      "Iteration: 1876; Percent done: 46.9%;                    Mean loss: 3.0577\n",
      "Iteration: 1877; Percent done: 46.9%;                    Mean loss: 3.3532\n",
      "Iteration: 1878; Percent done: 46.9%;                    Mean loss: 3.3077\n",
      "Iteration: 1879; Percent done: 47.0%;                    Mean loss: 3.2646\n",
      "Iteration: 1880; Percent done: 47.0%;                    Mean loss: 3.2006\n",
      "Iteration: 1881; Percent done: 47.0%;                    Mean loss: 3.3617\n",
      "Iteration: 1882; Percent done: 47.0%;                    Mean loss: 3.1676\n",
      "Iteration: 1883; Percent done: 47.1%;                    Mean loss: 3.4808\n",
      "Iteration: 1884; Percent done: 47.1%;                    Mean loss: 3.4691\n",
      "Iteration: 1885; Percent done: 47.1%;                    Mean loss: 3.1856\n",
      "Iteration: 1886; Percent done: 47.1%;                    Mean loss: 3.4469\n",
      "Iteration: 1887; Percent done: 47.2%;                    Mean loss: 3.3326\n",
      "Iteration: 1888; Percent done: 47.2%;                    Mean loss: 3.3525\n",
      "Iteration: 1889; Percent done: 47.2%;                    Mean loss: 3.4184\n",
      "Iteration: 1890; Percent done: 47.2%;                    Mean loss: 3.4201\n",
      "Iteration: 1891; Percent done: 47.3%;                    Mean loss: 3.1980\n",
      "Iteration: 1892; Percent done: 47.3%;                    Mean loss: 3.2575\n",
      "Iteration: 1893; Percent done: 47.3%;                    Mean loss: 3.2327\n",
      "Iteration: 1894; Percent done: 47.3%;                    Mean loss: 3.3598\n",
      "Iteration: 1895; Percent done: 47.4%;                    Mean loss: 3.4341\n",
      "Iteration: 1896; Percent done: 47.4%;                    Mean loss: 3.2879\n",
      "Iteration: 1897; Percent done: 47.4%;                    Mean loss: 3.1056\n",
      "Iteration: 1898; Percent done: 47.4%;                    Mean loss: 3.1969\n",
      "Iteration: 1899; Percent done: 47.5%;                    Mean loss: 3.1210\n",
      "Iteration: 1900; Percent done: 47.5%;                    Mean loss: 3.6480\n",
      "Iteration: 1901; Percent done: 47.5%;                    Mean loss: 3.3998\n",
      "Iteration: 1902; Percent done: 47.5%;                    Mean loss: 3.3638\n",
      "Iteration: 1903; Percent done: 47.6%;                    Mean loss: 3.4180\n",
      "Iteration: 1904; Percent done: 47.6%;                    Mean loss: 3.5788\n",
      "Iteration: 1905; Percent done: 47.6%;                    Mean loss: 3.3242\n",
      "Iteration: 1906; Percent done: 47.6%;                    Mean loss: 3.1580\n",
      "Iteration: 1907; Percent done: 47.7%;                    Mean loss: 3.1920\n",
      "Iteration: 1908; Percent done: 47.7%;                    Mean loss: 3.2210\n",
      "Iteration: 1909; Percent done: 47.7%;                    Mean loss: 2.9987\n",
      "Iteration: 1910; Percent done: 47.8%;                    Mean loss: 3.2364\n",
      "Iteration: 1911; Percent done: 47.8%;                    Mean loss: 3.4264\n",
      "Iteration: 1912; Percent done: 47.8%;                    Mean loss: 3.4590\n",
      "Iteration: 1913; Percent done: 47.8%;                    Mean loss: 3.3358\n",
      "Iteration: 1914; Percent done: 47.9%;                    Mean loss: 3.4367\n",
      "Iteration: 1915; Percent done: 47.9%;                    Mean loss: 3.3855\n",
      "Iteration: 1916; Percent done: 47.9%;                    Mean loss: 3.4956\n",
      "Iteration: 1917; Percent done: 47.9%;                    Mean loss: 3.5600\n",
      "Iteration: 1918; Percent done: 47.9%;                    Mean loss: 3.3453\n",
      "Iteration: 1919; Percent done: 48.0%;                    Mean loss: 3.4189\n",
      "Iteration: 1920; Percent done: 48.0%;                    Mean loss: 3.3155\n",
      "Iteration: 1921; Percent done: 48.0%;                    Mean loss: 3.4848\n",
      "Iteration: 1922; Percent done: 48.0%;                    Mean loss: 3.1589\n",
      "Iteration: 1923; Percent done: 48.1%;                    Mean loss: 3.1790\n",
      "Iteration: 1924; Percent done: 48.1%;                    Mean loss: 3.7083\n",
      "Iteration: 1925; Percent done: 48.1%;                    Mean loss: 3.2817\n",
      "Iteration: 1926; Percent done: 48.1%;                    Mean loss: 3.6962\n",
      "Iteration: 1927; Percent done: 48.2%;                    Mean loss: 3.3004\n",
      "Iteration: 1928; Percent done: 48.2%;                    Mean loss: 3.4420\n",
      "Iteration: 1929; Percent done: 48.2%;                    Mean loss: 3.5961\n",
      "Iteration: 1930; Percent done: 48.2%;                    Mean loss: 3.2274\n",
      "Iteration: 1931; Percent done: 48.3%;                    Mean loss: 3.2715\n",
      "Iteration: 1932; Percent done: 48.3%;                    Mean loss: 3.3333\n",
      "Iteration: 1933; Percent done: 48.3%;                    Mean loss: 3.4384\n",
      "Iteration: 1934; Percent done: 48.4%;                    Mean loss: 3.3075\n",
      "Iteration: 1935; Percent done: 48.4%;                    Mean loss: 3.4811\n",
      "Iteration: 1936; Percent done: 48.4%;                    Mean loss: 3.3816\n",
      "Iteration: 1937; Percent done: 48.4%;                    Mean loss: 3.3004\n",
      "Iteration: 1938; Percent done: 48.4%;                    Mean loss: 3.3139\n",
      "Iteration: 1939; Percent done: 48.5%;                    Mean loss: 3.2008\n",
      "Iteration: 1940; Percent done: 48.5%;                    Mean loss: 3.3373\n",
      "Iteration: 1941; Percent done: 48.5%;                    Mean loss: 3.2898\n",
      "Iteration: 1942; Percent done: 48.5%;                    Mean loss: 3.3183\n",
      "Iteration: 1943; Percent done: 48.6%;                    Mean loss: 3.3147\n",
      "Iteration: 1944; Percent done: 48.6%;                    Mean loss: 3.0851\n",
      "Iteration: 1945; Percent done: 48.6%;                    Mean loss: 3.2408\n",
      "Iteration: 1946; Percent done: 48.6%;                    Mean loss: 3.1976\n",
      "Iteration: 1947; Percent done: 48.7%;                    Mean loss: 3.3679\n",
      "Iteration: 1948; Percent done: 48.7%;                    Mean loss: 3.3754\n",
      "Iteration: 1949; Percent done: 48.7%;                    Mean loss: 3.5224\n",
      "Iteration: 1950; Percent done: 48.8%;                    Mean loss: 3.5711\n",
      "Iteration: 1951; Percent done: 48.8%;                    Mean loss: 3.2198\n",
      "Iteration: 1952; Percent done: 48.8%;                    Mean loss: 3.0778\n",
      "Iteration: 1953; Percent done: 48.8%;                    Mean loss: 3.3871\n",
      "Iteration: 1954; Percent done: 48.9%;                    Mean loss: 3.5263\n",
      "Iteration: 1955; Percent done: 48.9%;                    Mean loss: 3.4362\n",
      "Iteration: 1956; Percent done: 48.9%;                    Mean loss: 3.4080\n",
      "Iteration: 1957; Percent done: 48.9%;                    Mean loss: 3.2408\n",
      "Iteration: 1958; Percent done: 48.9%;                    Mean loss: 3.3026\n",
      "Iteration: 1959; Percent done: 49.0%;                    Mean loss: 3.3244\n",
      "Iteration: 1960; Percent done: 49.0%;                    Mean loss: 3.2167\n",
      "Iteration: 1961; Percent done: 49.0%;                    Mean loss: 3.2971\n",
      "Iteration: 1962; Percent done: 49.0%;                    Mean loss: 3.5793\n",
      "Iteration: 1963; Percent done: 49.1%;                    Mean loss: 3.2443\n",
      "Iteration: 1964; Percent done: 49.1%;                    Mean loss: 3.4971\n",
      "Iteration: 1965; Percent done: 49.1%;                    Mean loss: 3.4655\n",
      "Iteration: 1966; Percent done: 49.1%;                    Mean loss: 3.4639\n",
      "Iteration: 1967; Percent done: 49.2%;                    Mean loss: 3.0722\n",
      "Iteration: 1968; Percent done: 49.2%;                    Mean loss: 3.3299\n",
      "Iteration: 1969; Percent done: 49.2%;                    Mean loss: 3.2030\n",
      "Iteration: 1970; Percent done: 49.2%;                    Mean loss: 3.3398\n",
      "Iteration: 1971; Percent done: 49.3%;                    Mean loss: 3.5082\n",
      "Iteration: 1972; Percent done: 49.3%;                    Mean loss: 3.2448\n",
      "Iteration: 1973; Percent done: 49.3%;                    Mean loss: 3.4167\n",
      "Iteration: 1974; Percent done: 49.4%;                    Mean loss: 3.3591\n",
      "Iteration: 1975; Percent done: 49.4%;                    Mean loss: 3.4036\n",
      "Iteration: 1976; Percent done: 49.4%;                    Mean loss: 3.2575\n",
      "Iteration: 1977; Percent done: 49.4%;                    Mean loss: 3.3406\n",
      "Iteration: 1978; Percent done: 49.5%;                    Mean loss: 3.2795\n",
      "Iteration: 1979; Percent done: 49.5%;                    Mean loss: 3.5280\n",
      "Iteration: 1980; Percent done: 49.5%;                    Mean loss: 3.3350\n",
      "Iteration: 1981; Percent done: 49.5%;                    Mean loss: 3.4666\n",
      "Iteration: 1982; Percent done: 49.5%;                    Mean loss: 3.5942\n",
      "Iteration: 1983; Percent done: 49.6%;                    Mean loss: 3.4229\n",
      "Iteration: 1984; Percent done: 49.6%;                    Mean loss: 3.4134\n",
      "Iteration: 1985; Percent done: 49.6%;                    Mean loss: 3.1612\n",
      "Iteration: 1986; Percent done: 49.6%;                    Mean loss: 3.3230\n",
      "Iteration: 1987; Percent done: 49.7%;                    Mean loss: 3.3211\n",
      "Iteration: 1988; Percent done: 49.7%;                    Mean loss: 3.2631\n",
      "Iteration: 1989; Percent done: 49.7%;                    Mean loss: 3.3072\n",
      "Iteration: 1990; Percent done: 49.8%;                    Mean loss: 3.5582\n",
      "Iteration: 1991; Percent done: 49.8%;                    Mean loss: 3.3483\n",
      "Iteration: 1992; Percent done: 49.8%;                    Mean loss: 3.2301\n",
      "Iteration: 1993; Percent done: 49.8%;                    Mean loss: 3.3971\n",
      "Iteration: 1994; Percent done: 49.9%;                    Mean loss: 3.4595\n",
      "Iteration: 1995; Percent done: 49.9%;                    Mean loss: 3.5688\n",
      "Iteration: 1996; Percent done: 49.9%;                    Mean loss: 3.2984\n",
      "Iteration: 1997; Percent done: 49.9%;                    Mean loss: 3.4667\n",
      "Iteration: 1998; Percent done: 50.0%;                    Mean loss: 3.4334\n",
      "Iteration: 1999; Percent done: 50.0%;                    Mean loss: 3.2797\n",
      "Iteration: 2000; Percent done: 50.0%;                    Mean loss: 3.3266\n",
      "Iteration: 2001; Percent done: 50.0%;                    Mean loss: 3.1586\n",
      "Iteration: 2002; Percent done: 50.0%;                    Mean loss: 3.3100\n",
      "Iteration: 2003; Percent done: 50.1%;                    Mean loss: 3.1868\n",
      "Iteration: 2004; Percent done: 50.1%;                    Mean loss: 3.2542\n",
      "Iteration: 2005; Percent done: 50.1%;                    Mean loss: 3.3571\n",
      "Iteration: 2006; Percent done: 50.1%;                    Mean loss: 3.2270\n",
      "Iteration: 2007; Percent done: 50.2%;                    Mean loss: 3.4326\n",
      "Iteration: 2008; Percent done: 50.2%;                    Mean loss: 3.3662\n",
      "Iteration: 2009; Percent done: 50.2%;                    Mean loss: 3.4412\n",
      "Iteration: 2010; Percent done: 50.2%;                    Mean loss: 3.3795\n",
      "Iteration: 2011; Percent done: 50.3%;                    Mean loss: 3.3747\n",
      "Iteration: 2012; Percent done: 50.3%;                    Mean loss: 3.1323\n",
      "Iteration: 2013; Percent done: 50.3%;                    Mean loss: 3.2808\n",
      "Iteration: 2014; Percent done: 50.3%;                    Mean loss: 3.4223\n",
      "Iteration: 2015; Percent done: 50.4%;                    Mean loss: 3.3005\n",
      "Iteration: 2016; Percent done: 50.4%;                    Mean loss: 3.4210\n",
      "Iteration: 2017; Percent done: 50.4%;                    Mean loss: 3.1666\n",
      "Iteration: 2018; Percent done: 50.4%;                    Mean loss: 3.4549\n",
      "Iteration: 2019; Percent done: 50.5%;                    Mean loss: 3.4598\n",
      "Iteration: 2020; Percent done: 50.5%;                    Mean loss: 3.3653\n",
      "Iteration: 2021; Percent done: 50.5%;                    Mean loss: 3.4767\n",
      "Iteration: 2022; Percent done: 50.5%;                    Mean loss: 3.1590\n",
      "Iteration: 2023; Percent done: 50.6%;                    Mean loss: 3.3318\n",
      "Iteration: 2024; Percent done: 50.6%;                    Mean loss: 3.4319\n",
      "Iteration: 2025; Percent done: 50.6%;                    Mean loss: 3.3815\n",
      "Iteration: 2026; Percent done: 50.6%;                    Mean loss: 3.2470\n",
      "Iteration: 2027; Percent done: 50.7%;                    Mean loss: 3.2616\n",
      "Iteration: 2028; Percent done: 50.7%;                    Mean loss: 3.1299\n",
      "Iteration: 2029; Percent done: 50.7%;                    Mean loss: 3.1340\n",
      "Iteration: 2030; Percent done: 50.7%;                    Mean loss: 3.3473\n",
      "Iteration: 2031; Percent done: 50.8%;                    Mean loss: 3.4195\n",
      "Iteration: 2032; Percent done: 50.8%;                    Mean loss: 3.2811\n",
      "Iteration: 2033; Percent done: 50.8%;                    Mean loss: 3.3971\n",
      "Iteration: 2034; Percent done: 50.8%;                    Mean loss: 3.3318\n",
      "Iteration: 2035; Percent done: 50.9%;                    Mean loss: 3.2616\n",
      "Iteration: 2036; Percent done: 50.9%;                    Mean loss: 3.3569\n",
      "Iteration: 2037; Percent done: 50.9%;                    Mean loss: 3.2666\n",
      "Iteration: 2038; Percent done: 50.9%;                    Mean loss: 3.1658\n",
      "Iteration: 2039; Percent done: 51.0%;                    Mean loss: 3.1580\n",
      "Iteration: 2040; Percent done: 51.0%;                    Mean loss: 3.1609\n",
      "Iteration: 2041; Percent done: 51.0%;                    Mean loss: 3.3645\n",
      "Iteration: 2042; Percent done: 51.0%;                    Mean loss: 3.4741\n",
      "Iteration: 2043; Percent done: 51.1%;                    Mean loss: 3.3125\n",
      "Iteration: 2044; Percent done: 51.1%;                    Mean loss: 3.3833\n",
      "Iteration: 2045; Percent done: 51.1%;                    Mean loss: 3.4537\n",
      "Iteration: 2046; Percent done: 51.1%;                    Mean loss: 3.1019\n",
      "Iteration: 2047; Percent done: 51.2%;                    Mean loss: 3.3711\n",
      "Iteration: 2048; Percent done: 51.2%;                    Mean loss: 3.5425\n",
      "Iteration: 2049; Percent done: 51.2%;                    Mean loss: 3.2146\n",
      "Iteration: 2050; Percent done: 51.2%;                    Mean loss: 3.1653\n",
      "Iteration: 2051; Percent done: 51.3%;                    Mean loss: 3.5947\n",
      "Iteration: 2052; Percent done: 51.3%;                    Mean loss: 3.2529\n",
      "Iteration: 2053; Percent done: 51.3%;                    Mean loss: 3.2421\n",
      "Iteration: 2054; Percent done: 51.3%;                    Mean loss: 3.2022\n",
      "Iteration: 2055; Percent done: 51.4%;                    Mean loss: 3.3383\n",
      "Iteration: 2056; Percent done: 51.4%;                    Mean loss: 3.4091\n",
      "Iteration: 2057; Percent done: 51.4%;                    Mean loss: 3.4109\n",
      "Iteration: 2058; Percent done: 51.4%;                    Mean loss: 3.1590\n",
      "Iteration: 2059; Percent done: 51.5%;                    Mean loss: 3.3983\n",
      "Iteration: 2060; Percent done: 51.5%;                    Mean loss: 3.2920\n",
      "Iteration: 2061; Percent done: 51.5%;                    Mean loss: 3.2722\n",
      "Iteration: 2062; Percent done: 51.5%;                    Mean loss: 3.4463\n",
      "Iteration: 2063; Percent done: 51.6%;                    Mean loss: 3.3202\n",
      "Iteration: 2064; Percent done: 51.6%;                    Mean loss: 3.6179\n",
      "Iteration: 2065; Percent done: 51.6%;                    Mean loss: 3.3237\n",
      "Iteration: 2066; Percent done: 51.6%;                    Mean loss: 3.5046\n",
      "Iteration: 2067; Percent done: 51.7%;                    Mean loss: 3.3815\n",
      "Iteration: 2068; Percent done: 51.7%;                    Mean loss: 3.1997\n",
      "Iteration: 2069; Percent done: 51.7%;                    Mean loss: 3.5614\n",
      "Iteration: 2070; Percent done: 51.7%;                    Mean loss: 3.2124\n",
      "Iteration: 2071; Percent done: 51.8%;                    Mean loss: 3.3776\n",
      "Iteration: 2072; Percent done: 51.8%;                    Mean loss: 3.1694\n",
      "Iteration: 2073; Percent done: 51.8%;                    Mean loss: 3.1381\n",
      "Iteration: 2074; Percent done: 51.8%;                    Mean loss: 3.2966\n",
      "Iteration: 2075; Percent done: 51.9%;                    Mean loss: 3.4504\n",
      "Iteration: 2076; Percent done: 51.9%;                    Mean loss: 3.3464\n",
      "Iteration: 2077; Percent done: 51.9%;                    Mean loss: 3.2466\n",
      "Iteration: 2078; Percent done: 51.9%;                    Mean loss: 3.3687\n",
      "Iteration: 2079; Percent done: 52.0%;                    Mean loss: 3.2828\n",
      "Iteration: 2080; Percent done: 52.0%;                    Mean loss: 3.4635\n",
      "Iteration: 2081; Percent done: 52.0%;                    Mean loss: 3.1501\n",
      "Iteration: 2082; Percent done: 52.0%;                    Mean loss: 3.4950\n",
      "Iteration: 2083; Percent done: 52.1%;                    Mean loss: 3.3871\n",
      "Iteration: 2084; Percent done: 52.1%;                    Mean loss: 3.3672\n",
      "Iteration: 2085; Percent done: 52.1%;                    Mean loss: 3.3293\n",
      "Iteration: 2086; Percent done: 52.1%;                    Mean loss: 3.1238\n",
      "Iteration: 2087; Percent done: 52.2%;                    Mean loss: 3.3477\n",
      "Iteration: 2088; Percent done: 52.2%;                    Mean loss: 3.0255\n",
      "Iteration: 2089; Percent done: 52.2%;                    Mean loss: 3.2896\n",
      "Iteration: 2090; Percent done: 52.2%;                    Mean loss: 3.3284\n",
      "Iteration: 2091; Percent done: 52.3%;                    Mean loss: 3.1928\n",
      "Iteration: 2092; Percent done: 52.3%;                    Mean loss: 3.3800\n",
      "Iteration: 2093; Percent done: 52.3%;                    Mean loss: 3.3866\n",
      "Iteration: 2094; Percent done: 52.3%;                    Mean loss: 3.2633\n",
      "Iteration: 2095; Percent done: 52.4%;                    Mean loss: 3.3384\n",
      "Iteration: 2096; Percent done: 52.4%;                    Mean loss: 3.3558\n",
      "Iteration: 2097; Percent done: 52.4%;                    Mean loss: 3.1814\n",
      "Iteration: 2098; Percent done: 52.4%;                    Mean loss: 3.4400\n",
      "Iteration: 2099; Percent done: 52.5%;                    Mean loss: 3.3088\n",
      "Iteration: 2100; Percent done: 52.5%;                    Mean loss: 3.6287\n",
      "Iteration: 2101; Percent done: 52.5%;                    Mean loss: 3.2013\n",
      "Iteration: 2102; Percent done: 52.5%;                    Mean loss: 3.1928\n",
      "Iteration: 2103; Percent done: 52.6%;                    Mean loss: 3.2030\n",
      "Iteration: 2104; Percent done: 52.6%;                    Mean loss: 3.2194\n",
      "Iteration: 2105; Percent done: 52.6%;                    Mean loss: 3.3342\n",
      "Iteration: 2106; Percent done: 52.6%;                    Mean loss: 3.1876\n",
      "Iteration: 2107; Percent done: 52.7%;                    Mean loss: 3.3006\n",
      "Iteration: 2108; Percent done: 52.7%;                    Mean loss: 3.2641\n",
      "Iteration: 2109; Percent done: 52.7%;                    Mean loss: 3.5063\n",
      "Iteration: 2110; Percent done: 52.8%;                    Mean loss: 3.4328\n",
      "Iteration: 2111; Percent done: 52.8%;                    Mean loss: 3.1887\n",
      "Iteration: 2112; Percent done: 52.8%;                    Mean loss: 3.3109\n",
      "Iteration: 2113; Percent done: 52.8%;                    Mean loss: 3.1942\n",
      "Iteration: 2114; Percent done: 52.8%;                    Mean loss: 3.3955\n",
      "Iteration: 2115; Percent done: 52.9%;                    Mean loss: 2.8833\n",
      "Iteration: 2116; Percent done: 52.9%;                    Mean loss: 3.2458\n",
      "Iteration: 2117; Percent done: 52.9%;                    Mean loss: 3.3215\n",
      "Iteration: 2118; Percent done: 52.9%;                    Mean loss: 3.3473\n",
      "Iteration: 2119; Percent done: 53.0%;                    Mean loss: 3.1388\n",
      "Iteration: 2120; Percent done: 53.0%;                    Mean loss: 3.4859\n",
      "Iteration: 2121; Percent done: 53.0%;                    Mean loss: 3.5141\n",
      "Iteration: 2122; Percent done: 53.0%;                    Mean loss: 3.0702\n",
      "Iteration: 2123; Percent done: 53.1%;                    Mean loss: 3.3834\n",
      "Iteration: 2124; Percent done: 53.1%;                    Mean loss: 3.1566\n",
      "Iteration: 2125; Percent done: 53.1%;                    Mean loss: 3.5319\n",
      "Iteration: 2126; Percent done: 53.1%;                    Mean loss: 3.2253\n",
      "Iteration: 2127; Percent done: 53.2%;                    Mean loss: 3.3614\n",
      "Iteration: 2128; Percent done: 53.2%;                    Mean loss: 2.9522\n",
      "Iteration: 2129; Percent done: 53.2%;                    Mean loss: 3.1699\n",
      "Iteration: 2130; Percent done: 53.2%;                    Mean loss: 3.2951\n",
      "Iteration: 2131; Percent done: 53.3%;                    Mean loss: 3.3787\n",
      "Iteration: 2132; Percent done: 53.3%;                    Mean loss: 3.2786\n",
      "Iteration: 2133; Percent done: 53.3%;                    Mean loss: 3.1281\n",
      "Iteration: 2134; Percent done: 53.3%;                    Mean loss: 3.5290\n",
      "Iteration: 2135; Percent done: 53.4%;                    Mean loss: 3.1337\n",
      "Iteration: 2136; Percent done: 53.4%;                    Mean loss: 3.2105\n",
      "Iteration: 2137; Percent done: 53.4%;                    Mean loss: 3.5734\n",
      "Iteration: 2138; Percent done: 53.4%;                    Mean loss: 3.0704\n",
      "Iteration: 2139; Percent done: 53.5%;                    Mean loss: 3.0685\n",
      "Iteration: 2140; Percent done: 53.5%;                    Mean loss: 3.4646\n",
      "Iteration: 2141; Percent done: 53.5%;                    Mean loss: 3.2301\n",
      "Iteration: 2142; Percent done: 53.5%;                    Mean loss: 3.4726\n",
      "Iteration: 2143; Percent done: 53.6%;                    Mean loss: 3.2763\n",
      "Iteration: 2144; Percent done: 53.6%;                    Mean loss: 3.6799\n",
      "Iteration: 2145; Percent done: 53.6%;                    Mean loss: 3.3070\n",
      "Iteration: 2146; Percent done: 53.6%;                    Mean loss: 3.0210\n",
      "Iteration: 2147; Percent done: 53.7%;                    Mean loss: 3.4311\n",
      "Iteration: 2148; Percent done: 53.7%;                    Mean loss: 3.4021\n",
      "Iteration: 2149; Percent done: 53.7%;                    Mean loss: 3.2746\n",
      "Iteration: 2150; Percent done: 53.8%;                    Mean loss: 3.4231\n",
      "Iteration: 2151; Percent done: 53.8%;                    Mean loss: 3.1126\n",
      "Iteration: 2152; Percent done: 53.8%;                    Mean loss: 3.3765\n",
      "Iteration: 2153; Percent done: 53.8%;                    Mean loss: 3.3513\n",
      "Iteration: 2154; Percent done: 53.8%;                    Mean loss: 3.2140\n",
      "Iteration: 2155; Percent done: 53.9%;                    Mean loss: 3.3531\n",
      "Iteration: 2156; Percent done: 53.9%;                    Mean loss: 3.4236\n",
      "Iteration: 2157; Percent done: 53.9%;                    Mean loss: 3.2348\n",
      "Iteration: 2158; Percent done: 53.9%;                    Mean loss: 3.4065\n",
      "Iteration: 2159; Percent done: 54.0%;                    Mean loss: 3.4398\n",
      "Iteration: 2160; Percent done: 54.0%;                    Mean loss: 3.7514\n",
      "Iteration: 2161; Percent done: 54.0%;                    Mean loss: 3.3190\n",
      "Iteration: 2162; Percent done: 54.0%;                    Mean loss: 3.3110\n",
      "Iteration: 2163; Percent done: 54.1%;                    Mean loss: 3.2552\n",
      "Iteration: 2164; Percent done: 54.1%;                    Mean loss: 3.4582\n",
      "Iteration: 2165; Percent done: 54.1%;                    Mean loss: 3.3252\n",
      "Iteration: 2166; Percent done: 54.1%;                    Mean loss: 3.1777\n",
      "Iteration: 2167; Percent done: 54.2%;                    Mean loss: 3.1657\n",
      "Iteration: 2168; Percent done: 54.2%;                    Mean loss: 3.3890\n",
      "Iteration: 2169; Percent done: 54.2%;                    Mean loss: 3.3549\n",
      "Iteration: 2170; Percent done: 54.2%;                    Mean loss: 3.3812\n",
      "Iteration: 2171; Percent done: 54.3%;                    Mean loss: 3.4095\n",
      "Iteration: 2172; Percent done: 54.3%;                    Mean loss: 3.2833\n",
      "Iteration: 2173; Percent done: 54.3%;                    Mean loss: 3.3827\n",
      "Iteration: 2174; Percent done: 54.4%;                    Mean loss: 3.3201\n",
      "Iteration: 2175; Percent done: 54.4%;                    Mean loss: 2.9712\n",
      "Iteration: 2176; Percent done: 54.4%;                    Mean loss: 3.2693\n",
      "Iteration: 2177; Percent done: 54.4%;                    Mean loss: 3.4125\n",
      "Iteration: 2178; Percent done: 54.4%;                    Mean loss: 3.2363\n",
      "Iteration: 2179; Percent done: 54.5%;                    Mean loss: 3.3615\n",
      "Iteration: 2180; Percent done: 54.5%;                    Mean loss: 2.9783\n",
      "Iteration: 2181; Percent done: 54.5%;                    Mean loss: 3.1469\n",
      "Iteration: 2182; Percent done: 54.5%;                    Mean loss: 3.1668\n",
      "Iteration: 2183; Percent done: 54.6%;                    Mean loss: 2.9565\n",
      "Iteration: 2184; Percent done: 54.6%;                    Mean loss: 3.1339\n",
      "Iteration: 2185; Percent done: 54.6%;                    Mean loss: 3.2807\n",
      "Iteration: 2186; Percent done: 54.6%;                    Mean loss: 3.3109\n",
      "Iteration: 2187; Percent done: 54.7%;                    Mean loss: 3.3281\n",
      "Iteration: 2188; Percent done: 54.7%;                    Mean loss: 3.1594\n",
      "Iteration: 2189; Percent done: 54.7%;                    Mean loss: 3.4299\n",
      "Iteration: 2190; Percent done: 54.8%;                    Mean loss: 3.5255\n",
      "Iteration: 2191; Percent done: 54.8%;                    Mean loss: 3.1299\n",
      "Iteration: 2192; Percent done: 54.8%;                    Mean loss: 3.0824\n",
      "Iteration: 2193; Percent done: 54.8%;                    Mean loss: 3.1904\n",
      "Iteration: 2194; Percent done: 54.9%;                    Mean loss: 3.3690\n",
      "Iteration: 2195; Percent done: 54.9%;                    Mean loss: 3.3218\n",
      "Iteration: 2196; Percent done: 54.9%;                    Mean loss: 3.3197\n",
      "Iteration: 2197; Percent done: 54.9%;                    Mean loss: 3.1639\n",
      "Iteration: 2198; Percent done: 54.9%;                    Mean loss: 3.7085\n",
      "Iteration: 2199; Percent done: 55.0%;                    Mean loss: 3.1516\n",
      "Iteration: 2200; Percent done: 55.0%;                    Mean loss: 3.5293\n",
      "Iteration: 2201; Percent done: 55.0%;                    Mean loss: 3.3823\n",
      "Iteration: 2202; Percent done: 55.0%;                    Mean loss: 3.3732\n",
      "Iteration: 2203; Percent done: 55.1%;                    Mean loss: 3.4230\n",
      "Iteration: 2204; Percent done: 55.1%;                    Mean loss: 3.2466\n",
      "Iteration: 2205; Percent done: 55.1%;                    Mean loss: 3.2489\n",
      "Iteration: 2206; Percent done: 55.1%;                    Mean loss: 3.3320\n",
      "Iteration: 2207; Percent done: 55.2%;                    Mean loss: 3.2976\n",
      "Iteration: 2208; Percent done: 55.2%;                    Mean loss: 3.4873\n",
      "Iteration: 2209; Percent done: 55.2%;                    Mean loss: 3.1327\n",
      "Iteration: 2210; Percent done: 55.2%;                    Mean loss: 3.1533\n",
      "Iteration: 2211; Percent done: 55.3%;                    Mean loss: 3.0813\n",
      "Iteration: 2212; Percent done: 55.3%;                    Mean loss: 3.5522\n",
      "Iteration: 2213; Percent done: 55.3%;                    Mean loss: 3.0792\n",
      "Iteration: 2214; Percent done: 55.4%;                    Mean loss: 3.5158\n",
      "Iteration: 2215; Percent done: 55.4%;                    Mean loss: 3.0826\n",
      "Iteration: 2216; Percent done: 55.4%;                    Mean loss: 3.1495\n",
      "Iteration: 2217; Percent done: 55.4%;                    Mean loss: 3.3246\n",
      "Iteration: 2218; Percent done: 55.5%;                    Mean loss: 3.4572\n",
      "Iteration: 2219; Percent done: 55.5%;                    Mean loss: 3.0347\n",
      "Iteration: 2220; Percent done: 55.5%;                    Mean loss: 3.4083\n",
      "Iteration: 2221; Percent done: 55.5%;                    Mean loss: 3.3341\n",
      "Iteration: 2222; Percent done: 55.5%;                    Mean loss: 3.3116\n",
      "Iteration: 2223; Percent done: 55.6%;                    Mean loss: 3.1274\n",
      "Iteration: 2224; Percent done: 55.6%;                    Mean loss: 3.2211\n",
      "Iteration: 2225; Percent done: 55.6%;                    Mean loss: 3.1619\n",
      "Iteration: 2226; Percent done: 55.6%;                    Mean loss: 3.3445\n",
      "Iteration: 2227; Percent done: 55.7%;                    Mean loss: 3.2613\n",
      "Iteration: 2228; Percent done: 55.7%;                    Mean loss: 3.4112\n",
      "Iteration: 2229; Percent done: 55.7%;                    Mean loss: 3.1772\n",
      "Iteration: 2230; Percent done: 55.8%;                    Mean loss: 2.9359\n",
      "Iteration: 2231; Percent done: 55.8%;                    Mean loss: 3.3354\n",
      "Iteration: 2232; Percent done: 55.8%;                    Mean loss: 3.3155\n",
      "Iteration: 2233; Percent done: 55.8%;                    Mean loss: 3.4592\n",
      "Iteration: 2234; Percent done: 55.9%;                    Mean loss: 3.0783\n",
      "Iteration: 2235; Percent done: 55.9%;                    Mean loss: 3.1260\n",
      "Iteration: 2236; Percent done: 55.9%;                    Mean loss: 3.3916\n",
      "Iteration: 2237; Percent done: 55.9%;                    Mean loss: 3.2107\n",
      "Iteration: 2238; Percent done: 56.0%;                    Mean loss: 3.3250\n",
      "Iteration: 2239; Percent done: 56.0%;                    Mean loss: 3.2332\n",
      "Iteration: 2240; Percent done: 56.0%;                    Mean loss: 3.0939\n",
      "Iteration: 2241; Percent done: 56.0%;                    Mean loss: 3.1442\n",
      "Iteration: 2242; Percent done: 56.0%;                    Mean loss: 3.1574\n",
      "Iteration: 2243; Percent done: 56.1%;                    Mean loss: 3.2101\n",
      "Iteration: 2244; Percent done: 56.1%;                    Mean loss: 3.1660\n",
      "Iteration: 2245; Percent done: 56.1%;                    Mean loss: 3.1628\n",
      "Iteration: 2246; Percent done: 56.1%;                    Mean loss: 3.2591\n",
      "Iteration: 2247; Percent done: 56.2%;                    Mean loss: 3.2054\n",
      "Iteration: 2248; Percent done: 56.2%;                    Mean loss: 3.6351\n",
      "Iteration: 2249; Percent done: 56.2%;                    Mean loss: 3.4169\n",
      "Iteration: 2250; Percent done: 56.2%;                    Mean loss: 3.2197\n",
      "Iteration: 2251; Percent done: 56.3%;                    Mean loss: 3.0374\n",
      "Iteration: 2252; Percent done: 56.3%;                    Mean loss: 3.2325\n",
      "Iteration: 2253; Percent done: 56.3%;                    Mean loss: 3.0818\n",
      "Iteration: 2254; Percent done: 56.4%;                    Mean loss: 3.2291\n",
      "Iteration: 2255; Percent done: 56.4%;                    Mean loss: 3.3281\n",
      "Iteration: 2256; Percent done: 56.4%;                    Mean loss: 3.2153\n",
      "Iteration: 2257; Percent done: 56.4%;                    Mean loss: 3.2282\n",
      "Iteration: 2258; Percent done: 56.5%;                    Mean loss: 3.3873\n",
      "Iteration: 2259; Percent done: 56.5%;                    Mean loss: 3.3749\n",
      "Iteration: 2260; Percent done: 56.5%;                    Mean loss: 3.1939\n",
      "Iteration: 2261; Percent done: 56.5%;                    Mean loss: 3.3597\n",
      "Iteration: 2262; Percent done: 56.5%;                    Mean loss: 3.1119\n",
      "Iteration: 2263; Percent done: 56.6%;                    Mean loss: 3.3391\n",
      "Iteration: 2264; Percent done: 56.6%;                    Mean loss: 3.1850\n",
      "Iteration: 2265; Percent done: 56.6%;                    Mean loss: 3.2360\n",
      "Iteration: 2266; Percent done: 56.6%;                    Mean loss: 3.1485\n",
      "Iteration: 2267; Percent done: 56.7%;                    Mean loss: 3.3890\n",
      "Iteration: 2268; Percent done: 56.7%;                    Mean loss: 3.2510\n",
      "Iteration: 2269; Percent done: 56.7%;                    Mean loss: 3.4405\n",
      "Iteration: 2270; Percent done: 56.8%;                    Mean loss: 3.1249\n",
      "Iteration: 2271; Percent done: 56.8%;                    Mean loss: 3.2399\n",
      "Iteration: 2272; Percent done: 56.8%;                    Mean loss: 3.2280\n",
      "Iteration: 2273; Percent done: 56.8%;                    Mean loss: 3.3285\n",
      "Iteration: 2274; Percent done: 56.9%;                    Mean loss: 3.3663\n",
      "Iteration: 2275; Percent done: 56.9%;                    Mean loss: 3.1480\n",
      "Iteration: 2276; Percent done: 56.9%;                    Mean loss: 3.3834\n",
      "Iteration: 2277; Percent done: 56.9%;                    Mean loss: 3.3474\n",
      "Iteration: 2278; Percent done: 57.0%;                    Mean loss: 3.4087\n",
      "Iteration: 2279; Percent done: 57.0%;                    Mean loss: 3.3612\n",
      "Iteration: 2280; Percent done: 57.0%;                    Mean loss: 3.3476\n",
      "Iteration: 2281; Percent done: 57.0%;                    Mean loss: 3.2629\n",
      "Iteration: 2282; Percent done: 57.0%;                    Mean loss: 3.3512\n",
      "Iteration: 2283; Percent done: 57.1%;                    Mean loss: 3.1906\n",
      "Iteration: 2284; Percent done: 57.1%;                    Mean loss: 3.0791\n",
      "Iteration: 2285; Percent done: 57.1%;                    Mean loss: 3.2380\n",
      "Iteration: 2286; Percent done: 57.1%;                    Mean loss: 3.3207\n",
      "Iteration: 2287; Percent done: 57.2%;                    Mean loss: 3.1982\n",
      "Iteration: 2288; Percent done: 57.2%;                    Mean loss: 3.2375\n",
      "Iteration: 2289; Percent done: 57.2%;                    Mean loss: 3.1357\n",
      "Iteration: 2290; Percent done: 57.2%;                    Mean loss: 2.9408\n",
      "Iteration: 2291; Percent done: 57.3%;                    Mean loss: 3.1572\n",
      "Iteration: 2292; Percent done: 57.3%;                    Mean loss: 3.2115\n",
      "Iteration: 2293; Percent done: 57.3%;                    Mean loss: 3.1235\n",
      "Iteration: 2294; Percent done: 57.4%;                    Mean loss: 3.2456\n",
      "Iteration: 2295; Percent done: 57.4%;                    Mean loss: 3.3249\n",
      "Iteration: 2296; Percent done: 57.4%;                    Mean loss: 3.2377\n",
      "Iteration: 2297; Percent done: 57.4%;                    Mean loss: 3.3511\n",
      "Iteration: 2298; Percent done: 57.5%;                    Mean loss: 3.2237\n",
      "Iteration: 2299; Percent done: 57.5%;                    Mean loss: 3.3662\n",
      "Iteration: 2300; Percent done: 57.5%;                    Mean loss: 3.4293\n",
      "Iteration: 2301; Percent done: 57.5%;                    Mean loss: 3.3439\n",
      "Iteration: 2302; Percent done: 57.6%;                    Mean loss: 3.3871\n",
      "Iteration: 2303; Percent done: 57.6%;                    Mean loss: 3.3898\n",
      "Iteration: 2304; Percent done: 57.6%;                    Mean loss: 3.2927\n",
      "Iteration: 2305; Percent done: 57.6%;                    Mean loss: 3.2019\n",
      "Iteration: 2306; Percent done: 57.6%;                    Mean loss: 3.2296\n",
      "Iteration: 2307; Percent done: 57.7%;                    Mean loss: 3.3094\n",
      "Iteration: 2308; Percent done: 57.7%;                    Mean loss: 3.1910\n",
      "Iteration: 2309; Percent done: 57.7%;                    Mean loss: 3.5362\n",
      "Iteration: 2310; Percent done: 57.8%;                    Mean loss: 2.9417\n",
      "Iteration: 2311; Percent done: 57.8%;                    Mean loss: 3.1830\n",
      "Iteration: 2312; Percent done: 57.8%;                    Mean loss: 3.3992\n",
      "Iteration: 2313; Percent done: 57.8%;                    Mean loss: 3.0878\n",
      "Iteration: 2314; Percent done: 57.9%;                    Mean loss: 3.4347\n",
      "Iteration: 2315; Percent done: 57.9%;                    Mean loss: 3.4025\n",
      "Iteration: 2316; Percent done: 57.9%;                    Mean loss: 3.0335\n",
      "Iteration: 2317; Percent done: 57.9%;                    Mean loss: 2.9147\n",
      "Iteration: 2318; Percent done: 58.0%;                    Mean loss: 3.3995\n",
      "Iteration: 2319; Percent done: 58.0%;                    Mean loss: 3.2707\n",
      "Iteration: 2320; Percent done: 58.0%;                    Mean loss: 3.3418\n",
      "Iteration: 2321; Percent done: 58.0%;                    Mean loss: 2.9744\n",
      "Iteration: 2322; Percent done: 58.1%;                    Mean loss: 3.4647\n",
      "Iteration: 2323; Percent done: 58.1%;                    Mean loss: 3.4185\n",
      "Iteration: 2324; Percent done: 58.1%;                    Mean loss: 3.2494\n",
      "Iteration: 2325; Percent done: 58.1%;                    Mean loss: 3.0628\n",
      "Iteration: 2326; Percent done: 58.1%;                    Mean loss: 3.2967\n",
      "Iteration: 2327; Percent done: 58.2%;                    Mean loss: 3.0762\n",
      "Iteration: 2328; Percent done: 58.2%;                    Mean loss: 3.3545\n",
      "Iteration: 2329; Percent done: 58.2%;                    Mean loss: 2.9651\n",
      "Iteration: 2330; Percent done: 58.2%;                    Mean loss: 3.1238\n",
      "Iteration: 2331; Percent done: 58.3%;                    Mean loss: 3.3061\n",
      "Iteration: 2332; Percent done: 58.3%;                    Mean loss: 3.4912\n",
      "Iteration: 2333; Percent done: 58.3%;                    Mean loss: 3.2535\n",
      "Iteration: 2334; Percent done: 58.4%;                    Mean loss: 3.5009\n",
      "Iteration: 2335; Percent done: 58.4%;                    Mean loss: 3.2159\n",
      "Iteration: 2336; Percent done: 58.4%;                    Mean loss: 3.1371\n",
      "Iteration: 2337; Percent done: 58.4%;                    Mean loss: 3.3387\n",
      "Iteration: 2338; Percent done: 58.5%;                    Mean loss: 3.2616\n",
      "Iteration: 2339; Percent done: 58.5%;                    Mean loss: 3.2171\n",
      "Iteration: 2340; Percent done: 58.5%;                    Mean loss: 3.2135\n",
      "Iteration: 2341; Percent done: 58.5%;                    Mean loss: 3.2955\n",
      "Iteration: 2342; Percent done: 58.6%;                    Mean loss: 3.4624\n",
      "Iteration: 2343; Percent done: 58.6%;                    Mean loss: 3.3388\n",
      "Iteration: 2344; Percent done: 58.6%;                    Mean loss: 3.4050\n",
      "Iteration: 2345; Percent done: 58.6%;                    Mean loss: 3.2865\n",
      "Iteration: 2346; Percent done: 58.7%;                    Mean loss: 3.1702\n",
      "Iteration: 2347; Percent done: 58.7%;                    Mean loss: 3.4530\n",
      "Iteration: 2348; Percent done: 58.7%;                    Mean loss: 3.2813\n",
      "Iteration: 2349; Percent done: 58.7%;                    Mean loss: 3.2207\n",
      "Iteration: 2350; Percent done: 58.8%;                    Mean loss: 3.2893\n",
      "Iteration: 2351; Percent done: 58.8%;                    Mean loss: 3.1553\n",
      "Iteration: 2352; Percent done: 58.8%;                    Mean loss: 3.1079\n",
      "Iteration: 2353; Percent done: 58.8%;                    Mean loss: 3.2936\n",
      "Iteration: 2354; Percent done: 58.9%;                    Mean loss: 3.3667\n",
      "Iteration: 2355; Percent done: 58.9%;                    Mean loss: 3.3423\n",
      "Iteration: 2356; Percent done: 58.9%;                    Mean loss: 3.2891\n",
      "Iteration: 2357; Percent done: 58.9%;                    Mean loss: 3.5611\n",
      "Iteration: 2358; Percent done: 59.0%;                    Mean loss: 3.0735\n",
      "Iteration: 2359; Percent done: 59.0%;                    Mean loss: 3.1826\n",
      "Iteration: 2360; Percent done: 59.0%;                    Mean loss: 3.1361\n",
      "Iteration: 2361; Percent done: 59.0%;                    Mean loss: 3.3063\n",
      "Iteration: 2362; Percent done: 59.1%;                    Mean loss: 3.3424\n",
      "Iteration: 2363; Percent done: 59.1%;                    Mean loss: 3.1482\n",
      "Iteration: 2364; Percent done: 59.1%;                    Mean loss: 3.0493\n",
      "Iteration: 2365; Percent done: 59.1%;                    Mean loss: 3.2996\n",
      "Iteration: 2366; Percent done: 59.2%;                    Mean loss: 3.2502\n",
      "Iteration: 2367; Percent done: 59.2%;                    Mean loss: 3.6392\n",
      "Iteration: 2368; Percent done: 59.2%;                    Mean loss: 3.5456\n",
      "Iteration: 2369; Percent done: 59.2%;                    Mean loss: 3.0613\n",
      "Iteration: 2370; Percent done: 59.2%;                    Mean loss: 3.2358\n",
      "Iteration: 2371; Percent done: 59.3%;                    Mean loss: 3.1765\n",
      "Iteration: 2372; Percent done: 59.3%;                    Mean loss: 3.0740\n",
      "Iteration: 2373; Percent done: 59.3%;                    Mean loss: 3.5321\n",
      "Iteration: 2374; Percent done: 59.4%;                    Mean loss: 3.2130\n",
      "Iteration: 2375; Percent done: 59.4%;                    Mean loss: 3.1432\n",
      "Iteration: 2376; Percent done: 59.4%;                    Mean loss: 3.3833\n",
      "Iteration: 2377; Percent done: 59.4%;                    Mean loss: 3.5359\n",
      "Iteration: 2378; Percent done: 59.5%;                    Mean loss: 3.2612\n",
      "Iteration: 2379; Percent done: 59.5%;                    Mean loss: 3.2326\n",
      "Iteration: 2380; Percent done: 59.5%;                    Mean loss: 3.3024\n",
      "Iteration: 2381; Percent done: 59.5%;                    Mean loss: 3.2980\n",
      "Iteration: 2382; Percent done: 59.6%;                    Mean loss: 3.2666\n",
      "Iteration: 2383; Percent done: 59.6%;                    Mean loss: 3.0483\n",
      "Iteration: 2384; Percent done: 59.6%;                    Mean loss: 3.2025\n",
      "Iteration: 2385; Percent done: 59.6%;                    Mean loss: 3.2743\n",
      "Iteration: 2386; Percent done: 59.7%;                    Mean loss: 3.1823\n",
      "Iteration: 2387; Percent done: 59.7%;                    Mean loss: 3.1203\n",
      "Iteration: 2388; Percent done: 59.7%;                    Mean loss: 3.2936\n",
      "Iteration: 2389; Percent done: 59.7%;                    Mean loss: 3.3600\n",
      "Iteration: 2390; Percent done: 59.8%;                    Mean loss: 3.1501\n",
      "Iteration: 2391; Percent done: 59.8%;                    Mean loss: 3.3114\n",
      "Iteration: 2392; Percent done: 59.8%;                    Mean loss: 3.0718\n",
      "Iteration: 2393; Percent done: 59.8%;                    Mean loss: 3.1284\n",
      "Iteration: 2394; Percent done: 59.9%;                    Mean loss: 3.2879\n",
      "Iteration: 2395; Percent done: 59.9%;                    Mean loss: 3.3343\n",
      "Iteration: 2396; Percent done: 59.9%;                    Mean loss: 3.0335\n",
      "Iteration: 2397; Percent done: 59.9%;                    Mean loss: 3.0836\n",
      "Iteration: 2398; Percent done: 60.0%;                    Mean loss: 3.2423\n",
      "Iteration: 2399; Percent done: 60.0%;                    Mean loss: 2.9688\n",
      "Iteration: 2400; Percent done: 60.0%;                    Mean loss: 3.2039\n",
      "Iteration: 2401; Percent done: 60.0%;                    Mean loss: 3.2667\n",
      "Iteration: 2402; Percent done: 60.1%;                    Mean loss: 3.1823\n",
      "Iteration: 2403; Percent done: 60.1%;                    Mean loss: 3.2117\n",
      "Iteration: 2404; Percent done: 60.1%;                    Mean loss: 2.9702\n",
      "Iteration: 2405; Percent done: 60.1%;                    Mean loss: 3.1541\n",
      "Iteration: 2406; Percent done: 60.2%;                    Mean loss: 3.3106\n",
      "Iteration: 2407; Percent done: 60.2%;                    Mean loss: 3.3188\n",
      "Iteration: 2408; Percent done: 60.2%;                    Mean loss: 3.2317\n",
      "Iteration: 2409; Percent done: 60.2%;                    Mean loss: 3.4008\n",
      "Iteration: 2410; Percent done: 60.2%;                    Mean loss: 2.9203\n",
      "Iteration: 2411; Percent done: 60.3%;                    Mean loss: 3.2721\n",
      "Iteration: 2412; Percent done: 60.3%;                    Mean loss: 3.5025\n",
      "Iteration: 2413; Percent done: 60.3%;                    Mean loss: 3.3109\n",
      "Iteration: 2414; Percent done: 60.4%;                    Mean loss: 3.2847\n",
      "Iteration: 2415; Percent done: 60.4%;                    Mean loss: 3.3639\n",
      "Iteration: 2416; Percent done: 60.4%;                    Mean loss: 3.3495\n",
      "Iteration: 2417; Percent done: 60.4%;                    Mean loss: 3.4696\n",
      "Iteration: 2418; Percent done: 60.5%;                    Mean loss: 3.2838\n",
      "Iteration: 2419; Percent done: 60.5%;                    Mean loss: 3.4550\n",
      "Iteration: 2420; Percent done: 60.5%;                    Mean loss: 3.1268\n",
      "Iteration: 2421; Percent done: 60.5%;                    Mean loss: 3.3609\n",
      "Iteration: 2422; Percent done: 60.6%;                    Mean loss: 3.3175\n",
      "Iteration: 2423; Percent done: 60.6%;                    Mean loss: 3.4627\n",
      "Iteration: 2424; Percent done: 60.6%;                    Mean loss: 3.1421\n",
      "Iteration: 2425; Percent done: 60.6%;                    Mean loss: 3.1847\n",
      "Iteration: 2426; Percent done: 60.7%;                    Mean loss: 3.2927\n",
      "Iteration: 2427; Percent done: 60.7%;                    Mean loss: 2.7937\n",
      "Iteration: 2428; Percent done: 60.7%;                    Mean loss: 3.1237\n",
      "Iteration: 2429; Percent done: 60.7%;                    Mean loss: 3.4938\n",
      "Iteration: 2430; Percent done: 60.8%;                    Mean loss: 3.0435\n",
      "Iteration: 2431; Percent done: 60.8%;                    Mean loss: 3.1645\n",
      "Iteration: 2432; Percent done: 60.8%;                    Mean loss: 3.2137\n",
      "Iteration: 2433; Percent done: 60.8%;                    Mean loss: 3.0496\n",
      "Iteration: 2434; Percent done: 60.9%;                    Mean loss: 3.2550\n",
      "Iteration: 2435; Percent done: 60.9%;                    Mean loss: 3.2772\n",
      "Iteration: 2436; Percent done: 60.9%;                    Mean loss: 3.2853\n",
      "Iteration: 2437; Percent done: 60.9%;                    Mean loss: 3.3053\n",
      "Iteration: 2438; Percent done: 61.0%;                    Mean loss: 3.3644\n",
      "Iteration: 2439; Percent done: 61.0%;                    Mean loss: 3.0281\n",
      "Iteration: 2440; Percent done: 61.0%;                    Mean loss: 3.1550\n",
      "Iteration: 2441; Percent done: 61.0%;                    Mean loss: 3.2269\n",
      "Iteration: 2442; Percent done: 61.1%;                    Mean loss: 3.1981\n",
      "Iteration: 2443; Percent done: 61.1%;                    Mean loss: 3.3483\n",
      "Iteration: 2444; Percent done: 61.1%;                    Mean loss: 3.4648\n",
      "Iteration: 2445; Percent done: 61.1%;                    Mean loss: 3.3894\n",
      "Iteration: 2446; Percent done: 61.2%;                    Mean loss: 2.9826\n",
      "Iteration: 2447; Percent done: 61.2%;                    Mean loss: 3.1970\n",
      "Iteration: 2448; Percent done: 61.2%;                    Mean loss: 3.2703\n",
      "Iteration: 2449; Percent done: 61.2%;                    Mean loss: 3.3272\n",
      "Iteration: 2450; Percent done: 61.3%;                    Mean loss: 3.3428\n",
      "Iteration: 2451; Percent done: 61.3%;                    Mean loss: 3.3681\n",
      "Iteration: 2452; Percent done: 61.3%;                    Mean loss: 3.1949\n",
      "Iteration: 2453; Percent done: 61.3%;                    Mean loss: 3.1746\n",
      "Iteration: 2454; Percent done: 61.4%;                    Mean loss: 3.3332\n",
      "Iteration: 2455; Percent done: 61.4%;                    Mean loss: 3.0743\n",
      "Iteration: 2456; Percent done: 61.4%;                    Mean loss: 3.3093\n",
      "Iteration: 2457; Percent done: 61.4%;                    Mean loss: 3.2383\n",
      "Iteration: 2458; Percent done: 61.5%;                    Mean loss: 3.2408\n",
      "Iteration: 2459; Percent done: 61.5%;                    Mean loss: 3.1880\n",
      "Iteration: 2460; Percent done: 61.5%;                    Mean loss: 3.2253\n",
      "Iteration: 2461; Percent done: 61.5%;                    Mean loss: 3.2359\n",
      "Iteration: 2462; Percent done: 61.6%;                    Mean loss: 3.3860\n",
      "Iteration: 2463; Percent done: 61.6%;                    Mean loss: 3.2217\n",
      "Iteration: 2464; Percent done: 61.6%;                    Mean loss: 3.1227\n",
      "Iteration: 2465; Percent done: 61.6%;                    Mean loss: 3.2734\n",
      "Iteration: 2466; Percent done: 61.7%;                    Mean loss: 3.3823\n",
      "Iteration: 2467; Percent done: 61.7%;                    Mean loss: 2.8192\n",
      "Iteration: 2468; Percent done: 61.7%;                    Mean loss: 3.0166\n",
      "Iteration: 2469; Percent done: 61.7%;                    Mean loss: 3.0946\n",
      "Iteration: 2470; Percent done: 61.8%;                    Mean loss: 3.1699\n",
      "Iteration: 2471; Percent done: 61.8%;                    Mean loss: 3.0924\n",
      "Iteration: 2472; Percent done: 61.8%;                    Mean loss: 3.0940\n",
      "Iteration: 2473; Percent done: 61.8%;                    Mean loss: 3.1338\n",
      "Iteration: 2474; Percent done: 61.9%;                    Mean loss: 3.3208\n",
      "Iteration: 2475; Percent done: 61.9%;                    Mean loss: 3.2578\n",
      "Iteration: 2476; Percent done: 61.9%;                    Mean loss: 3.2765\n",
      "Iteration: 2477; Percent done: 61.9%;                    Mean loss: 3.2540\n",
      "Iteration: 2478; Percent done: 62.0%;                    Mean loss: 3.4640\n",
      "Iteration: 2479; Percent done: 62.0%;                    Mean loss: 3.0194\n",
      "Iteration: 2480; Percent done: 62.0%;                    Mean loss: 3.3702\n",
      "Iteration: 2481; Percent done: 62.0%;                    Mean loss: 3.0820\n",
      "Iteration: 2482; Percent done: 62.1%;                    Mean loss: 3.0963\n",
      "Iteration: 2483; Percent done: 62.1%;                    Mean loss: 3.1530\n",
      "Iteration: 2484; Percent done: 62.1%;                    Mean loss: 2.9708\n",
      "Iteration: 2485; Percent done: 62.1%;                    Mean loss: 3.5423\n",
      "Iteration: 2486; Percent done: 62.2%;                    Mean loss: 3.1031\n",
      "Iteration: 2487; Percent done: 62.2%;                    Mean loss: 3.2752\n",
      "Iteration: 2488; Percent done: 62.2%;                    Mean loss: 3.2630\n",
      "Iteration: 2489; Percent done: 62.2%;                    Mean loss: 3.0541\n",
      "Iteration: 2490; Percent done: 62.3%;                    Mean loss: 3.2743\n",
      "Iteration: 2491; Percent done: 62.3%;                    Mean loss: 3.4630\n",
      "Iteration: 2492; Percent done: 62.3%;                    Mean loss: 3.3774\n",
      "Iteration: 2493; Percent done: 62.3%;                    Mean loss: 3.2819\n",
      "Iteration: 2494; Percent done: 62.4%;                    Mean loss: 3.2670\n",
      "Iteration: 2495; Percent done: 62.4%;                    Mean loss: 2.8809\n",
      "Iteration: 2496; Percent done: 62.4%;                    Mean loss: 3.1718\n",
      "Iteration: 2497; Percent done: 62.4%;                    Mean loss: 3.4534\n",
      "Iteration: 2498; Percent done: 62.5%;                    Mean loss: 2.8183\n",
      "Iteration: 2499; Percent done: 62.5%;                    Mean loss: 3.1825\n",
      "Iteration: 2500; Percent done: 62.5%;                    Mean loss: 3.1485\n",
      "Iteration: 2501; Percent done: 62.5%;                    Mean loss: 3.2997\n",
      "Iteration: 2502; Percent done: 62.5%;                    Mean loss: 3.3864\n",
      "Iteration: 2503; Percent done: 62.6%;                    Mean loss: 3.1258\n",
      "Iteration: 2504; Percent done: 62.6%;                    Mean loss: 3.3785\n",
      "Iteration: 2505; Percent done: 62.6%;                    Mean loss: 3.2130\n",
      "Iteration: 2506; Percent done: 62.6%;                    Mean loss: 3.3462\n",
      "Iteration: 2507; Percent done: 62.7%;                    Mean loss: 3.2232\n",
      "Iteration: 2508; Percent done: 62.7%;                    Mean loss: 2.9607\n",
      "Iteration: 2509; Percent done: 62.7%;                    Mean loss: 3.0846\n",
      "Iteration: 2510; Percent done: 62.7%;                    Mean loss: 3.1555\n",
      "Iteration: 2511; Percent done: 62.8%;                    Mean loss: 3.3788\n",
      "Iteration: 2512; Percent done: 62.8%;                    Mean loss: 3.3613\n",
      "Iteration: 2513; Percent done: 62.8%;                    Mean loss: 3.2531\n",
      "Iteration: 2514; Percent done: 62.8%;                    Mean loss: 3.1008\n",
      "Iteration: 2515; Percent done: 62.9%;                    Mean loss: 3.3702\n",
      "Iteration: 2516; Percent done: 62.9%;                    Mean loss: 3.1344\n",
      "Iteration: 2517; Percent done: 62.9%;                    Mean loss: 3.0953\n",
      "Iteration: 2518; Percent done: 62.9%;                    Mean loss: 3.1022\n",
      "Iteration: 2519; Percent done: 63.0%;                    Mean loss: 3.0051\n",
      "Iteration: 2520; Percent done: 63.0%;                    Mean loss: 3.0340\n",
      "Iteration: 2521; Percent done: 63.0%;                    Mean loss: 2.9742\n",
      "Iteration: 2522; Percent done: 63.0%;                    Mean loss: 3.3231\n",
      "Iteration: 2523; Percent done: 63.1%;                    Mean loss: 3.1215\n",
      "Iteration: 2524; Percent done: 63.1%;                    Mean loss: 3.0809\n",
      "Iteration: 2525; Percent done: 63.1%;                    Mean loss: 3.3291\n",
      "Iteration: 2526; Percent done: 63.1%;                    Mean loss: 2.9183\n",
      "Iteration: 2527; Percent done: 63.2%;                    Mean loss: 3.0580\n",
      "Iteration: 2528; Percent done: 63.2%;                    Mean loss: 3.0352\n",
      "Iteration: 2529; Percent done: 63.2%;                    Mean loss: 3.3585\n",
      "Iteration: 2530; Percent done: 63.2%;                    Mean loss: 3.3306\n",
      "Iteration: 2531; Percent done: 63.3%;                    Mean loss: 3.0872\n",
      "Iteration: 2532; Percent done: 63.3%;                    Mean loss: 2.9787\n",
      "Iteration: 2533; Percent done: 63.3%;                    Mean loss: 2.9418\n",
      "Iteration: 2534; Percent done: 63.3%;                    Mean loss: 3.1861\n",
      "Iteration: 2535; Percent done: 63.4%;                    Mean loss: 3.2736\n",
      "Iteration: 2536; Percent done: 63.4%;                    Mean loss: 2.9589\n",
      "Iteration: 2537; Percent done: 63.4%;                    Mean loss: 3.2660\n",
      "Iteration: 2538; Percent done: 63.4%;                    Mean loss: 3.0777\n",
      "Iteration: 2539; Percent done: 63.5%;                    Mean loss: 3.0118\n",
      "Iteration: 2540; Percent done: 63.5%;                    Mean loss: 3.0499\n",
      "Iteration: 2541; Percent done: 63.5%;                    Mean loss: 3.1029\n",
      "Iteration: 2542; Percent done: 63.5%;                    Mean loss: 3.0263\n",
      "Iteration: 2543; Percent done: 63.6%;                    Mean loss: 3.4797\n",
      "Iteration: 2544; Percent done: 63.6%;                    Mean loss: 2.9786\n",
      "Iteration: 2545; Percent done: 63.6%;                    Mean loss: 3.0444\n",
      "Iteration: 2546; Percent done: 63.6%;                    Mean loss: 2.8888\n",
      "Iteration: 2547; Percent done: 63.7%;                    Mean loss: 3.2503\n",
      "Iteration: 2548; Percent done: 63.7%;                    Mean loss: 2.9974\n",
      "Iteration: 2549; Percent done: 63.7%;                    Mean loss: 3.2139\n",
      "Iteration: 2550; Percent done: 63.7%;                    Mean loss: 3.1938\n",
      "Iteration: 2551; Percent done: 63.8%;                    Mean loss: 3.2546\n",
      "Iteration: 2552; Percent done: 63.8%;                    Mean loss: 3.1460\n",
      "Iteration: 2553; Percent done: 63.8%;                    Mean loss: 3.0958\n",
      "Iteration: 2554; Percent done: 63.8%;                    Mean loss: 3.2241\n",
      "Iteration: 2555; Percent done: 63.9%;                    Mean loss: 3.1983\n",
      "Iteration: 2556; Percent done: 63.9%;                    Mean loss: 3.2248\n",
      "Iteration: 2557; Percent done: 63.9%;                    Mean loss: 3.2344\n",
      "Iteration: 2558; Percent done: 63.9%;                    Mean loss: 3.4474\n",
      "Iteration: 2559; Percent done: 64.0%;                    Mean loss: 3.2600\n",
      "Iteration: 2560; Percent done: 64.0%;                    Mean loss: 3.0292\n",
      "Iteration: 2561; Percent done: 64.0%;                    Mean loss: 3.1021\n",
      "Iteration: 2562; Percent done: 64.0%;                    Mean loss: 3.4676\n",
      "Iteration: 2563; Percent done: 64.1%;                    Mean loss: 3.3971\n",
      "Iteration: 2564; Percent done: 64.1%;                    Mean loss: 3.5192\n",
      "Iteration: 2565; Percent done: 64.1%;                    Mean loss: 3.0031\n",
      "Iteration: 2566; Percent done: 64.1%;                    Mean loss: 3.1429\n",
      "Iteration: 2567; Percent done: 64.2%;                    Mean loss: 2.9442\n",
      "Iteration: 2568; Percent done: 64.2%;                    Mean loss: 3.1177\n",
      "Iteration: 2569; Percent done: 64.2%;                    Mean loss: 2.9495\n",
      "Iteration: 2570; Percent done: 64.2%;                    Mean loss: 3.1117\n",
      "Iteration: 2571; Percent done: 64.3%;                    Mean loss: 3.1786\n",
      "Iteration: 2572; Percent done: 64.3%;                    Mean loss: 3.2134\n",
      "Iteration: 2573; Percent done: 64.3%;                    Mean loss: 2.9725\n",
      "Iteration: 2574; Percent done: 64.3%;                    Mean loss: 3.1030\n",
      "Iteration: 2575; Percent done: 64.4%;                    Mean loss: 3.2907\n",
      "Iteration: 2576; Percent done: 64.4%;                    Mean loss: 3.0499\n",
      "Iteration: 2577; Percent done: 64.4%;                    Mean loss: 3.2374\n",
      "Iteration: 2578; Percent done: 64.5%;                    Mean loss: 3.0721\n",
      "Iteration: 2579; Percent done: 64.5%;                    Mean loss: 3.2590\n",
      "Iteration: 2580; Percent done: 64.5%;                    Mean loss: 3.2679\n",
      "Iteration: 2581; Percent done: 64.5%;                    Mean loss: 3.0481\n",
      "Iteration: 2582; Percent done: 64.5%;                    Mean loss: 3.1782\n",
      "Iteration: 2583; Percent done: 64.6%;                    Mean loss: 3.2580\n",
      "Iteration: 2584; Percent done: 64.6%;                    Mean loss: 3.0267\n",
      "Iteration: 2585; Percent done: 64.6%;                    Mean loss: 2.9541\n",
      "Iteration: 2586; Percent done: 64.6%;                    Mean loss: 3.0476\n",
      "Iteration: 2587; Percent done: 64.7%;                    Mean loss: 3.1525\n",
      "Iteration: 2588; Percent done: 64.7%;                    Mean loss: 3.3722\n",
      "Iteration: 2589; Percent done: 64.7%;                    Mean loss: 2.9941\n",
      "Iteration: 2590; Percent done: 64.8%;                    Mean loss: 3.2179\n",
      "Iteration: 2591; Percent done: 64.8%;                    Mean loss: 3.1996\n",
      "Iteration: 2592; Percent done: 64.8%;                    Mean loss: 3.1505\n",
      "Iteration: 2593; Percent done: 64.8%;                    Mean loss: 2.8644\n",
      "Iteration: 2594; Percent done: 64.8%;                    Mean loss: 3.3613\n",
      "Iteration: 2595; Percent done: 64.9%;                    Mean loss: 3.0453\n",
      "Iteration: 2596; Percent done: 64.9%;                    Mean loss: 3.3046\n",
      "Iteration: 2597; Percent done: 64.9%;                    Mean loss: 3.0554\n",
      "Iteration: 2598; Percent done: 65.0%;                    Mean loss: 2.9528\n",
      "Iteration: 2599; Percent done: 65.0%;                    Mean loss: 3.0533\n",
      "Iteration: 2600; Percent done: 65.0%;                    Mean loss: 3.2127\n",
      "Iteration: 2601; Percent done: 65.0%;                    Mean loss: 3.4581\n",
      "Iteration: 2602; Percent done: 65.0%;                    Mean loss: 3.1776\n",
      "Iteration: 2603; Percent done: 65.1%;                    Mean loss: 3.0412\n",
      "Iteration: 2604; Percent done: 65.1%;                    Mean loss: 3.2538\n",
      "Iteration: 2605; Percent done: 65.1%;                    Mean loss: 3.0185\n",
      "Iteration: 2606; Percent done: 65.1%;                    Mean loss: 3.0697\n",
      "Iteration: 2607; Percent done: 65.2%;                    Mean loss: 3.2103\n",
      "Iteration: 2608; Percent done: 65.2%;                    Mean loss: 3.1086\n",
      "Iteration: 2609; Percent done: 65.2%;                    Mean loss: 3.1958\n",
      "Iteration: 2610; Percent done: 65.2%;                    Mean loss: 3.0892\n",
      "Iteration: 2611; Percent done: 65.3%;                    Mean loss: 3.2144\n",
      "Iteration: 2612; Percent done: 65.3%;                    Mean loss: 3.0667\n",
      "Iteration: 2613; Percent done: 65.3%;                    Mean loss: 3.1453\n",
      "Iteration: 2614; Percent done: 65.3%;                    Mean loss: 2.9817\n",
      "Iteration: 2615; Percent done: 65.4%;                    Mean loss: 3.0039\n",
      "Iteration: 2616; Percent done: 65.4%;                    Mean loss: 3.1350\n",
      "Iteration: 2617; Percent done: 65.4%;                    Mean loss: 3.0524\n",
      "Iteration: 2618; Percent done: 65.5%;                    Mean loss: 3.2377\n",
      "Iteration: 2619; Percent done: 65.5%;                    Mean loss: 2.9534\n",
      "Iteration: 2620; Percent done: 65.5%;                    Mean loss: 2.9411\n",
      "Iteration: 2621; Percent done: 65.5%;                    Mean loss: 3.2076\n",
      "Iteration: 2622; Percent done: 65.5%;                    Mean loss: 3.1452\n",
      "Iteration: 2623; Percent done: 65.6%;                    Mean loss: 2.9527\n",
      "Iteration: 2624; Percent done: 65.6%;                    Mean loss: 3.2598\n",
      "Iteration: 2625; Percent done: 65.6%;                    Mean loss: 3.1002\n",
      "Iteration: 2626; Percent done: 65.6%;                    Mean loss: 3.3904\n",
      "Iteration: 2627; Percent done: 65.7%;                    Mean loss: 3.1954\n",
      "Iteration: 2628; Percent done: 65.7%;                    Mean loss: 3.0700\n",
      "Iteration: 2629; Percent done: 65.7%;                    Mean loss: 2.8973\n",
      "Iteration: 2630; Percent done: 65.8%;                    Mean loss: 3.3177\n",
      "Iteration: 2631; Percent done: 65.8%;                    Mean loss: 3.1914\n",
      "Iteration: 2632; Percent done: 65.8%;                    Mean loss: 3.2283\n",
      "Iteration: 2633; Percent done: 65.8%;                    Mean loss: 3.0979\n",
      "Iteration: 2634; Percent done: 65.8%;                    Mean loss: 3.1656\n",
      "Iteration: 2635; Percent done: 65.9%;                    Mean loss: 3.1428\n",
      "Iteration: 2636; Percent done: 65.9%;                    Mean loss: 3.1289\n",
      "Iteration: 2637; Percent done: 65.9%;                    Mean loss: 3.1405\n",
      "Iteration: 2638; Percent done: 66.0%;                    Mean loss: 3.1230\n",
      "Iteration: 2639; Percent done: 66.0%;                    Mean loss: 3.3167\n",
      "Iteration: 2640; Percent done: 66.0%;                    Mean loss: 2.7757\n",
      "Iteration: 2641; Percent done: 66.0%;                    Mean loss: 3.3464\n",
      "Iteration: 2642; Percent done: 66.0%;                    Mean loss: 3.0879\n",
      "Iteration: 2643; Percent done: 66.1%;                    Mean loss: 3.3382\n",
      "Iteration: 2644; Percent done: 66.1%;                    Mean loss: 2.8569\n",
      "Iteration: 2645; Percent done: 66.1%;                    Mean loss: 2.8396\n",
      "Iteration: 2646; Percent done: 66.1%;                    Mean loss: 3.3210\n",
      "Iteration: 2647; Percent done: 66.2%;                    Mean loss: 3.1457\n",
      "Iteration: 2648; Percent done: 66.2%;                    Mean loss: 2.8673\n",
      "Iteration: 2649; Percent done: 66.2%;                    Mean loss: 3.2411\n",
      "Iteration: 2650; Percent done: 66.2%;                    Mean loss: 3.0486\n",
      "Iteration: 2651; Percent done: 66.3%;                    Mean loss: 3.0822\n",
      "Iteration: 2652; Percent done: 66.3%;                    Mean loss: 3.1020\n",
      "Iteration: 2653; Percent done: 66.3%;                    Mean loss: 3.2015\n",
      "Iteration: 2654; Percent done: 66.3%;                    Mean loss: 3.1396\n",
      "Iteration: 2655; Percent done: 66.4%;                    Mean loss: 3.1640\n",
      "Iteration: 2656; Percent done: 66.4%;                    Mean loss: 3.1283\n",
      "Iteration: 2657; Percent done: 66.4%;                    Mean loss: 3.3239\n",
      "Iteration: 2658; Percent done: 66.5%;                    Mean loss: 3.1321\n",
      "Iteration: 2659; Percent done: 66.5%;                    Mean loss: 3.0544\n",
      "Iteration: 2660; Percent done: 66.5%;                    Mean loss: 3.0962\n",
      "Iteration: 2661; Percent done: 66.5%;                    Mean loss: 3.2326\n",
      "Iteration: 2662; Percent done: 66.5%;                    Mean loss: 2.9606\n",
      "Iteration: 2663; Percent done: 66.6%;                    Mean loss: 3.2176\n",
      "Iteration: 2664; Percent done: 66.6%;                    Mean loss: 3.1847\n",
      "Iteration: 2665; Percent done: 66.6%;                    Mean loss: 2.8606\n",
      "Iteration: 2666; Percent done: 66.6%;                    Mean loss: 3.2185\n",
      "Iteration: 2667; Percent done: 66.7%;                    Mean loss: 2.9831\n",
      "Iteration: 2668; Percent done: 66.7%;                    Mean loss: 3.0791\n",
      "Iteration: 2669; Percent done: 66.7%;                    Mean loss: 3.0472\n",
      "Iteration: 2670; Percent done: 66.8%;                    Mean loss: 2.9169\n",
      "Iteration: 2671; Percent done: 66.8%;                    Mean loss: 3.2034\n",
      "Iteration: 2672; Percent done: 66.8%;                    Mean loss: 3.3839\n",
      "Iteration: 2673; Percent done: 66.8%;                    Mean loss: 2.9712\n",
      "Iteration: 2674; Percent done: 66.8%;                    Mean loss: 3.0987\n",
      "Iteration: 2675; Percent done: 66.9%;                    Mean loss: 3.1882\n",
      "Iteration: 2676; Percent done: 66.9%;                    Mean loss: 3.1618\n",
      "Iteration: 2677; Percent done: 66.9%;                    Mean loss: 3.0692\n",
      "Iteration: 2678; Percent done: 67.0%;                    Mean loss: 3.3686\n",
      "Iteration: 2679; Percent done: 67.0%;                    Mean loss: 3.0183\n",
      "Iteration: 2680; Percent done: 67.0%;                    Mean loss: 3.1254\n",
      "Iteration: 2681; Percent done: 67.0%;                    Mean loss: 3.0560\n",
      "Iteration: 2682; Percent done: 67.0%;                    Mean loss: 3.0238\n",
      "Iteration: 2683; Percent done: 67.1%;                    Mean loss: 3.1715\n",
      "Iteration: 2684; Percent done: 67.1%;                    Mean loss: 3.1914\n",
      "Iteration: 2685; Percent done: 67.1%;                    Mean loss: 2.9125\n",
      "Iteration: 2686; Percent done: 67.2%;                    Mean loss: 3.0427\n",
      "Iteration: 2687; Percent done: 67.2%;                    Mean loss: 3.0504\n",
      "Iteration: 2688; Percent done: 67.2%;                    Mean loss: 3.1123\n",
      "Iteration: 2689; Percent done: 67.2%;                    Mean loss: 2.9302\n",
      "Iteration: 2690; Percent done: 67.2%;                    Mean loss: 3.0005\n",
      "Iteration: 2691; Percent done: 67.3%;                    Mean loss: 3.2822\n",
      "Iteration: 2692; Percent done: 67.3%;                    Mean loss: 2.9475\n",
      "Iteration: 2693; Percent done: 67.3%;                    Mean loss: 3.1533\n",
      "Iteration: 2694; Percent done: 67.3%;                    Mean loss: 3.2727\n",
      "Iteration: 2695; Percent done: 67.4%;                    Mean loss: 3.0424\n",
      "Iteration: 2696; Percent done: 67.4%;                    Mean loss: 3.0700\n",
      "Iteration: 2697; Percent done: 67.4%;                    Mean loss: 3.3474\n",
      "Iteration: 2698; Percent done: 67.5%;                    Mean loss: 3.0084\n",
      "Iteration: 2699; Percent done: 67.5%;                    Mean loss: 3.3589\n",
      "Iteration: 2700; Percent done: 67.5%;                    Mean loss: 3.2198\n",
      "Iteration: 2701; Percent done: 67.5%;                    Mean loss: 3.3766\n",
      "Iteration: 2702; Percent done: 67.5%;                    Mean loss: 3.1699\n",
      "Iteration: 2703; Percent done: 67.6%;                    Mean loss: 3.2583\n",
      "Iteration: 2704; Percent done: 67.6%;                    Mean loss: 3.2999\n",
      "Iteration: 2705; Percent done: 67.6%;                    Mean loss: 3.1292\n",
      "Iteration: 2706; Percent done: 67.7%;                    Mean loss: 3.1091\n",
      "Iteration: 2707; Percent done: 67.7%;                    Mean loss: 3.1166\n",
      "Iteration: 2708; Percent done: 67.7%;                    Mean loss: 3.3031\n",
      "Iteration: 2709; Percent done: 67.7%;                    Mean loss: 3.1406\n",
      "Iteration: 2710; Percent done: 67.8%;                    Mean loss: 3.3202\n",
      "Iteration: 2711; Percent done: 67.8%;                    Mean loss: 3.2348\n",
      "Iteration: 2712; Percent done: 67.8%;                    Mean loss: 2.8406\n",
      "Iteration: 2713; Percent done: 67.8%;                    Mean loss: 3.0887\n",
      "Iteration: 2714; Percent done: 67.8%;                    Mean loss: 3.0445\n",
      "Iteration: 2715; Percent done: 67.9%;                    Mean loss: 3.0809\n",
      "Iteration: 2716; Percent done: 67.9%;                    Mean loss: 3.0167\n",
      "Iteration: 2717; Percent done: 67.9%;                    Mean loss: 3.1682\n",
      "Iteration: 2718; Percent done: 68.0%;                    Mean loss: 3.5269\n",
      "Iteration: 2719; Percent done: 68.0%;                    Mean loss: 3.3406\n",
      "Iteration: 2720; Percent done: 68.0%;                    Mean loss: 3.1735\n",
      "Iteration: 2721; Percent done: 68.0%;                    Mean loss: 3.1730\n",
      "Iteration: 2722; Percent done: 68.0%;                    Mean loss: 3.2856\n",
      "Iteration: 2723; Percent done: 68.1%;                    Mean loss: 3.0669\n",
      "Iteration: 2724; Percent done: 68.1%;                    Mean loss: 3.1742\n",
      "Iteration: 2725; Percent done: 68.1%;                    Mean loss: 3.1935\n",
      "Iteration: 2726; Percent done: 68.2%;                    Mean loss: 3.1739\n",
      "Iteration: 2727; Percent done: 68.2%;                    Mean loss: 3.1074\n",
      "Iteration: 2728; Percent done: 68.2%;                    Mean loss: 3.2974\n",
      "Iteration: 2729; Percent done: 68.2%;                    Mean loss: 3.2888\n",
      "Iteration: 2730; Percent done: 68.2%;                    Mean loss: 3.1300\n",
      "Iteration: 2731; Percent done: 68.3%;                    Mean loss: 3.1120\n",
      "Iteration: 2732; Percent done: 68.3%;                    Mean loss: 2.9766\n",
      "Iteration: 2733; Percent done: 68.3%;                    Mean loss: 3.0222\n",
      "Iteration: 2734; Percent done: 68.3%;                    Mean loss: 2.9041\n",
      "Iteration: 2735; Percent done: 68.4%;                    Mean loss: 3.0258\n",
      "Iteration: 2736; Percent done: 68.4%;                    Mean loss: 3.0640\n",
      "Iteration: 2737; Percent done: 68.4%;                    Mean loss: 3.2008\n",
      "Iteration: 2738; Percent done: 68.5%;                    Mean loss: 2.9938\n",
      "Iteration: 2739; Percent done: 68.5%;                    Mean loss: 2.9528\n",
      "Iteration: 2740; Percent done: 68.5%;                    Mean loss: 2.7478\n",
      "Iteration: 2741; Percent done: 68.5%;                    Mean loss: 3.0803\n",
      "Iteration: 2742; Percent done: 68.5%;                    Mean loss: 2.9148\n",
      "Iteration: 2743; Percent done: 68.6%;                    Mean loss: 3.1324\n",
      "Iteration: 2744; Percent done: 68.6%;                    Mean loss: 3.0778\n",
      "Iteration: 2745; Percent done: 68.6%;                    Mean loss: 3.1147\n",
      "Iteration: 2746; Percent done: 68.7%;                    Mean loss: 3.0914\n",
      "Iteration: 2747; Percent done: 68.7%;                    Mean loss: 3.3137\n",
      "Iteration: 2748; Percent done: 68.7%;                    Mean loss: 3.1175\n",
      "Iteration: 2749; Percent done: 68.7%;                    Mean loss: 3.1990\n",
      "Iteration: 2750; Percent done: 68.8%;                    Mean loss: 3.0285\n",
      "Iteration: 2751; Percent done: 68.8%;                    Mean loss: 3.0455\n",
      "Iteration: 2752; Percent done: 68.8%;                    Mean loss: 3.1862\n",
      "Iteration: 2753; Percent done: 68.8%;                    Mean loss: 3.1446\n",
      "Iteration: 2754; Percent done: 68.8%;                    Mean loss: 2.9612\n",
      "Iteration: 2755; Percent done: 68.9%;                    Mean loss: 3.0884\n",
      "Iteration: 2756; Percent done: 68.9%;                    Mean loss: 3.1378\n",
      "Iteration: 2757; Percent done: 68.9%;                    Mean loss: 3.2782\n",
      "Iteration: 2758; Percent done: 69.0%;                    Mean loss: 3.2437\n",
      "Iteration: 2759; Percent done: 69.0%;                    Mean loss: 3.0695\n",
      "Iteration: 2760; Percent done: 69.0%;                    Mean loss: 3.1325\n",
      "Iteration: 2761; Percent done: 69.0%;                    Mean loss: 3.2329\n",
      "Iteration: 2762; Percent done: 69.0%;                    Mean loss: 3.2620\n",
      "Iteration: 2763; Percent done: 69.1%;                    Mean loss: 3.1128\n",
      "Iteration: 2764; Percent done: 69.1%;                    Mean loss: 3.0648\n",
      "Iteration: 2765; Percent done: 69.1%;                    Mean loss: 3.1677\n",
      "Iteration: 2766; Percent done: 69.2%;                    Mean loss: 3.1228\n",
      "Iteration: 2767; Percent done: 69.2%;                    Mean loss: 2.9496\n",
      "Iteration: 2768; Percent done: 69.2%;                    Mean loss: 3.0655\n",
      "Iteration: 2769; Percent done: 69.2%;                    Mean loss: 3.1210\n",
      "Iteration: 2770; Percent done: 69.2%;                    Mean loss: 2.7984\n",
      "Iteration: 2771; Percent done: 69.3%;                    Mean loss: 2.8059\n",
      "Iteration: 2772; Percent done: 69.3%;                    Mean loss: 3.1961\n",
      "Iteration: 2773; Percent done: 69.3%;                    Mean loss: 3.2380\n",
      "Iteration: 2774; Percent done: 69.3%;                    Mean loss: 3.1502\n",
      "Iteration: 2775; Percent done: 69.4%;                    Mean loss: 2.9957\n",
      "Iteration: 2776; Percent done: 69.4%;                    Mean loss: 3.0401\n",
      "Iteration: 2777; Percent done: 69.4%;                    Mean loss: 3.2631\n",
      "Iteration: 2778; Percent done: 69.5%;                    Mean loss: 2.9208\n",
      "Iteration: 2779; Percent done: 69.5%;                    Mean loss: 2.7959\n",
      "Iteration: 2780; Percent done: 69.5%;                    Mean loss: 3.2494\n",
      "Iteration: 2781; Percent done: 69.5%;                    Mean loss: 3.1165\n",
      "Iteration: 2782; Percent done: 69.5%;                    Mean loss: 2.8471\n",
      "Iteration: 2783; Percent done: 69.6%;                    Mean loss: 3.1585\n",
      "Iteration: 2784; Percent done: 69.6%;                    Mean loss: 3.0079\n",
      "Iteration: 2785; Percent done: 69.6%;                    Mean loss: 2.9615\n",
      "Iteration: 2786; Percent done: 69.7%;                    Mean loss: 3.0384\n",
      "Iteration: 2787; Percent done: 69.7%;                    Mean loss: 3.2569\n",
      "Iteration: 2788; Percent done: 69.7%;                    Mean loss: 3.1854\n",
      "Iteration: 2789; Percent done: 69.7%;                    Mean loss: 3.1630\n",
      "Iteration: 2790; Percent done: 69.8%;                    Mean loss: 3.1501\n",
      "Iteration: 2791; Percent done: 69.8%;                    Mean loss: 2.9538\n",
      "Iteration: 2792; Percent done: 69.8%;                    Mean loss: 3.0804\n",
      "Iteration: 2793; Percent done: 69.8%;                    Mean loss: 3.2722\n",
      "Iteration: 2794; Percent done: 69.8%;                    Mean loss: 3.1239\n",
      "Iteration: 2795; Percent done: 69.9%;                    Mean loss: 2.9715\n",
      "Iteration: 2796; Percent done: 69.9%;                    Mean loss: 2.9940\n",
      "Iteration: 2797; Percent done: 69.9%;                    Mean loss: 3.0423\n",
      "Iteration: 2798; Percent done: 70.0%;                    Mean loss: 2.9876\n",
      "Iteration: 2799; Percent done: 70.0%;                    Mean loss: 3.1278\n",
      "Iteration: 2800; Percent done: 70.0%;                    Mean loss: 2.8283\n",
      "Iteration: 2801; Percent done: 70.0%;                    Mean loss: 3.0574\n",
      "Iteration: 2802; Percent done: 70.0%;                    Mean loss: 3.1967\n",
      "Iteration: 2803; Percent done: 70.1%;                    Mean loss: 3.2637\n",
      "Iteration: 2804; Percent done: 70.1%;                    Mean loss: 2.9988\n",
      "Iteration: 2805; Percent done: 70.1%;                    Mean loss: 3.4649\n",
      "Iteration: 2806; Percent done: 70.2%;                    Mean loss: 2.9727\n",
      "Iteration: 2807; Percent done: 70.2%;                    Mean loss: 2.9815\n",
      "Iteration: 2808; Percent done: 70.2%;                    Mean loss: 3.0053\n",
      "Iteration: 2809; Percent done: 70.2%;                    Mean loss: 3.2070\n",
      "Iteration: 2810; Percent done: 70.2%;                    Mean loss: 2.8548\n",
      "Iteration: 2811; Percent done: 70.3%;                    Mean loss: 3.0289\n",
      "Iteration: 2812; Percent done: 70.3%;                    Mean loss: 2.9515\n",
      "Iteration: 2813; Percent done: 70.3%;                    Mean loss: 2.9503\n",
      "Iteration: 2814; Percent done: 70.3%;                    Mean loss: 2.8895\n",
      "Iteration: 2815; Percent done: 70.4%;                    Mean loss: 3.0565\n",
      "Iteration: 2816; Percent done: 70.4%;                    Mean loss: 2.8743\n",
      "Iteration: 2817; Percent done: 70.4%;                    Mean loss: 2.9469\n",
      "Iteration: 2818; Percent done: 70.5%;                    Mean loss: 3.0106\n",
      "Iteration: 2819; Percent done: 70.5%;                    Mean loss: 3.0315\n",
      "Iteration: 2820; Percent done: 70.5%;                    Mean loss: 2.9381\n",
      "Iteration: 2821; Percent done: 70.5%;                    Mean loss: 3.1664\n",
      "Iteration: 2822; Percent done: 70.5%;                    Mean loss: 3.0896\n",
      "Iteration: 2823; Percent done: 70.6%;                    Mean loss: 3.2354\n",
      "Iteration: 2824; Percent done: 70.6%;                    Mean loss: 3.3495\n",
      "Iteration: 2825; Percent done: 70.6%;                    Mean loss: 2.8598\n",
      "Iteration: 2826; Percent done: 70.7%;                    Mean loss: 3.2906\n",
      "Iteration: 2827; Percent done: 70.7%;                    Mean loss: 3.0897\n",
      "Iteration: 2828; Percent done: 70.7%;                    Mean loss: 3.0127\n",
      "Iteration: 2829; Percent done: 70.7%;                    Mean loss: 2.9864\n",
      "Iteration: 2830; Percent done: 70.8%;                    Mean loss: 3.0461\n",
      "Iteration: 2831; Percent done: 70.8%;                    Mean loss: 3.2016\n",
      "Iteration: 2832; Percent done: 70.8%;                    Mean loss: 3.1758\n",
      "Iteration: 2833; Percent done: 70.8%;                    Mean loss: 3.1893\n",
      "Iteration: 2834; Percent done: 70.9%;                    Mean loss: 3.0925\n",
      "Iteration: 2835; Percent done: 70.9%;                    Mean loss: 3.3049\n",
      "Iteration: 2836; Percent done: 70.9%;                    Mean loss: 2.9443\n",
      "Iteration: 2837; Percent done: 70.9%;                    Mean loss: 3.0699\n",
      "Iteration: 2838; Percent done: 71.0%;                    Mean loss: 3.1488\n",
      "Iteration: 2839; Percent done: 71.0%;                    Mean loss: 2.9824\n",
      "Iteration: 2840; Percent done: 71.0%;                    Mean loss: 3.1547\n",
      "Iteration: 2841; Percent done: 71.0%;                    Mean loss: 3.1462\n",
      "Iteration: 2842; Percent done: 71.0%;                    Mean loss: 3.1588\n",
      "Iteration: 2843; Percent done: 71.1%;                    Mean loss: 3.1463\n",
      "Iteration: 2844; Percent done: 71.1%;                    Mean loss: 3.2830\n",
      "Iteration: 2845; Percent done: 71.1%;                    Mean loss: 3.0296\n",
      "Iteration: 2846; Percent done: 71.2%;                    Mean loss: 3.0398\n",
      "Iteration: 2847; Percent done: 71.2%;                    Mean loss: 3.1604\n",
      "Iteration: 2848; Percent done: 71.2%;                    Mean loss: 3.0496\n",
      "Iteration: 2849; Percent done: 71.2%;                    Mean loss: 3.3366\n",
      "Iteration: 2850; Percent done: 71.2%;                    Mean loss: 3.1812\n",
      "Iteration: 2851; Percent done: 71.3%;                    Mean loss: 3.0240\n",
      "Iteration: 2852; Percent done: 71.3%;                    Mean loss: 3.1069\n",
      "Iteration: 2853; Percent done: 71.3%;                    Mean loss: 3.1624\n",
      "Iteration: 2854; Percent done: 71.4%;                    Mean loss: 3.2625\n",
      "Iteration: 2855; Percent done: 71.4%;                    Mean loss: 2.9930\n",
      "Iteration: 2856; Percent done: 71.4%;                    Mean loss: 3.3020\n",
      "Iteration: 2857; Percent done: 71.4%;                    Mean loss: 3.0870\n",
      "Iteration: 2858; Percent done: 71.5%;                    Mean loss: 2.9155\n",
      "Iteration: 2859; Percent done: 71.5%;                    Mean loss: 2.9761\n",
      "Iteration: 2860; Percent done: 71.5%;                    Mean loss: 3.2402\n",
      "Iteration: 2861; Percent done: 71.5%;                    Mean loss: 3.1519\n",
      "Iteration: 2862; Percent done: 71.5%;                    Mean loss: 3.2000\n",
      "Iteration: 2863; Percent done: 71.6%;                    Mean loss: 3.1321\n",
      "Iteration: 2864; Percent done: 71.6%;                    Mean loss: 3.2576\n",
      "Iteration: 2865; Percent done: 71.6%;                    Mean loss: 3.0728\n",
      "Iteration: 2866; Percent done: 71.7%;                    Mean loss: 3.1010\n",
      "Iteration: 2867; Percent done: 71.7%;                    Mean loss: 3.0419\n",
      "Iteration: 2868; Percent done: 71.7%;                    Mean loss: 3.1453\n",
      "Iteration: 2869; Percent done: 71.7%;                    Mean loss: 3.0939\n",
      "Iteration: 2870; Percent done: 71.8%;                    Mean loss: 3.0788\n",
      "Iteration: 2871; Percent done: 71.8%;                    Mean loss: 3.0217\n",
      "Iteration: 2872; Percent done: 71.8%;                    Mean loss: 3.3271\n",
      "Iteration: 2873; Percent done: 71.8%;                    Mean loss: 2.7929\n",
      "Iteration: 2874; Percent done: 71.9%;                    Mean loss: 3.1855\n",
      "Iteration: 2875; Percent done: 71.9%;                    Mean loss: 2.9609\n",
      "Iteration: 2876; Percent done: 71.9%;                    Mean loss: 2.9635\n",
      "Iteration: 2877; Percent done: 71.9%;                    Mean loss: 3.3827\n",
      "Iteration: 2878; Percent done: 72.0%;                    Mean loss: 3.0975\n",
      "Iteration: 2879; Percent done: 72.0%;                    Mean loss: 3.0390\n",
      "Iteration: 2880; Percent done: 72.0%;                    Mean loss: 2.9436\n",
      "Iteration: 2881; Percent done: 72.0%;                    Mean loss: 3.1053\n",
      "Iteration: 2882; Percent done: 72.0%;                    Mean loss: 3.3170\n",
      "Iteration: 2883; Percent done: 72.1%;                    Mean loss: 3.0053\n",
      "Iteration: 2884; Percent done: 72.1%;                    Mean loss: 3.2023\n",
      "Iteration: 2885; Percent done: 72.1%;                    Mean loss: 3.1638\n",
      "Iteration: 2886; Percent done: 72.2%;                    Mean loss: 2.9781\n",
      "Iteration: 2887; Percent done: 72.2%;                    Mean loss: 3.1748\n",
      "Iteration: 2888; Percent done: 72.2%;                    Mean loss: 3.0467\n",
      "Iteration: 2889; Percent done: 72.2%;                    Mean loss: 3.0253\n",
      "Iteration: 2890; Percent done: 72.2%;                    Mean loss: 3.2176\n",
      "Iteration: 2891; Percent done: 72.3%;                    Mean loss: 3.2359\n",
      "Iteration: 2892; Percent done: 72.3%;                    Mean loss: 2.9372\n",
      "Iteration: 2893; Percent done: 72.3%;                    Mean loss: 3.1691\n",
      "Iteration: 2894; Percent done: 72.4%;                    Mean loss: 2.9698\n",
      "Iteration: 2895; Percent done: 72.4%;                    Mean loss: 3.0235\n",
      "Iteration: 2896; Percent done: 72.4%;                    Mean loss: 3.0487\n",
      "Iteration: 2897; Percent done: 72.4%;                    Mean loss: 3.0665\n",
      "Iteration: 2898; Percent done: 72.5%;                    Mean loss: 3.0129\n",
      "Iteration: 2899; Percent done: 72.5%;                    Mean loss: 3.0325\n",
      "Iteration: 2900; Percent done: 72.5%;                    Mean loss: 2.7406\n",
      "Iteration: 2901; Percent done: 72.5%;                    Mean loss: 2.8491\n",
      "Iteration: 2902; Percent done: 72.5%;                    Mean loss: 3.0252\n",
      "Iteration: 2903; Percent done: 72.6%;                    Mean loss: 2.8416\n",
      "Iteration: 2904; Percent done: 72.6%;                    Mean loss: 3.2225\n",
      "Iteration: 2905; Percent done: 72.6%;                    Mean loss: 3.0008\n",
      "Iteration: 2906; Percent done: 72.7%;                    Mean loss: 2.9762\n",
      "Iteration: 2907; Percent done: 72.7%;                    Mean loss: 3.0407\n",
      "Iteration: 2908; Percent done: 72.7%;                    Mean loss: 3.0204\n",
      "Iteration: 2909; Percent done: 72.7%;                    Mean loss: 3.1112\n",
      "Iteration: 2910; Percent done: 72.8%;                    Mean loss: 3.1740\n",
      "Iteration: 2911; Percent done: 72.8%;                    Mean loss: 3.1040\n",
      "Iteration: 2912; Percent done: 72.8%;                    Mean loss: 3.2300\n",
      "Iteration: 2913; Percent done: 72.8%;                    Mean loss: 2.9885\n",
      "Iteration: 2914; Percent done: 72.9%;                    Mean loss: 3.2389\n",
      "Iteration: 2915; Percent done: 72.9%;                    Mean loss: 2.8975\n",
      "Iteration: 2916; Percent done: 72.9%;                    Mean loss: 2.9720\n",
      "Iteration: 2917; Percent done: 72.9%;                    Mean loss: 2.9565\n",
      "Iteration: 2918; Percent done: 73.0%;                    Mean loss: 3.2932\n",
      "Iteration: 2919; Percent done: 73.0%;                    Mean loss: 3.0027\n",
      "Iteration: 2920; Percent done: 73.0%;                    Mean loss: 3.1085\n",
      "Iteration: 2921; Percent done: 73.0%;                    Mean loss: 3.1962\n",
      "Iteration: 2922; Percent done: 73.0%;                    Mean loss: 3.2013\n",
      "Iteration: 2923; Percent done: 73.1%;                    Mean loss: 2.8590\n",
      "Iteration: 2924; Percent done: 73.1%;                    Mean loss: 2.6642\n",
      "Iteration: 2925; Percent done: 73.1%;                    Mean loss: 2.9153\n",
      "Iteration: 2926; Percent done: 73.2%;                    Mean loss: 3.1284\n",
      "Iteration: 2927; Percent done: 73.2%;                    Mean loss: 2.9017\n",
      "Iteration: 2928; Percent done: 73.2%;                    Mean loss: 2.9082\n",
      "Iteration: 2929; Percent done: 73.2%;                    Mean loss: 3.1809\n",
      "Iteration: 2930; Percent done: 73.2%;                    Mean loss: 2.9886\n",
      "Iteration: 2931; Percent done: 73.3%;                    Mean loss: 2.9591\n",
      "Iteration: 2932; Percent done: 73.3%;                    Mean loss: 3.1082\n",
      "Iteration: 2933; Percent done: 73.3%;                    Mean loss: 3.0537\n",
      "Iteration: 2934; Percent done: 73.4%;                    Mean loss: 3.2258\n",
      "Iteration: 2935; Percent done: 73.4%;                    Mean loss: 2.8060\n",
      "Iteration: 2936; Percent done: 73.4%;                    Mean loss: 3.1631\n",
      "Iteration: 2937; Percent done: 73.4%;                    Mean loss: 3.2084\n",
      "Iteration: 2938; Percent done: 73.5%;                    Mean loss: 2.9906\n",
      "Iteration: 2939; Percent done: 73.5%;                    Mean loss: 3.3298\n",
      "Iteration: 2940; Percent done: 73.5%;                    Mean loss: 3.1773\n",
      "Iteration: 2941; Percent done: 73.5%;                    Mean loss: 3.0708\n",
      "Iteration: 2942; Percent done: 73.6%;                    Mean loss: 3.1594\n",
      "Iteration: 2943; Percent done: 73.6%;                    Mean loss: 3.2018\n",
      "Iteration: 2944; Percent done: 73.6%;                    Mean loss: 3.0166\n",
      "Iteration: 2945; Percent done: 73.6%;                    Mean loss: 3.1455\n",
      "Iteration: 2946; Percent done: 73.7%;                    Mean loss: 2.9985\n",
      "Iteration: 2947; Percent done: 73.7%;                    Mean loss: 3.0403\n",
      "Iteration: 2948; Percent done: 73.7%;                    Mean loss: 3.0286\n",
      "Iteration: 2949; Percent done: 73.7%;                    Mean loss: 3.0608\n",
      "Iteration: 2950; Percent done: 73.8%;                    Mean loss: 3.2343\n",
      "Iteration: 2951; Percent done: 73.8%;                    Mean loss: 2.8928\n",
      "Iteration: 2952; Percent done: 73.8%;                    Mean loss: 3.0077\n",
      "Iteration: 2953; Percent done: 73.8%;                    Mean loss: 3.2962\n",
      "Iteration: 2954; Percent done: 73.9%;                    Mean loss: 2.7999\n",
      "Iteration: 2955; Percent done: 73.9%;                    Mean loss: 3.3064\n",
      "Iteration: 2956; Percent done: 73.9%;                    Mean loss: 3.1433\n",
      "Iteration: 2957; Percent done: 73.9%;                    Mean loss: 2.8281\n",
      "Iteration: 2958; Percent done: 74.0%;                    Mean loss: 2.8625\n",
      "Iteration: 2959; Percent done: 74.0%;                    Mean loss: 3.1352\n",
      "Iteration: 2960; Percent done: 74.0%;                    Mean loss: 2.9456\n",
      "Iteration: 2961; Percent done: 74.0%;                    Mean loss: 2.7932\n",
      "Iteration: 2962; Percent done: 74.1%;                    Mean loss: 3.0936\n",
      "Iteration: 2963; Percent done: 74.1%;                    Mean loss: 2.7589\n",
      "Iteration: 2964; Percent done: 74.1%;                    Mean loss: 2.9626\n",
      "Iteration: 2965; Percent done: 74.1%;                    Mean loss: 3.1522\n",
      "Iteration: 2966; Percent done: 74.2%;                    Mean loss: 3.1482\n",
      "Iteration: 2967; Percent done: 74.2%;                    Mean loss: 2.8851\n",
      "Iteration: 2968; Percent done: 74.2%;                    Mean loss: 3.0193\n",
      "Iteration: 2969; Percent done: 74.2%;                    Mean loss: 2.9771\n",
      "Iteration: 2970; Percent done: 74.2%;                    Mean loss: 3.1230\n",
      "Iteration: 2971; Percent done: 74.3%;                    Mean loss: 3.0887\n",
      "Iteration: 2972; Percent done: 74.3%;                    Mean loss: 3.0047\n",
      "Iteration: 2973; Percent done: 74.3%;                    Mean loss: 3.1736\n",
      "Iteration: 2974; Percent done: 74.4%;                    Mean loss: 3.1540\n",
      "Iteration: 2975; Percent done: 74.4%;                    Mean loss: 3.0355\n",
      "Iteration: 2976; Percent done: 74.4%;                    Mean loss: 3.1295\n",
      "Iteration: 2977; Percent done: 74.4%;                    Mean loss: 2.9613\n",
      "Iteration: 2978; Percent done: 74.5%;                    Mean loss: 3.0444\n",
      "Iteration: 2979; Percent done: 74.5%;                    Mean loss: 3.1680\n",
      "Iteration: 2980; Percent done: 74.5%;                    Mean loss: 3.1609\n",
      "Iteration: 2981; Percent done: 74.5%;                    Mean loss: 2.9298\n",
      "Iteration: 2982; Percent done: 74.6%;                    Mean loss: 3.1678\n",
      "Iteration: 2983; Percent done: 74.6%;                    Mean loss: 3.0218\n",
      "Iteration: 2984; Percent done: 74.6%;                    Mean loss: 2.9821\n",
      "Iteration: 2985; Percent done: 74.6%;                    Mean loss: 3.1815\n",
      "Iteration: 2986; Percent done: 74.7%;                    Mean loss: 2.8691\n",
      "Iteration: 2987; Percent done: 74.7%;                    Mean loss: 2.8575\n",
      "Iteration: 2988; Percent done: 74.7%;                    Mean loss: 3.1001\n",
      "Iteration: 2989; Percent done: 74.7%;                    Mean loss: 2.9157\n",
      "Iteration: 2990; Percent done: 74.8%;                    Mean loss: 3.1538\n",
      "Iteration: 2991; Percent done: 74.8%;                    Mean loss: 3.0261\n",
      "Iteration: 2992; Percent done: 74.8%;                    Mean loss: 3.2153\n",
      "Iteration: 2993; Percent done: 74.8%;                    Mean loss: 3.0531\n",
      "Iteration: 2994; Percent done: 74.9%;                    Mean loss: 3.1477\n",
      "Iteration: 2995; Percent done: 74.9%;                    Mean loss: 3.0555\n",
      "Iteration: 2996; Percent done: 74.9%;                    Mean loss: 2.8669\n",
      "Iteration: 2997; Percent done: 74.9%;                    Mean loss: 3.1288\n",
      "Iteration: 2998; Percent done: 75.0%;                    Mean loss: 3.0934\n",
      "Iteration: 2999; Percent done: 75.0%;                    Mean loss: 3.0478\n",
      "Iteration: 3000; Percent done: 75.0%;                    Mean loss: 2.8241\n",
      "Iteration: 3001; Percent done: 75.0%;                    Mean loss: 3.0927\n",
      "Iteration: 3002; Percent done: 75.0%;                    Mean loss: 3.2143\n",
      "Iteration: 3003; Percent done: 75.1%;                    Mean loss: 3.2168\n",
      "Iteration: 3004; Percent done: 75.1%;                    Mean loss: 3.0048\n",
      "Iteration: 3005; Percent done: 75.1%;                    Mean loss: 2.9603\n",
      "Iteration: 3006; Percent done: 75.1%;                    Mean loss: 2.9641\n",
      "Iteration: 3007; Percent done: 75.2%;                    Mean loss: 3.1286\n",
      "Iteration: 3008; Percent done: 75.2%;                    Mean loss: 2.9664\n",
      "Iteration: 3009; Percent done: 75.2%;                    Mean loss: 3.2365\n",
      "Iteration: 3010; Percent done: 75.2%;                    Mean loss: 2.9025\n",
      "Iteration: 3011; Percent done: 75.3%;                    Mean loss: 2.9280\n",
      "Iteration: 3012; Percent done: 75.3%;                    Mean loss: 3.1378\n",
      "Iteration: 3013; Percent done: 75.3%;                    Mean loss: 2.9457\n",
      "Iteration: 3014; Percent done: 75.3%;                    Mean loss: 3.1729\n",
      "Iteration: 3015; Percent done: 75.4%;                    Mean loss: 3.3279\n",
      "Iteration: 3016; Percent done: 75.4%;                    Mean loss: 2.9745\n",
      "Iteration: 3017; Percent done: 75.4%;                    Mean loss: 3.0596\n",
      "Iteration: 3018; Percent done: 75.4%;                    Mean loss: 2.9401\n",
      "Iteration: 3019; Percent done: 75.5%;                    Mean loss: 3.1456\n",
      "Iteration: 3020; Percent done: 75.5%;                    Mean loss: 3.3080\n",
      "Iteration: 3021; Percent done: 75.5%;                    Mean loss: 3.1850\n",
      "Iteration: 3022; Percent done: 75.5%;                    Mean loss: 3.1435\n",
      "Iteration: 3023; Percent done: 75.6%;                    Mean loss: 2.9705\n",
      "Iteration: 3024; Percent done: 75.6%;                    Mean loss: 2.9748\n",
      "Iteration: 3025; Percent done: 75.6%;                    Mean loss: 3.1089\n",
      "Iteration: 3026; Percent done: 75.6%;                    Mean loss: 3.1104\n",
      "Iteration: 3027; Percent done: 75.7%;                    Mean loss: 2.8238\n",
      "Iteration: 3028; Percent done: 75.7%;                    Mean loss: 3.0458\n",
      "Iteration: 3029; Percent done: 75.7%;                    Mean loss: 3.0479\n",
      "Iteration: 3030; Percent done: 75.8%;                    Mean loss: 2.6677\n",
      "Iteration: 3031; Percent done: 75.8%;                    Mean loss: 3.0833\n",
      "Iteration: 3032; Percent done: 75.8%;                    Mean loss: 2.8546\n",
      "Iteration: 3033; Percent done: 75.8%;                    Mean loss: 3.0486\n",
      "Iteration: 3034; Percent done: 75.8%;                    Mean loss: 3.2710\n",
      "Iteration: 3035; Percent done: 75.9%;                    Mean loss: 3.0390\n",
      "Iteration: 3036; Percent done: 75.9%;                    Mean loss: 2.9396\n",
      "Iteration: 3037; Percent done: 75.9%;                    Mean loss: 3.2989\n",
      "Iteration: 3038; Percent done: 75.9%;                    Mean loss: 3.0428\n",
      "Iteration: 3039; Percent done: 76.0%;                    Mean loss: 2.9752\n",
      "Iteration: 3040; Percent done: 76.0%;                    Mean loss: 3.0083\n",
      "Iteration: 3041; Percent done: 76.0%;                    Mean loss: 2.9263\n",
      "Iteration: 3042; Percent done: 76.0%;                    Mean loss: 2.9635\n",
      "Iteration: 3043; Percent done: 76.1%;                    Mean loss: 3.1590\n",
      "Iteration: 3044; Percent done: 76.1%;                    Mean loss: 3.1244\n",
      "Iteration: 3045; Percent done: 76.1%;                    Mean loss: 3.1448\n",
      "Iteration: 3046; Percent done: 76.1%;                    Mean loss: 3.3125\n",
      "Iteration: 3047; Percent done: 76.2%;                    Mean loss: 3.2092\n",
      "Iteration: 3048; Percent done: 76.2%;                    Mean loss: 2.9017\n",
      "Iteration: 3049; Percent done: 76.2%;                    Mean loss: 3.0161\n",
      "Iteration: 3050; Percent done: 76.2%;                    Mean loss: 2.6793\n",
      "Iteration: 3051; Percent done: 76.3%;                    Mean loss: 3.1430\n",
      "Iteration: 3052; Percent done: 76.3%;                    Mean loss: 2.8363\n",
      "Iteration: 3053; Percent done: 76.3%;                    Mean loss: 2.9657\n",
      "Iteration: 3054; Percent done: 76.3%;                    Mean loss: 3.2309\n",
      "Iteration: 3055; Percent done: 76.4%;                    Mean loss: 2.9735\n",
      "Iteration: 3056; Percent done: 76.4%;                    Mean loss: 3.3540\n",
      "Iteration: 3057; Percent done: 76.4%;                    Mean loss: 2.9947\n",
      "Iteration: 3058; Percent done: 76.4%;                    Mean loss: 3.1860\n",
      "Iteration: 3059; Percent done: 76.5%;                    Mean loss: 2.9676\n",
      "Iteration: 3060; Percent done: 76.5%;                    Mean loss: 3.2162\n",
      "Iteration: 3061; Percent done: 76.5%;                    Mean loss: 2.9492\n",
      "Iteration: 3062; Percent done: 76.5%;                    Mean loss: 2.9783\n",
      "Iteration: 3063; Percent done: 76.6%;                    Mean loss: 3.1899\n",
      "Iteration: 3064; Percent done: 76.6%;                    Mean loss: 3.2573\n",
      "Iteration: 3065; Percent done: 76.6%;                    Mean loss: 3.0510\n",
      "Iteration: 3066; Percent done: 76.6%;                    Mean loss: 3.2643\n",
      "Iteration: 3067; Percent done: 76.7%;                    Mean loss: 3.0833\n",
      "Iteration: 3068; Percent done: 76.7%;                    Mean loss: 3.3672\n",
      "Iteration: 3069; Percent done: 76.7%;                    Mean loss: 3.2744\n",
      "Iteration: 3070; Percent done: 76.8%;                    Mean loss: 3.1620\n",
      "Iteration: 3071; Percent done: 76.8%;                    Mean loss: 3.1926\n",
      "Iteration: 3072; Percent done: 76.8%;                    Mean loss: 3.0060\n",
      "Iteration: 3073; Percent done: 76.8%;                    Mean loss: 2.8386\n",
      "Iteration: 3074; Percent done: 76.8%;                    Mean loss: 2.8448\n",
      "Iteration: 3075; Percent done: 76.9%;                    Mean loss: 3.0132\n",
      "Iteration: 3076; Percent done: 76.9%;                    Mean loss: 3.0653\n",
      "Iteration: 3077; Percent done: 76.9%;                    Mean loss: 3.1836\n",
      "Iteration: 3078; Percent done: 77.0%;                    Mean loss: 3.0560\n",
      "Iteration: 3079; Percent done: 77.0%;                    Mean loss: 2.9988\n",
      "Iteration: 3080; Percent done: 77.0%;                    Mean loss: 3.0183\n",
      "Iteration: 3081; Percent done: 77.0%;                    Mean loss: 3.0094\n",
      "Iteration: 3082; Percent done: 77.0%;                    Mean loss: 3.4104\n",
      "Iteration: 3083; Percent done: 77.1%;                    Mean loss: 2.8676\n",
      "Iteration: 3084; Percent done: 77.1%;                    Mean loss: 2.9182\n",
      "Iteration: 3085; Percent done: 77.1%;                    Mean loss: 3.1090\n",
      "Iteration: 3086; Percent done: 77.1%;                    Mean loss: 3.0274\n",
      "Iteration: 3087; Percent done: 77.2%;                    Mean loss: 3.1788\n",
      "Iteration: 3088; Percent done: 77.2%;                    Mean loss: 3.1013\n",
      "Iteration: 3089; Percent done: 77.2%;                    Mean loss: 3.1473\n",
      "Iteration: 3090; Percent done: 77.2%;                    Mean loss: 3.0312\n",
      "Iteration: 3091; Percent done: 77.3%;                    Mean loss: 3.0096\n",
      "Iteration: 3092; Percent done: 77.3%;                    Mean loss: 3.1863\n",
      "Iteration: 3093; Percent done: 77.3%;                    Mean loss: 3.0817\n",
      "Iteration: 3094; Percent done: 77.3%;                    Mean loss: 3.1679\n",
      "Iteration: 3095; Percent done: 77.4%;                    Mean loss: 2.9211\n",
      "Iteration: 3096; Percent done: 77.4%;                    Mean loss: 2.9024\n",
      "Iteration: 3097; Percent done: 77.4%;                    Mean loss: 2.8066\n",
      "Iteration: 3098; Percent done: 77.5%;                    Mean loss: 3.0822\n",
      "Iteration: 3099; Percent done: 77.5%;                    Mean loss: 3.1233\n",
      "Iteration: 3100; Percent done: 77.5%;                    Mean loss: 2.8801\n",
      "Iteration: 3101; Percent done: 77.5%;                    Mean loss: 2.9762\n",
      "Iteration: 3102; Percent done: 77.5%;                    Mean loss: 3.0822\n",
      "Iteration: 3103; Percent done: 77.6%;                    Mean loss: 3.2628\n",
      "Iteration: 3104; Percent done: 77.6%;                    Mean loss: 2.9754\n",
      "Iteration: 3105; Percent done: 77.6%;                    Mean loss: 2.9700\n",
      "Iteration: 3106; Percent done: 77.6%;                    Mean loss: 2.9900\n",
      "Iteration: 3107; Percent done: 77.7%;                    Mean loss: 3.0251\n",
      "Iteration: 3108; Percent done: 77.7%;                    Mean loss: 2.9717\n",
      "Iteration: 3109; Percent done: 77.7%;                    Mean loss: 2.9963\n",
      "Iteration: 3110; Percent done: 77.8%;                    Mean loss: 3.1245\n",
      "Iteration: 3111; Percent done: 77.8%;                    Mean loss: 3.0419\n",
      "Iteration: 3112; Percent done: 77.8%;                    Mean loss: 2.9733\n",
      "Iteration: 3113; Percent done: 77.8%;                    Mean loss: 3.0421\n",
      "Iteration: 3114; Percent done: 77.8%;                    Mean loss: 2.9358\n",
      "Iteration: 3115; Percent done: 77.9%;                    Mean loss: 2.8765\n",
      "Iteration: 3116; Percent done: 77.9%;                    Mean loss: 3.0853\n",
      "Iteration: 3117; Percent done: 77.9%;                    Mean loss: 3.1427\n",
      "Iteration: 3118; Percent done: 78.0%;                    Mean loss: 2.8498\n",
      "Iteration: 3119; Percent done: 78.0%;                    Mean loss: 2.7519\n",
      "Iteration: 3120; Percent done: 78.0%;                    Mean loss: 2.9151\n",
      "Iteration: 3121; Percent done: 78.0%;                    Mean loss: 2.8567\n",
      "Iteration: 3122; Percent done: 78.0%;                    Mean loss: 2.8515\n",
      "Iteration: 3123; Percent done: 78.1%;                    Mean loss: 2.9604\n",
      "Iteration: 3124; Percent done: 78.1%;                    Mean loss: 3.0588\n",
      "Iteration: 3125; Percent done: 78.1%;                    Mean loss: 2.8440\n",
      "Iteration: 3126; Percent done: 78.1%;                    Mean loss: 3.1301\n",
      "Iteration: 3127; Percent done: 78.2%;                    Mean loss: 3.1297\n",
      "Iteration: 3128; Percent done: 78.2%;                    Mean loss: 3.1495\n",
      "Iteration: 3129; Percent done: 78.2%;                    Mean loss: 2.8953\n",
      "Iteration: 3130; Percent done: 78.2%;                    Mean loss: 2.8859\n",
      "Iteration: 3131; Percent done: 78.3%;                    Mean loss: 2.7871\n",
      "Iteration: 3132; Percent done: 78.3%;                    Mean loss: 2.9837\n",
      "Iteration: 3133; Percent done: 78.3%;                    Mean loss: 3.2945\n",
      "Iteration: 3134; Percent done: 78.3%;                    Mean loss: 2.7758\n",
      "Iteration: 3135; Percent done: 78.4%;                    Mean loss: 2.9932\n",
      "Iteration: 3136; Percent done: 78.4%;                    Mean loss: 3.0346\n",
      "Iteration: 3137; Percent done: 78.4%;                    Mean loss: 3.0889\n",
      "Iteration: 3138; Percent done: 78.5%;                    Mean loss: 2.8759\n",
      "Iteration: 3139; Percent done: 78.5%;                    Mean loss: 2.7623\n",
      "Iteration: 3140; Percent done: 78.5%;                    Mean loss: 2.9794\n",
      "Iteration: 3141; Percent done: 78.5%;                    Mean loss: 3.1300\n",
      "Iteration: 3142; Percent done: 78.5%;                    Mean loss: 3.0630\n",
      "Iteration: 3143; Percent done: 78.6%;                    Mean loss: 3.2112\n",
      "Iteration: 3144; Percent done: 78.6%;                    Mean loss: 2.8754\n",
      "Iteration: 3145; Percent done: 78.6%;                    Mean loss: 3.1976\n",
      "Iteration: 3146; Percent done: 78.6%;                    Mean loss: 3.1170\n",
      "Iteration: 3147; Percent done: 78.7%;                    Mean loss: 2.9808\n",
      "Iteration: 3148; Percent done: 78.7%;                    Mean loss: 2.9617\n",
      "Iteration: 3149; Percent done: 78.7%;                    Mean loss: 2.9206\n",
      "Iteration: 3150; Percent done: 78.8%;                    Mean loss: 3.1310\n",
      "Iteration: 3151; Percent done: 78.8%;                    Mean loss: 2.9959\n",
      "Iteration: 3152; Percent done: 78.8%;                    Mean loss: 2.9467\n",
      "Iteration: 3153; Percent done: 78.8%;                    Mean loss: 2.8897\n",
      "Iteration: 3154; Percent done: 78.8%;                    Mean loss: 2.9515\n",
      "Iteration: 3155; Percent done: 78.9%;                    Mean loss: 2.8966\n",
      "Iteration: 3156; Percent done: 78.9%;                    Mean loss: 3.0668\n",
      "Iteration: 3157; Percent done: 78.9%;                    Mean loss: 2.9882\n",
      "Iteration: 3158; Percent done: 79.0%;                    Mean loss: 3.2833\n",
      "Iteration: 3159; Percent done: 79.0%;                    Mean loss: 3.0882\n",
      "Iteration: 3160; Percent done: 79.0%;                    Mean loss: 3.0750\n",
      "Iteration: 3161; Percent done: 79.0%;                    Mean loss: 2.8507\n",
      "Iteration: 3162; Percent done: 79.0%;                    Mean loss: 3.1146\n",
      "Iteration: 3163; Percent done: 79.1%;                    Mean loss: 2.8421\n",
      "Iteration: 3164; Percent done: 79.1%;                    Mean loss: 2.7579\n",
      "Iteration: 3165; Percent done: 79.1%;                    Mean loss: 2.8623\n",
      "Iteration: 3166; Percent done: 79.1%;                    Mean loss: 2.9820\n",
      "Iteration: 3167; Percent done: 79.2%;                    Mean loss: 2.9051\n",
      "Iteration: 3168; Percent done: 79.2%;                    Mean loss: 2.9286\n",
      "Iteration: 3169; Percent done: 79.2%;                    Mean loss: 2.9103\n",
      "Iteration: 3170; Percent done: 79.2%;                    Mean loss: 3.1348\n",
      "Iteration: 3171; Percent done: 79.3%;                    Mean loss: 3.0610\n",
      "Iteration: 3172; Percent done: 79.3%;                    Mean loss: 3.0992\n",
      "Iteration: 3173; Percent done: 79.3%;                    Mean loss: 3.1235\n",
      "Iteration: 3174; Percent done: 79.3%;                    Mean loss: 3.1121\n",
      "Iteration: 3175; Percent done: 79.4%;                    Mean loss: 2.8715\n",
      "Iteration: 3176; Percent done: 79.4%;                    Mean loss: 2.9812\n",
      "Iteration: 3177; Percent done: 79.4%;                    Mean loss: 3.1429\n",
      "Iteration: 3178; Percent done: 79.5%;                    Mean loss: 2.8654\n",
      "Iteration: 3179; Percent done: 79.5%;                    Mean loss: 2.8211\n",
      "Iteration: 3180; Percent done: 79.5%;                    Mean loss: 2.7936\n",
      "Iteration: 3181; Percent done: 79.5%;                    Mean loss: 3.1246\n",
      "Iteration: 3182; Percent done: 79.5%;                    Mean loss: 3.1277\n",
      "Iteration: 3183; Percent done: 79.6%;                    Mean loss: 2.7351\n",
      "Iteration: 3184; Percent done: 79.6%;                    Mean loss: 3.0816\n",
      "Iteration: 3185; Percent done: 79.6%;                    Mean loss: 2.7933\n",
      "Iteration: 3186; Percent done: 79.7%;                    Mean loss: 3.0343\n",
      "Iteration: 3187; Percent done: 79.7%;                    Mean loss: 2.7675\n",
      "Iteration: 3188; Percent done: 79.7%;                    Mean loss: 2.7915\n",
      "Iteration: 3189; Percent done: 79.7%;                    Mean loss: 2.8180\n",
      "Iteration: 3190; Percent done: 79.8%;                    Mean loss: 2.7764\n",
      "Iteration: 3191; Percent done: 79.8%;                    Mean loss: 3.2783\n",
      "Iteration: 3192; Percent done: 79.8%;                    Mean loss: 2.9887\n",
      "Iteration: 3193; Percent done: 79.8%;                    Mean loss: 2.9071\n",
      "Iteration: 3194; Percent done: 79.8%;                    Mean loss: 3.2381\n",
      "Iteration: 3195; Percent done: 79.9%;                    Mean loss: 3.1346\n",
      "Iteration: 3196; Percent done: 79.9%;                    Mean loss: 3.1995\n",
      "Iteration: 3197; Percent done: 79.9%;                    Mean loss: 2.9209\n",
      "Iteration: 3198; Percent done: 80.0%;                    Mean loss: 3.1689\n",
      "Iteration: 3199; Percent done: 80.0%;                    Mean loss: 2.8793\n",
      "Iteration: 3200; Percent done: 80.0%;                    Mean loss: 3.0696\n",
      "Iteration: 3201; Percent done: 80.0%;                    Mean loss: 2.9016\n",
      "Iteration: 3202; Percent done: 80.0%;                    Mean loss: 3.0940\n",
      "Iteration: 3203; Percent done: 80.1%;                    Mean loss: 2.7925\n",
      "Iteration: 3204; Percent done: 80.1%;                    Mean loss: 2.9822\n",
      "Iteration: 3205; Percent done: 80.1%;                    Mean loss: 2.8800\n",
      "Iteration: 3206; Percent done: 80.2%;                    Mean loss: 2.8761\n",
      "Iteration: 3207; Percent done: 80.2%;                    Mean loss: 2.8894\n",
      "Iteration: 3208; Percent done: 80.2%;                    Mean loss: 2.7498\n",
      "Iteration: 3209; Percent done: 80.2%;                    Mean loss: 2.8031\n",
      "Iteration: 3210; Percent done: 80.2%;                    Mean loss: 2.9327\n",
      "Iteration: 3211; Percent done: 80.3%;                    Mean loss: 3.0565\n",
      "Iteration: 3212; Percent done: 80.3%;                    Mean loss: 3.0729\n",
      "Iteration: 3213; Percent done: 80.3%;                    Mean loss: 2.9177\n",
      "Iteration: 3214; Percent done: 80.3%;                    Mean loss: 2.8861\n",
      "Iteration: 3215; Percent done: 80.4%;                    Mean loss: 2.7491\n",
      "Iteration: 3216; Percent done: 80.4%;                    Mean loss: 3.1414\n",
      "Iteration: 3217; Percent done: 80.4%;                    Mean loss: 2.8904\n",
      "Iteration: 3218; Percent done: 80.5%;                    Mean loss: 2.8961\n",
      "Iteration: 3219; Percent done: 80.5%;                    Mean loss: 2.9870\n",
      "Iteration: 3220; Percent done: 80.5%;                    Mean loss: 2.8917\n",
      "Iteration: 3221; Percent done: 80.5%;                    Mean loss: 2.9143\n",
      "Iteration: 3222; Percent done: 80.5%;                    Mean loss: 2.7848\n",
      "Iteration: 3223; Percent done: 80.6%;                    Mean loss: 2.9962\n",
      "Iteration: 3224; Percent done: 80.6%;                    Mean loss: 2.9326\n",
      "Iteration: 3225; Percent done: 80.6%;                    Mean loss: 3.1639\n",
      "Iteration: 3226; Percent done: 80.7%;                    Mean loss: 2.9797\n",
      "Iteration: 3227; Percent done: 80.7%;                    Mean loss: 3.3477\n",
      "Iteration: 3228; Percent done: 80.7%;                    Mean loss: 2.8094\n",
      "Iteration: 3229; Percent done: 80.7%;                    Mean loss: 3.1702\n",
      "Iteration: 3230; Percent done: 80.8%;                    Mean loss: 2.8700\n",
      "Iteration: 3231; Percent done: 80.8%;                    Mean loss: 3.0499\n",
      "Iteration: 3232; Percent done: 80.8%;                    Mean loss: 2.8180\n",
      "Iteration: 3233; Percent done: 80.8%;                    Mean loss: 2.7935\n",
      "Iteration: 3234; Percent done: 80.8%;                    Mean loss: 3.0607\n",
      "Iteration: 3235; Percent done: 80.9%;                    Mean loss: 2.8447\n",
      "Iteration: 3236; Percent done: 80.9%;                    Mean loss: 2.8127\n",
      "Iteration: 3237; Percent done: 80.9%;                    Mean loss: 3.0301\n",
      "Iteration: 3238; Percent done: 81.0%;                    Mean loss: 2.8696\n",
      "Iteration: 3239; Percent done: 81.0%;                    Mean loss: 2.9806\n",
      "Iteration: 3240; Percent done: 81.0%;                    Mean loss: 2.8512\n",
      "Iteration: 3241; Percent done: 81.0%;                    Mean loss: 2.7221\n",
      "Iteration: 3242; Percent done: 81.0%;                    Mean loss: 3.2638\n",
      "Iteration: 3243; Percent done: 81.1%;                    Mean loss: 3.1079\n",
      "Iteration: 3244; Percent done: 81.1%;                    Mean loss: 2.8232\n",
      "Iteration: 3245; Percent done: 81.1%;                    Mean loss: 2.9482\n",
      "Iteration: 3246; Percent done: 81.2%;                    Mean loss: 3.0955\n",
      "Iteration: 3247; Percent done: 81.2%;                    Mean loss: 3.0923\n",
      "Iteration: 3248; Percent done: 81.2%;                    Mean loss: 2.9339\n",
      "Iteration: 3249; Percent done: 81.2%;                    Mean loss: 2.8543\n",
      "Iteration: 3250; Percent done: 81.2%;                    Mean loss: 3.0566\n",
      "Iteration: 3251; Percent done: 81.3%;                    Mean loss: 3.0035\n",
      "Iteration: 3252; Percent done: 81.3%;                    Mean loss: 2.7637\n",
      "Iteration: 3253; Percent done: 81.3%;                    Mean loss: 3.2390\n",
      "Iteration: 3254; Percent done: 81.3%;                    Mean loss: 2.9919\n",
      "Iteration: 3255; Percent done: 81.4%;                    Mean loss: 2.8832\n",
      "Iteration: 3256; Percent done: 81.4%;                    Mean loss: 2.9944\n",
      "Iteration: 3257; Percent done: 81.4%;                    Mean loss: 2.9748\n",
      "Iteration: 3258; Percent done: 81.5%;                    Mean loss: 2.8297\n",
      "Iteration: 3259; Percent done: 81.5%;                    Mean loss: 2.9397\n",
      "Iteration: 3260; Percent done: 81.5%;                    Mean loss: 2.9861\n",
      "Iteration: 3261; Percent done: 81.5%;                    Mean loss: 3.0223\n",
      "Iteration: 3262; Percent done: 81.5%;                    Mean loss: 3.0531\n",
      "Iteration: 3263; Percent done: 81.6%;                    Mean loss: 2.9622\n",
      "Iteration: 3264; Percent done: 81.6%;                    Mean loss: 2.8249\n",
      "Iteration: 3265; Percent done: 81.6%;                    Mean loss: 3.1595\n",
      "Iteration: 3266; Percent done: 81.7%;                    Mean loss: 3.0909\n",
      "Iteration: 3267; Percent done: 81.7%;                    Mean loss: 2.9348\n",
      "Iteration: 3268; Percent done: 81.7%;                    Mean loss: 3.2575\n",
      "Iteration: 3269; Percent done: 81.7%;                    Mean loss: 2.8955\n",
      "Iteration: 3270; Percent done: 81.8%;                    Mean loss: 2.9398\n",
      "Iteration: 3271; Percent done: 81.8%;                    Mean loss: 2.8037\n",
      "Iteration: 3272; Percent done: 81.8%;                    Mean loss: 3.1376\n",
      "Iteration: 3273; Percent done: 81.8%;                    Mean loss: 3.1834\n",
      "Iteration: 3274; Percent done: 81.8%;                    Mean loss: 3.0318\n",
      "Iteration: 3275; Percent done: 81.9%;                    Mean loss: 3.1313\n",
      "Iteration: 3276; Percent done: 81.9%;                    Mean loss: 3.2015\n",
      "Iteration: 3277; Percent done: 81.9%;                    Mean loss: 3.2281\n",
      "Iteration: 3278; Percent done: 82.0%;                    Mean loss: 3.1059\n",
      "Iteration: 3279; Percent done: 82.0%;                    Mean loss: 3.0393\n",
      "Iteration: 3280; Percent done: 82.0%;                    Mean loss: 3.2154\n",
      "Iteration: 3281; Percent done: 82.0%;                    Mean loss: 2.9288\n",
      "Iteration: 3282; Percent done: 82.0%;                    Mean loss: 2.9671\n",
      "Iteration: 3283; Percent done: 82.1%;                    Mean loss: 2.9126\n",
      "Iteration: 3284; Percent done: 82.1%;                    Mean loss: 3.1105\n",
      "Iteration: 3285; Percent done: 82.1%;                    Mean loss: 2.9435\n",
      "Iteration: 3286; Percent done: 82.2%;                    Mean loss: 3.2954\n",
      "Iteration: 3287; Percent done: 82.2%;                    Mean loss: 3.0106\n",
      "Iteration: 3288; Percent done: 82.2%;                    Mean loss: 2.8989\n",
      "Iteration: 3289; Percent done: 82.2%;                    Mean loss: 2.9979\n",
      "Iteration: 3290; Percent done: 82.2%;                    Mean loss: 3.0080\n",
      "Iteration: 3291; Percent done: 82.3%;                    Mean loss: 2.8425\n",
      "Iteration: 3292; Percent done: 82.3%;                    Mean loss: 2.8837\n",
      "Iteration: 3293; Percent done: 82.3%;                    Mean loss: 3.1137\n",
      "Iteration: 3294; Percent done: 82.3%;                    Mean loss: 2.9950\n",
      "Iteration: 3295; Percent done: 82.4%;                    Mean loss: 2.9098\n",
      "Iteration: 3296; Percent done: 82.4%;                    Mean loss: 2.7402\n",
      "Iteration: 3297; Percent done: 82.4%;                    Mean loss: 3.2775\n",
      "Iteration: 3298; Percent done: 82.5%;                    Mean loss: 3.1497\n",
      "Iteration: 3299; Percent done: 82.5%;                    Mean loss: 2.8232\n",
      "Iteration: 3300; Percent done: 82.5%;                    Mean loss: 3.0241\n",
      "Iteration: 3301; Percent done: 82.5%;                    Mean loss: 2.8218\n",
      "Iteration: 3302; Percent done: 82.5%;                    Mean loss: 3.1257\n",
      "Iteration: 3303; Percent done: 82.6%;                    Mean loss: 3.0193\n",
      "Iteration: 3304; Percent done: 82.6%;                    Mean loss: 3.2999\n",
      "Iteration: 3305; Percent done: 82.6%;                    Mean loss: 2.9609\n",
      "Iteration: 3306; Percent done: 82.7%;                    Mean loss: 3.0049\n",
      "Iteration: 3307; Percent done: 82.7%;                    Mean loss: 3.2205\n",
      "Iteration: 3308; Percent done: 82.7%;                    Mean loss: 2.8842\n",
      "Iteration: 3309; Percent done: 82.7%;                    Mean loss: 3.0697\n",
      "Iteration: 3310; Percent done: 82.8%;                    Mean loss: 2.9227\n",
      "Iteration: 3311; Percent done: 82.8%;                    Mean loss: 2.8466\n",
      "Iteration: 3312; Percent done: 82.8%;                    Mean loss: 2.9574\n",
      "Iteration: 3313; Percent done: 82.8%;                    Mean loss: 2.9525\n",
      "Iteration: 3314; Percent done: 82.8%;                    Mean loss: 2.9921\n",
      "Iteration: 3315; Percent done: 82.9%;                    Mean loss: 2.8761\n",
      "Iteration: 3316; Percent done: 82.9%;                    Mean loss: 2.7946\n",
      "Iteration: 3317; Percent done: 82.9%;                    Mean loss: 2.7226\n",
      "Iteration: 3318; Percent done: 83.0%;                    Mean loss: 3.2282\n",
      "Iteration: 3319; Percent done: 83.0%;                    Mean loss: 2.9785\n",
      "Iteration: 3320; Percent done: 83.0%;                    Mean loss: 3.0099\n",
      "Iteration: 3321; Percent done: 83.0%;                    Mean loss: 3.1621\n",
      "Iteration: 3322; Percent done: 83.0%;                    Mean loss: 3.2419\n",
      "Iteration: 3323; Percent done: 83.1%;                    Mean loss: 2.9286\n",
      "Iteration: 3324; Percent done: 83.1%;                    Mean loss: 2.6950\n",
      "Iteration: 3325; Percent done: 83.1%;                    Mean loss: 2.8037\n",
      "Iteration: 3326; Percent done: 83.2%;                    Mean loss: 3.0208\n",
      "Iteration: 3327; Percent done: 83.2%;                    Mean loss: 2.9700\n",
      "Iteration: 3328; Percent done: 83.2%;                    Mean loss: 2.9428\n",
      "Iteration: 3329; Percent done: 83.2%;                    Mean loss: 3.1630\n",
      "Iteration: 3330; Percent done: 83.2%;                    Mean loss: 3.0110\n",
      "Iteration: 3331; Percent done: 83.3%;                    Mean loss: 3.1174\n",
      "Iteration: 3332; Percent done: 83.3%;                    Mean loss: 2.8042\n",
      "Iteration: 3333; Percent done: 83.3%;                    Mean loss: 2.9753\n",
      "Iteration: 3334; Percent done: 83.4%;                    Mean loss: 2.7471\n",
      "Iteration: 3335; Percent done: 83.4%;                    Mean loss: 3.1720\n",
      "Iteration: 3336; Percent done: 83.4%;                    Mean loss: 2.9824\n",
      "Iteration: 3337; Percent done: 83.4%;                    Mean loss: 3.0650\n",
      "Iteration: 3338; Percent done: 83.5%;                    Mean loss: 2.9164\n",
      "Iteration: 3339; Percent done: 83.5%;                    Mean loss: 3.1316\n",
      "Iteration: 3340; Percent done: 83.5%;                    Mean loss: 3.1528\n",
      "Iteration: 3341; Percent done: 83.5%;                    Mean loss: 2.9494\n",
      "Iteration: 3342; Percent done: 83.5%;                    Mean loss: 2.9939\n",
      "Iteration: 3343; Percent done: 83.6%;                    Mean loss: 2.8612\n",
      "Iteration: 3344; Percent done: 83.6%;                    Mean loss: 2.7782\n",
      "Iteration: 3345; Percent done: 83.6%;                    Mean loss: 3.0360\n",
      "Iteration: 3346; Percent done: 83.7%;                    Mean loss: 2.9125\n",
      "Iteration: 3347; Percent done: 83.7%;                    Mean loss: 2.8025\n",
      "Iteration: 3348; Percent done: 83.7%;                    Mean loss: 2.8243\n",
      "Iteration: 3349; Percent done: 83.7%;                    Mean loss: 2.9440\n",
      "Iteration: 3350; Percent done: 83.8%;                    Mean loss: 2.9007\n",
      "Iteration: 3351; Percent done: 83.8%;                    Mean loss: 3.0900\n",
      "Iteration: 3352; Percent done: 83.8%;                    Mean loss: 2.8599\n",
      "Iteration: 3353; Percent done: 83.8%;                    Mean loss: 2.9659\n",
      "Iteration: 3354; Percent done: 83.9%;                    Mean loss: 2.9555\n",
      "Iteration: 3355; Percent done: 83.9%;                    Mean loss: 2.9237\n",
      "Iteration: 3356; Percent done: 83.9%;                    Mean loss: 2.9445\n",
      "Iteration: 3357; Percent done: 83.9%;                    Mean loss: 3.0347\n",
      "Iteration: 3358; Percent done: 84.0%;                    Mean loss: 3.0217\n",
      "Iteration: 3359; Percent done: 84.0%;                    Mean loss: 2.8013\n",
      "Iteration: 3360; Percent done: 84.0%;                    Mean loss: 3.1899\n",
      "Iteration: 3361; Percent done: 84.0%;                    Mean loss: 2.9862\n",
      "Iteration: 3362; Percent done: 84.0%;                    Mean loss: 2.7772\n",
      "Iteration: 3363; Percent done: 84.1%;                    Mean loss: 3.1277\n",
      "Iteration: 3364; Percent done: 84.1%;                    Mean loss: 2.8458\n",
      "Iteration: 3365; Percent done: 84.1%;                    Mean loss: 2.8793\n",
      "Iteration: 3366; Percent done: 84.2%;                    Mean loss: 2.8323\n",
      "Iteration: 3367; Percent done: 84.2%;                    Mean loss: 3.1037\n",
      "Iteration: 3368; Percent done: 84.2%;                    Mean loss: 2.8126\n",
      "Iteration: 3369; Percent done: 84.2%;                    Mean loss: 3.0420\n",
      "Iteration: 3370; Percent done: 84.2%;                    Mean loss: 3.1194\n",
      "Iteration: 3371; Percent done: 84.3%;                    Mean loss: 2.9805\n",
      "Iteration: 3372; Percent done: 84.3%;                    Mean loss: 2.9164\n",
      "Iteration: 3373; Percent done: 84.3%;                    Mean loss: 2.8315\n",
      "Iteration: 3374; Percent done: 84.4%;                    Mean loss: 2.8798\n",
      "Iteration: 3375; Percent done: 84.4%;                    Mean loss: 3.0343\n",
      "Iteration: 3376; Percent done: 84.4%;                    Mean loss: 2.9978\n",
      "Iteration: 3377; Percent done: 84.4%;                    Mean loss: 2.9247\n",
      "Iteration: 3378; Percent done: 84.5%;                    Mean loss: 2.8075\n",
      "Iteration: 3379; Percent done: 84.5%;                    Mean loss: 3.0480\n",
      "Iteration: 3380; Percent done: 84.5%;                    Mean loss: 2.8885\n",
      "Iteration: 3381; Percent done: 84.5%;                    Mean loss: 2.8416\n",
      "Iteration: 3382; Percent done: 84.5%;                    Mean loss: 3.0418\n",
      "Iteration: 3383; Percent done: 84.6%;                    Mean loss: 3.0217\n",
      "Iteration: 3384; Percent done: 84.6%;                    Mean loss: 2.7811\n",
      "Iteration: 3385; Percent done: 84.6%;                    Mean loss: 2.9839\n",
      "Iteration: 3386; Percent done: 84.7%;                    Mean loss: 2.9599\n",
      "Iteration: 3387; Percent done: 84.7%;                    Mean loss: 2.8823\n",
      "Iteration: 3388; Percent done: 84.7%;                    Mean loss: 3.0011\n",
      "Iteration: 3389; Percent done: 84.7%;                    Mean loss: 3.0174\n",
      "Iteration: 3390; Percent done: 84.8%;                    Mean loss: 2.8229\n",
      "Iteration: 3391; Percent done: 84.8%;                    Mean loss: 2.8219\n",
      "Iteration: 3392; Percent done: 84.8%;                    Mean loss: 2.9530\n",
      "Iteration: 3393; Percent done: 84.8%;                    Mean loss: 2.9385\n",
      "Iteration: 3394; Percent done: 84.9%;                    Mean loss: 2.8563\n",
      "Iteration: 3395; Percent done: 84.9%;                    Mean loss: 2.8262\n",
      "Iteration: 3396; Percent done: 84.9%;                    Mean loss: 3.1062\n",
      "Iteration: 3397; Percent done: 84.9%;                    Mean loss: 2.9583\n",
      "Iteration: 3398; Percent done: 85.0%;                    Mean loss: 2.7528\n",
      "Iteration: 3399; Percent done: 85.0%;                    Mean loss: 2.9374\n",
      "Iteration: 3400; Percent done: 85.0%;                    Mean loss: 3.2123\n",
      "Iteration: 3401; Percent done: 85.0%;                    Mean loss: 2.5671\n",
      "Iteration: 3402; Percent done: 85.0%;                    Mean loss: 2.7605\n",
      "Iteration: 3403; Percent done: 85.1%;                    Mean loss: 2.9696\n",
      "Iteration: 3404; Percent done: 85.1%;                    Mean loss: 3.1418\n",
      "Iteration: 3405; Percent done: 85.1%;                    Mean loss: 2.9062\n",
      "Iteration: 3406; Percent done: 85.2%;                    Mean loss: 3.0475\n",
      "Iteration: 3407; Percent done: 85.2%;                    Mean loss: 2.9330\n",
      "Iteration: 3408; Percent done: 85.2%;                    Mean loss: 2.7289\n",
      "Iteration: 3409; Percent done: 85.2%;                    Mean loss: 2.8846\n",
      "Iteration: 3410; Percent done: 85.2%;                    Mean loss: 2.7817\n",
      "Iteration: 3411; Percent done: 85.3%;                    Mean loss: 2.8469\n",
      "Iteration: 3412; Percent done: 85.3%;                    Mean loss: 2.9280\n",
      "Iteration: 3413; Percent done: 85.3%;                    Mean loss: 2.8226\n",
      "Iteration: 3414; Percent done: 85.4%;                    Mean loss: 2.8640\n",
      "Iteration: 3415; Percent done: 85.4%;                    Mean loss: 2.7719\n",
      "Iteration: 3416; Percent done: 85.4%;                    Mean loss: 2.5572\n",
      "Iteration: 3417; Percent done: 85.4%;                    Mean loss: 2.9785\n",
      "Iteration: 3418; Percent done: 85.5%;                    Mean loss: 2.7564\n",
      "Iteration: 3419; Percent done: 85.5%;                    Mean loss: 2.9002\n",
      "Iteration: 3420; Percent done: 85.5%;                    Mean loss: 3.0266\n",
      "Iteration: 3421; Percent done: 85.5%;                    Mean loss: 2.9500\n",
      "Iteration: 3422; Percent done: 85.5%;                    Mean loss: 3.0429\n",
      "Iteration: 3423; Percent done: 85.6%;                    Mean loss: 2.7499\n",
      "Iteration: 3424; Percent done: 85.6%;                    Mean loss: 2.9028\n",
      "Iteration: 3425; Percent done: 85.6%;                    Mean loss: 3.0416\n",
      "Iteration: 3426; Percent done: 85.7%;                    Mean loss: 2.8792\n",
      "Iteration: 3427; Percent done: 85.7%;                    Mean loss: 2.6752\n",
      "Iteration: 3428; Percent done: 85.7%;                    Mean loss: 2.6822\n",
      "Iteration: 3429; Percent done: 85.7%;                    Mean loss: 2.9176\n",
      "Iteration: 3430; Percent done: 85.8%;                    Mean loss: 2.9636\n",
      "Iteration: 3431; Percent done: 85.8%;                    Mean loss: 2.9569\n",
      "Iteration: 3432; Percent done: 85.8%;                    Mean loss: 2.8153\n",
      "Iteration: 3433; Percent done: 85.8%;                    Mean loss: 2.8393\n",
      "Iteration: 3434; Percent done: 85.9%;                    Mean loss: 2.8747\n",
      "Iteration: 3435; Percent done: 85.9%;                    Mean loss: 3.0791\n",
      "Iteration: 3436; Percent done: 85.9%;                    Mean loss: 2.7422\n",
      "Iteration: 3437; Percent done: 85.9%;                    Mean loss: 3.0474\n",
      "Iteration: 3438; Percent done: 86.0%;                    Mean loss: 2.6743\n",
      "Iteration: 3439; Percent done: 86.0%;                    Mean loss: 2.7328\n",
      "Iteration: 3440; Percent done: 86.0%;                    Mean loss: 3.1109\n",
      "Iteration: 3441; Percent done: 86.0%;                    Mean loss: 2.6979\n",
      "Iteration: 3442; Percent done: 86.1%;                    Mean loss: 2.8999\n",
      "Iteration: 3443; Percent done: 86.1%;                    Mean loss: 2.7411\n",
      "Iteration: 3444; Percent done: 86.1%;                    Mean loss: 3.1250\n",
      "Iteration: 3445; Percent done: 86.1%;                    Mean loss: 2.9399\n",
      "Iteration: 3446; Percent done: 86.2%;                    Mean loss: 2.7810\n",
      "Iteration: 3447; Percent done: 86.2%;                    Mean loss: 3.0940\n",
      "Iteration: 3448; Percent done: 86.2%;                    Mean loss: 2.9595\n",
      "Iteration: 3449; Percent done: 86.2%;                    Mean loss: 2.8427\n",
      "Iteration: 3450; Percent done: 86.2%;                    Mean loss: 2.8721\n",
      "Iteration: 3451; Percent done: 86.3%;                    Mean loss: 2.7666\n",
      "Iteration: 3452; Percent done: 86.3%;                    Mean loss: 2.8968\n",
      "Iteration: 3453; Percent done: 86.3%;                    Mean loss: 2.8781\n",
      "Iteration: 3454; Percent done: 86.4%;                    Mean loss: 3.0371\n",
      "Iteration: 3455; Percent done: 86.4%;                    Mean loss: 3.0414\n",
      "Iteration: 3456; Percent done: 86.4%;                    Mean loss: 2.8895\n",
      "Iteration: 3457; Percent done: 86.4%;                    Mean loss: 2.8618\n",
      "Iteration: 3458; Percent done: 86.5%;                    Mean loss: 2.8562\n",
      "Iteration: 3459; Percent done: 86.5%;                    Mean loss: 2.9671\n",
      "Iteration: 3460; Percent done: 86.5%;                    Mean loss: 3.1729\n",
      "Iteration: 3461; Percent done: 86.5%;                    Mean loss: 3.0381\n",
      "Iteration: 3462; Percent done: 86.6%;                    Mean loss: 2.8692\n",
      "Iteration: 3463; Percent done: 86.6%;                    Mean loss: 2.7709\n",
      "Iteration: 3464; Percent done: 86.6%;                    Mean loss: 3.0076\n",
      "Iteration: 3465; Percent done: 86.6%;                    Mean loss: 3.1006\n",
      "Iteration: 3466; Percent done: 86.7%;                    Mean loss: 2.9210\n",
      "Iteration: 3467; Percent done: 86.7%;                    Mean loss: 2.7133\n",
      "Iteration: 3468; Percent done: 86.7%;                    Mean loss: 2.8828\n",
      "Iteration: 3469; Percent done: 86.7%;                    Mean loss: 2.9348\n",
      "Iteration: 3470; Percent done: 86.8%;                    Mean loss: 2.9076\n",
      "Iteration: 3471; Percent done: 86.8%;                    Mean loss: 2.6785\n",
      "Iteration: 3472; Percent done: 86.8%;                    Mean loss: 2.8001\n",
      "Iteration: 3473; Percent done: 86.8%;                    Mean loss: 2.8682\n",
      "Iteration: 3474; Percent done: 86.9%;                    Mean loss: 2.9158\n",
      "Iteration: 3475; Percent done: 86.9%;                    Mean loss: 3.0726\n",
      "Iteration: 3476; Percent done: 86.9%;                    Mean loss: 2.8708\n",
      "Iteration: 3477; Percent done: 86.9%;                    Mean loss: 3.0730\n",
      "Iteration: 3478; Percent done: 87.0%;                    Mean loss: 3.2517\n",
      "Iteration: 3479; Percent done: 87.0%;                    Mean loss: 2.9756\n",
      "Iteration: 3480; Percent done: 87.0%;                    Mean loss: 2.9802\n",
      "Iteration: 3481; Percent done: 87.0%;                    Mean loss: 3.0571\n",
      "Iteration: 3482; Percent done: 87.1%;                    Mean loss: 2.6630\n",
      "Iteration: 3483; Percent done: 87.1%;                    Mean loss: 2.7824\n",
      "Iteration: 3484; Percent done: 87.1%;                    Mean loss: 2.9501\n",
      "Iteration: 3485; Percent done: 87.1%;                    Mean loss: 2.8042\n",
      "Iteration: 3486; Percent done: 87.2%;                    Mean loss: 2.8873\n",
      "Iteration: 3487; Percent done: 87.2%;                    Mean loss: 2.9078\n",
      "Iteration: 3488; Percent done: 87.2%;                    Mean loss: 2.8537\n",
      "Iteration: 3489; Percent done: 87.2%;                    Mean loss: 2.7941\n",
      "Iteration: 3490; Percent done: 87.2%;                    Mean loss: 2.8943\n",
      "Iteration: 3491; Percent done: 87.3%;                    Mean loss: 2.7810\n",
      "Iteration: 3492; Percent done: 87.3%;                    Mean loss: 2.8850\n",
      "Iteration: 3493; Percent done: 87.3%;                    Mean loss: 2.9878\n",
      "Iteration: 3494; Percent done: 87.4%;                    Mean loss: 2.7595\n",
      "Iteration: 3495; Percent done: 87.4%;                    Mean loss: 2.9407\n",
      "Iteration: 3496; Percent done: 87.4%;                    Mean loss: 2.9265\n",
      "Iteration: 3497; Percent done: 87.4%;                    Mean loss: 2.7398\n",
      "Iteration: 3498; Percent done: 87.5%;                    Mean loss: 2.9191\n",
      "Iteration: 3499; Percent done: 87.5%;                    Mean loss: 2.5897\n",
      "Iteration: 3500; Percent done: 87.5%;                    Mean loss: 2.6360\n",
      "Iteration: 3501; Percent done: 87.5%;                    Mean loss: 2.7068\n",
      "Iteration: 3502; Percent done: 87.5%;                    Mean loss: 3.0009\n",
      "Iteration: 3503; Percent done: 87.6%;                    Mean loss: 2.6743\n",
      "Iteration: 3504; Percent done: 87.6%;                    Mean loss: 2.8837\n",
      "Iteration: 3505; Percent done: 87.6%;                    Mean loss: 2.8305\n",
      "Iteration: 3506; Percent done: 87.6%;                    Mean loss: 2.9588\n",
      "Iteration: 3507; Percent done: 87.7%;                    Mean loss: 3.1524\n",
      "Iteration: 3508; Percent done: 87.7%;                    Mean loss: 3.0570\n",
      "Iteration: 3509; Percent done: 87.7%;                    Mean loss: 2.6207\n",
      "Iteration: 3510; Percent done: 87.8%;                    Mean loss: 2.9292\n",
      "Iteration: 3511; Percent done: 87.8%;                    Mean loss: 3.0493\n",
      "Iteration: 3512; Percent done: 87.8%;                    Mean loss: 2.9043\n",
      "Iteration: 3513; Percent done: 87.8%;                    Mean loss: 2.8693\n",
      "Iteration: 3514; Percent done: 87.8%;                    Mean loss: 2.7452\n",
      "Iteration: 3515; Percent done: 87.9%;                    Mean loss: 3.1842\n",
      "Iteration: 3516; Percent done: 87.9%;                    Mean loss: 2.5839\n",
      "Iteration: 3517; Percent done: 87.9%;                    Mean loss: 2.8515\n",
      "Iteration: 3518; Percent done: 87.9%;                    Mean loss: 2.7448\n",
      "Iteration: 3519; Percent done: 88.0%;                    Mean loss: 3.1610\n",
      "Iteration: 3520; Percent done: 88.0%;                    Mean loss: 2.9726\n",
      "Iteration: 3521; Percent done: 88.0%;                    Mean loss: 2.8826\n",
      "Iteration: 3522; Percent done: 88.0%;                    Mean loss: 2.9073\n",
      "Iteration: 3523; Percent done: 88.1%;                    Mean loss: 2.8715\n",
      "Iteration: 3524; Percent done: 88.1%;                    Mean loss: 2.9862\n",
      "Iteration: 3525; Percent done: 88.1%;                    Mean loss: 2.8527\n",
      "Iteration: 3526; Percent done: 88.1%;                    Mean loss: 2.7084\n",
      "Iteration: 3527; Percent done: 88.2%;                    Mean loss: 2.9499\n",
      "Iteration: 3528; Percent done: 88.2%;                    Mean loss: 2.9235\n",
      "Iteration: 3529; Percent done: 88.2%;                    Mean loss: 2.7667\n",
      "Iteration: 3530; Percent done: 88.2%;                    Mean loss: 2.7068\n",
      "Iteration: 3531; Percent done: 88.3%;                    Mean loss: 3.1330\n",
      "Iteration: 3532; Percent done: 88.3%;                    Mean loss: 2.8617\n",
      "Iteration: 3533; Percent done: 88.3%;                    Mean loss: 2.7907\n",
      "Iteration: 3534; Percent done: 88.3%;                    Mean loss: 2.9595\n",
      "Iteration: 3535; Percent done: 88.4%;                    Mean loss: 3.0649\n",
      "Iteration: 3536; Percent done: 88.4%;                    Mean loss: 2.9660\n",
      "Iteration: 3537; Percent done: 88.4%;                    Mean loss: 2.9561\n",
      "Iteration: 3538; Percent done: 88.4%;                    Mean loss: 2.6892\n",
      "Iteration: 3539; Percent done: 88.5%;                    Mean loss: 2.8901\n",
      "Iteration: 3540; Percent done: 88.5%;                    Mean loss: 2.7751\n",
      "Iteration: 3541; Percent done: 88.5%;                    Mean loss: 2.9018\n",
      "Iteration: 3542; Percent done: 88.5%;                    Mean loss: 2.8049\n",
      "Iteration: 3543; Percent done: 88.6%;                    Mean loss: 2.9417\n",
      "Iteration: 3544; Percent done: 88.6%;                    Mean loss: 2.9346\n",
      "Iteration: 3545; Percent done: 88.6%;                    Mean loss: 2.9507\n",
      "Iteration: 3546; Percent done: 88.6%;                    Mean loss: 3.0482\n",
      "Iteration: 3547; Percent done: 88.7%;                    Mean loss: 2.7518\n",
      "Iteration: 3548; Percent done: 88.7%;                    Mean loss: 2.7080\n",
      "Iteration: 3549; Percent done: 88.7%;                    Mean loss: 2.9317\n",
      "Iteration: 3550; Percent done: 88.8%;                    Mean loss: 2.8035\n",
      "Iteration: 3551; Percent done: 88.8%;                    Mean loss: 2.9193\n",
      "Iteration: 3552; Percent done: 88.8%;                    Mean loss: 2.9203\n",
      "Iteration: 3553; Percent done: 88.8%;                    Mean loss: 2.9514\n",
      "Iteration: 3554; Percent done: 88.8%;                    Mean loss: 2.6246\n",
      "Iteration: 3555; Percent done: 88.9%;                    Mean loss: 2.9398\n",
      "Iteration: 3556; Percent done: 88.9%;                    Mean loss: 2.8859\n",
      "Iteration: 3557; Percent done: 88.9%;                    Mean loss: 3.0811\n",
      "Iteration: 3558; Percent done: 88.9%;                    Mean loss: 2.7525\n",
      "Iteration: 3559; Percent done: 89.0%;                    Mean loss: 2.9532\n",
      "Iteration: 3560; Percent done: 89.0%;                    Mean loss: 2.8472\n",
      "Iteration: 3561; Percent done: 89.0%;                    Mean loss: 2.8519\n",
      "Iteration: 3562; Percent done: 89.0%;                    Mean loss: 3.0288\n",
      "Iteration: 3563; Percent done: 89.1%;                    Mean loss: 3.1455\n",
      "Iteration: 3564; Percent done: 89.1%;                    Mean loss: 2.9518\n",
      "Iteration: 3565; Percent done: 89.1%;                    Mean loss: 2.9069\n",
      "Iteration: 3566; Percent done: 89.1%;                    Mean loss: 2.6453\n",
      "Iteration: 3567; Percent done: 89.2%;                    Mean loss: 2.8725\n",
      "Iteration: 3568; Percent done: 89.2%;                    Mean loss: 2.9721\n",
      "Iteration: 3569; Percent done: 89.2%;                    Mean loss: 3.0689\n",
      "Iteration: 3570; Percent done: 89.2%;                    Mean loss: 2.8704\n",
      "Iteration: 3571; Percent done: 89.3%;                    Mean loss: 2.6661\n",
      "Iteration: 3572; Percent done: 89.3%;                    Mean loss: 2.8979\n",
      "Iteration: 3573; Percent done: 89.3%;                    Mean loss: 2.8108\n",
      "Iteration: 3574; Percent done: 89.3%;                    Mean loss: 2.8247\n",
      "Iteration: 3575; Percent done: 89.4%;                    Mean loss: 2.7891\n",
      "Iteration: 3576; Percent done: 89.4%;                    Mean loss: 2.9685\n",
      "Iteration: 3577; Percent done: 89.4%;                    Mean loss: 3.0545\n",
      "Iteration: 3578; Percent done: 89.5%;                    Mean loss: 2.7767\n",
      "Iteration: 3579; Percent done: 89.5%;                    Mean loss: 2.9590\n",
      "Iteration: 3580; Percent done: 89.5%;                    Mean loss: 2.6718\n",
      "Iteration: 3581; Percent done: 89.5%;                    Mean loss: 2.7260\n",
      "Iteration: 3582; Percent done: 89.5%;                    Mean loss: 3.1362\n",
      "Iteration: 3583; Percent done: 89.6%;                    Mean loss: 3.0565\n",
      "Iteration: 3584; Percent done: 89.6%;                    Mean loss: 3.0387\n",
      "Iteration: 3585; Percent done: 89.6%;                    Mean loss: 2.8632\n",
      "Iteration: 3586; Percent done: 89.6%;                    Mean loss: 2.9662\n",
      "Iteration: 3587; Percent done: 89.7%;                    Mean loss: 2.9550\n",
      "Iteration: 3588; Percent done: 89.7%;                    Mean loss: 2.8116\n",
      "Iteration: 3589; Percent done: 89.7%;                    Mean loss: 2.8447\n",
      "Iteration: 3590; Percent done: 89.8%;                    Mean loss: 2.9649\n",
      "Iteration: 3591; Percent done: 89.8%;                    Mean loss: 3.0557\n",
      "Iteration: 3592; Percent done: 89.8%;                    Mean loss: 2.9380\n",
      "Iteration: 3593; Percent done: 89.8%;                    Mean loss: 3.0422\n",
      "Iteration: 3594; Percent done: 89.8%;                    Mean loss: 3.0702\n",
      "Iteration: 3595; Percent done: 89.9%;                    Mean loss: 2.9243\n",
      "Iteration: 3596; Percent done: 89.9%;                    Mean loss: 2.8292\n",
      "Iteration: 3597; Percent done: 89.9%;                    Mean loss: 2.8830\n",
      "Iteration: 3598; Percent done: 90.0%;                    Mean loss: 2.9170\n",
      "Iteration: 3599; Percent done: 90.0%;                    Mean loss: 2.7734\n",
      "Iteration: 3600; Percent done: 90.0%;                    Mean loss: 2.8559\n",
      "Iteration: 3601; Percent done: 90.0%;                    Mean loss: 2.7648\n",
      "Iteration: 3602; Percent done: 90.0%;                    Mean loss: 2.8016\n",
      "Iteration: 3603; Percent done: 90.1%;                    Mean loss: 2.9713\n",
      "Iteration: 3604; Percent done: 90.1%;                    Mean loss: 2.8609\n",
      "Iteration: 3605; Percent done: 90.1%;                    Mean loss: 2.8786\n",
      "Iteration: 3606; Percent done: 90.1%;                    Mean loss: 2.8515\n",
      "Iteration: 3607; Percent done: 90.2%;                    Mean loss: 3.0199\n",
      "Iteration: 3608; Percent done: 90.2%;                    Mean loss: 2.8920\n",
      "Iteration: 3609; Percent done: 90.2%;                    Mean loss: 2.7321\n",
      "Iteration: 3610; Percent done: 90.2%;                    Mean loss: 2.9198\n",
      "Iteration: 3611; Percent done: 90.3%;                    Mean loss: 2.9155\n",
      "Iteration: 3612; Percent done: 90.3%;                    Mean loss: 2.7865\n",
      "Iteration: 3613; Percent done: 90.3%;                    Mean loss: 2.9494\n",
      "Iteration: 3614; Percent done: 90.3%;                    Mean loss: 2.9297\n",
      "Iteration: 3615; Percent done: 90.4%;                    Mean loss: 3.1732\n",
      "Iteration: 3616; Percent done: 90.4%;                    Mean loss: 2.8868\n",
      "Iteration: 3617; Percent done: 90.4%;                    Mean loss: 2.9064\n",
      "Iteration: 3618; Percent done: 90.5%;                    Mean loss: 2.8100\n",
      "Iteration: 3619; Percent done: 90.5%;                    Mean loss: 2.7815\n",
      "Iteration: 3620; Percent done: 90.5%;                    Mean loss: 2.9816\n",
      "Iteration: 3621; Percent done: 90.5%;                    Mean loss: 2.8296\n",
      "Iteration: 3622; Percent done: 90.5%;                    Mean loss: 2.8102\n",
      "Iteration: 3623; Percent done: 90.6%;                    Mean loss: 2.8708\n",
      "Iteration: 3624; Percent done: 90.6%;                    Mean loss: 2.7515\n",
      "Iteration: 3625; Percent done: 90.6%;                    Mean loss: 2.9655\n",
      "Iteration: 3626; Percent done: 90.6%;                    Mean loss: 2.7925\n",
      "Iteration: 3627; Percent done: 90.7%;                    Mean loss: 2.9287\n",
      "Iteration: 3628; Percent done: 90.7%;                    Mean loss: 3.1762\n",
      "Iteration: 3629; Percent done: 90.7%;                    Mean loss: 2.9418\n",
      "Iteration: 3630; Percent done: 90.8%;                    Mean loss: 2.9505\n",
      "Iteration: 3631; Percent done: 90.8%;                    Mean loss: 2.7000\n",
      "Iteration: 3632; Percent done: 90.8%;                    Mean loss: 2.9296\n",
      "Iteration: 3633; Percent done: 90.8%;                    Mean loss: 2.8857\n",
      "Iteration: 3634; Percent done: 90.8%;                    Mean loss: 2.7881\n",
      "Iteration: 3635; Percent done: 90.9%;                    Mean loss: 2.6161\n",
      "Iteration: 3636; Percent done: 90.9%;                    Mean loss: 2.9383\n",
      "Iteration: 3637; Percent done: 90.9%;                    Mean loss: 3.0459\n",
      "Iteration: 3638; Percent done: 91.0%;                    Mean loss: 2.8611\n",
      "Iteration: 3639; Percent done: 91.0%;                    Mean loss: 2.9044\n",
      "Iteration: 3640; Percent done: 91.0%;                    Mean loss: 2.7409\n",
      "Iteration: 3641; Percent done: 91.0%;                    Mean loss: 2.7397\n",
      "Iteration: 3642; Percent done: 91.0%;                    Mean loss: 3.0293\n",
      "Iteration: 3643; Percent done: 91.1%;                    Mean loss: 2.8413\n",
      "Iteration: 3644; Percent done: 91.1%;                    Mean loss: 2.9524\n",
      "Iteration: 3645; Percent done: 91.1%;                    Mean loss: 2.9365\n",
      "Iteration: 3646; Percent done: 91.1%;                    Mean loss: 2.8602\n",
      "Iteration: 3647; Percent done: 91.2%;                    Mean loss: 2.9829\n",
      "Iteration: 3648; Percent done: 91.2%;                    Mean loss: 2.8127\n",
      "Iteration: 3649; Percent done: 91.2%;                    Mean loss: 2.9399\n",
      "Iteration: 3650; Percent done: 91.2%;                    Mean loss: 2.8214\n",
      "Iteration: 3651; Percent done: 91.3%;                    Mean loss: 2.8468\n",
      "Iteration: 3652; Percent done: 91.3%;                    Mean loss: 2.9564\n",
      "Iteration: 3653; Percent done: 91.3%;                    Mean loss: 2.7005\n",
      "Iteration: 3654; Percent done: 91.3%;                    Mean loss: 2.7189\n",
      "Iteration: 3655; Percent done: 91.4%;                    Mean loss: 2.9216\n",
      "Iteration: 3656; Percent done: 91.4%;                    Mean loss: 2.7949\n",
      "Iteration: 3657; Percent done: 91.4%;                    Mean loss: 2.7857\n",
      "Iteration: 3658; Percent done: 91.5%;                    Mean loss: 2.8550\n",
      "Iteration: 3659; Percent done: 91.5%;                    Mean loss: 2.9437\n",
      "Iteration: 3660; Percent done: 91.5%;                    Mean loss: 2.5844\n",
      "Iteration: 3661; Percent done: 91.5%;                    Mean loss: 2.8312\n",
      "Iteration: 3662; Percent done: 91.5%;                    Mean loss: 2.9174\n",
      "Iteration: 3663; Percent done: 91.6%;                    Mean loss: 2.8372\n",
      "Iteration: 3664; Percent done: 91.6%;                    Mean loss: 2.7659\n",
      "Iteration: 3665; Percent done: 91.6%;                    Mean loss: 2.8709\n",
      "Iteration: 3666; Percent done: 91.6%;                    Mean loss: 2.6641\n",
      "Iteration: 3667; Percent done: 91.7%;                    Mean loss: 3.1342\n",
      "Iteration: 3668; Percent done: 91.7%;                    Mean loss: 2.8794\n",
      "Iteration: 3669; Percent done: 91.7%;                    Mean loss: 2.7926\n",
      "Iteration: 3670; Percent done: 91.8%;                    Mean loss: 2.6237\n",
      "Iteration: 3671; Percent done: 91.8%;                    Mean loss: 2.8327\n",
      "Iteration: 3672; Percent done: 91.8%;                    Mean loss: 2.8591\n",
      "Iteration: 3673; Percent done: 91.8%;                    Mean loss: 2.8365\n",
      "Iteration: 3674; Percent done: 91.8%;                    Mean loss: 2.7864\n",
      "Iteration: 3675; Percent done: 91.9%;                    Mean loss: 2.9978\n",
      "Iteration: 3676; Percent done: 91.9%;                    Mean loss: 2.6181\n",
      "Iteration: 3677; Percent done: 91.9%;                    Mean loss: 2.7404\n",
      "Iteration: 3678; Percent done: 92.0%;                    Mean loss: 2.9717\n",
      "Iteration: 3679; Percent done: 92.0%;                    Mean loss: 2.7413\n",
      "Iteration: 3680; Percent done: 92.0%;                    Mean loss: 2.8362\n",
      "Iteration: 3681; Percent done: 92.0%;                    Mean loss: 2.6157\n",
      "Iteration: 3682; Percent done: 92.0%;                    Mean loss: 2.7772\n",
      "Iteration: 3683; Percent done: 92.1%;                    Mean loss: 3.0204\n",
      "Iteration: 3684; Percent done: 92.1%;                    Mean loss: 2.7339\n",
      "Iteration: 3685; Percent done: 92.1%;                    Mean loss: 2.9444\n",
      "Iteration: 3686; Percent done: 92.2%;                    Mean loss: 2.9328\n",
      "Iteration: 3687; Percent done: 92.2%;                    Mean loss: 2.8737\n",
      "Iteration: 3688; Percent done: 92.2%;                    Mean loss: 2.7171\n",
      "Iteration: 3689; Percent done: 92.2%;                    Mean loss: 2.8481\n",
      "Iteration: 3690; Percent done: 92.2%;                    Mean loss: 2.8409\n",
      "Iteration: 3691; Percent done: 92.3%;                    Mean loss: 2.8609\n",
      "Iteration: 3692; Percent done: 92.3%;                    Mean loss: 2.9062\n",
      "Iteration: 3693; Percent done: 92.3%;                    Mean loss: 2.9142\n",
      "Iteration: 3694; Percent done: 92.3%;                    Mean loss: 2.9411\n",
      "Iteration: 3695; Percent done: 92.4%;                    Mean loss: 2.8828\n",
      "Iteration: 3696; Percent done: 92.4%;                    Mean loss: 2.7381\n",
      "Iteration: 3697; Percent done: 92.4%;                    Mean loss: 2.9221\n",
      "Iteration: 3698; Percent done: 92.5%;                    Mean loss: 2.8376\n",
      "Iteration: 3699; Percent done: 92.5%;                    Mean loss: 2.8388\n",
      "Iteration: 3700; Percent done: 92.5%;                    Mean loss: 2.8428\n",
      "Iteration: 3701; Percent done: 92.5%;                    Mean loss: 2.7177\n",
      "Iteration: 3702; Percent done: 92.5%;                    Mean loss: 2.9267\n",
      "Iteration: 3703; Percent done: 92.6%;                    Mean loss: 2.9950\n",
      "Iteration: 3704; Percent done: 92.6%;                    Mean loss: 2.8166\n",
      "Iteration: 3705; Percent done: 92.6%;                    Mean loss: 2.9201\n",
      "Iteration: 3706; Percent done: 92.7%;                    Mean loss: 2.8704\n",
      "Iteration: 3707; Percent done: 92.7%;                    Mean loss: 3.0667\n",
      "Iteration: 3708; Percent done: 92.7%;                    Mean loss: 2.8146\n",
      "Iteration: 3709; Percent done: 92.7%;                    Mean loss: 2.9164\n",
      "Iteration: 3710; Percent done: 92.8%;                    Mean loss: 2.6230\n",
      "Iteration: 3711; Percent done: 92.8%;                    Mean loss: 2.8976\n",
      "Iteration: 3712; Percent done: 92.8%;                    Mean loss: 2.8591\n",
      "Iteration: 3713; Percent done: 92.8%;                    Mean loss: 2.5872\n",
      "Iteration: 3714; Percent done: 92.8%;                    Mean loss: 2.9220\n",
      "Iteration: 3715; Percent done: 92.9%;                    Mean loss: 2.9575\n",
      "Iteration: 3716; Percent done: 92.9%;                    Mean loss: 2.6630\n",
      "Iteration: 3717; Percent done: 92.9%;                    Mean loss: 2.5956\n",
      "Iteration: 3718; Percent done: 93.0%;                    Mean loss: 2.6701\n",
      "Iteration: 3719; Percent done: 93.0%;                    Mean loss: 2.7171\n",
      "Iteration: 3720; Percent done: 93.0%;                    Mean loss: 2.7133\n",
      "Iteration: 3721; Percent done: 93.0%;                    Mean loss: 3.0302\n",
      "Iteration: 3722; Percent done: 93.0%;                    Mean loss: 2.7380\n",
      "Iteration: 3723; Percent done: 93.1%;                    Mean loss: 2.9454\n",
      "Iteration: 3724; Percent done: 93.1%;                    Mean loss: 2.6135\n",
      "Iteration: 3725; Percent done: 93.1%;                    Mean loss: 2.6772\n",
      "Iteration: 3726; Percent done: 93.2%;                    Mean loss: 2.7705\n",
      "Iteration: 3727; Percent done: 93.2%;                    Mean loss: 3.0415\n",
      "Iteration: 3728; Percent done: 93.2%;                    Mean loss: 2.7081\n",
      "Iteration: 3729; Percent done: 93.2%;                    Mean loss: 2.7427\n",
      "Iteration: 3730; Percent done: 93.2%;                    Mean loss: 2.8123\n",
      "Iteration: 3731; Percent done: 93.3%;                    Mean loss: 2.7098\n",
      "Iteration: 3732; Percent done: 93.3%;                    Mean loss: 2.7885\n",
      "Iteration: 3733; Percent done: 93.3%;                    Mean loss: 2.5544\n",
      "Iteration: 3734; Percent done: 93.3%;                    Mean loss: 2.7795\n",
      "Iteration: 3735; Percent done: 93.4%;                    Mean loss: 2.8480\n",
      "Iteration: 3736; Percent done: 93.4%;                    Mean loss: 2.9084\n",
      "Iteration: 3737; Percent done: 93.4%;                    Mean loss: 3.1302\n",
      "Iteration: 3738; Percent done: 93.5%;                    Mean loss: 2.9370\n",
      "Iteration: 3739; Percent done: 93.5%;                    Mean loss: 2.8658\n",
      "Iteration: 3740; Percent done: 93.5%;                    Mean loss: 2.6962\n",
      "Iteration: 3741; Percent done: 93.5%;                    Mean loss: 2.9163\n",
      "Iteration: 3742; Percent done: 93.5%;                    Mean loss: 2.5862\n",
      "Iteration: 3743; Percent done: 93.6%;                    Mean loss: 2.8725\n",
      "Iteration: 3744; Percent done: 93.6%;                    Mean loss: 2.8420\n",
      "Iteration: 3745; Percent done: 93.6%;                    Mean loss: 2.7861\n",
      "Iteration: 3746; Percent done: 93.7%;                    Mean loss: 2.9507\n",
      "Iteration: 3747; Percent done: 93.7%;                    Mean loss: 2.8599\n",
      "Iteration: 3748; Percent done: 93.7%;                    Mean loss: 2.7159\n",
      "Iteration: 3749; Percent done: 93.7%;                    Mean loss: 2.8220\n",
      "Iteration: 3750; Percent done: 93.8%;                    Mean loss: 2.8593\n",
      "Iteration: 3751; Percent done: 93.8%;                    Mean loss: 2.8056\n",
      "Iteration: 3752; Percent done: 93.8%;                    Mean loss: 2.8904\n",
      "Iteration: 3753; Percent done: 93.8%;                    Mean loss: 2.7036\n",
      "Iteration: 3754; Percent done: 93.8%;                    Mean loss: 2.8001\n",
      "Iteration: 3755; Percent done: 93.9%;                    Mean loss: 2.6767\n",
      "Iteration: 3756; Percent done: 93.9%;                    Mean loss: 2.6025\n",
      "Iteration: 3757; Percent done: 93.9%;                    Mean loss: 3.1750\n",
      "Iteration: 3758; Percent done: 94.0%;                    Mean loss: 3.0368\n",
      "Iteration: 3759; Percent done: 94.0%;                    Mean loss: 2.7523\n",
      "Iteration: 3760; Percent done: 94.0%;                    Mean loss: 3.0266\n",
      "Iteration: 3761; Percent done: 94.0%;                    Mean loss: 2.5276\n",
      "Iteration: 3762; Percent done: 94.0%;                    Mean loss: 3.0853\n",
      "Iteration: 3763; Percent done: 94.1%;                    Mean loss: 2.8531\n",
      "Iteration: 3764; Percent done: 94.1%;                    Mean loss: 2.6642\n",
      "Iteration: 3765; Percent done: 94.1%;                    Mean loss: 2.6829\n",
      "Iteration: 3766; Percent done: 94.2%;                    Mean loss: 2.6986\n",
      "Iteration: 3767; Percent done: 94.2%;                    Mean loss: 2.6930\n",
      "Iteration: 3768; Percent done: 94.2%;                    Mean loss: 2.8261\n",
      "Iteration: 3769; Percent done: 94.2%;                    Mean loss: 2.8901\n",
      "Iteration: 3770; Percent done: 94.2%;                    Mean loss: 2.8438\n",
      "Iteration: 3771; Percent done: 94.3%;                    Mean loss: 2.7934\n",
      "Iteration: 3772; Percent done: 94.3%;                    Mean loss: 2.7399\n",
      "Iteration: 3773; Percent done: 94.3%;                    Mean loss: 2.7992\n",
      "Iteration: 3774; Percent done: 94.3%;                    Mean loss: 2.8175\n",
      "Iteration: 3775; Percent done: 94.4%;                    Mean loss: 2.8902\n",
      "Iteration: 3776; Percent done: 94.4%;                    Mean loss: 2.6860\n",
      "Iteration: 3777; Percent done: 94.4%;                    Mean loss: 2.7484\n",
      "Iteration: 3778; Percent done: 94.5%;                    Mean loss: 2.8170\n",
      "Iteration: 3779; Percent done: 94.5%;                    Mean loss: 2.9700\n",
      "Iteration: 3780; Percent done: 94.5%;                    Mean loss: 2.7121\n",
      "Iteration: 3781; Percent done: 94.5%;                    Mean loss: 2.7599\n",
      "Iteration: 3782; Percent done: 94.5%;                    Mean loss: 2.6684\n",
      "Iteration: 3783; Percent done: 94.6%;                    Mean loss: 2.9106\n",
      "Iteration: 3784; Percent done: 94.6%;                    Mean loss: 2.6293\n",
      "Iteration: 3785; Percent done: 94.6%;                    Mean loss: 2.8503\n",
      "Iteration: 3786; Percent done: 94.7%;                    Mean loss: 2.9771\n",
      "Iteration: 3787; Percent done: 94.7%;                    Mean loss: 2.6812\n",
      "Iteration: 3788; Percent done: 94.7%;                    Mean loss: 2.6014\n",
      "Iteration: 3789; Percent done: 94.7%;                    Mean loss: 2.8373\n",
      "Iteration: 3790; Percent done: 94.8%;                    Mean loss: 2.8535\n",
      "Iteration: 3791; Percent done: 94.8%;                    Mean loss: 2.9359\n",
      "Iteration: 3792; Percent done: 94.8%;                    Mean loss: 2.9374\n",
      "Iteration: 3793; Percent done: 94.8%;                    Mean loss: 3.0222\n",
      "Iteration: 3794; Percent done: 94.8%;                    Mean loss: 3.0149\n",
      "Iteration: 3795; Percent done: 94.9%;                    Mean loss: 2.8006\n",
      "Iteration: 3796; Percent done: 94.9%;                    Mean loss: 2.5484\n",
      "Iteration: 3797; Percent done: 94.9%;                    Mean loss: 2.7663\n",
      "Iteration: 3798; Percent done: 95.0%;                    Mean loss: 2.8966\n",
      "Iteration: 3799; Percent done: 95.0%;                    Mean loss: 2.8055\n",
      "Iteration: 3800; Percent done: 95.0%;                    Mean loss: 2.6245\n",
      "Iteration: 3801; Percent done: 95.0%;                    Mean loss: 2.9142\n",
      "Iteration: 3802; Percent done: 95.0%;                    Mean loss: 2.9788\n",
      "Iteration: 3803; Percent done: 95.1%;                    Mean loss: 2.6921\n",
      "Iteration: 3804; Percent done: 95.1%;                    Mean loss: 2.6741\n",
      "Iteration: 3805; Percent done: 95.1%;                    Mean loss: 2.9069\n",
      "Iteration: 3806; Percent done: 95.2%;                    Mean loss: 3.1430\n",
      "Iteration: 3807; Percent done: 95.2%;                    Mean loss: 2.5976\n",
      "Iteration: 3808; Percent done: 95.2%;                    Mean loss: 3.1831\n",
      "Iteration: 3809; Percent done: 95.2%;                    Mean loss: 2.9647\n",
      "Iteration: 3810; Percent done: 95.2%;                    Mean loss: 2.7667\n",
      "Iteration: 3811; Percent done: 95.3%;                    Mean loss: 2.8848\n",
      "Iteration: 3812; Percent done: 95.3%;                    Mean loss: 2.9706\n",
      "Iteration: 3813; Percent done: 95.3%;                    Mean loss: 2.7263\n",
      "Iteration: 3814; Percent done: 95.3%;                    Mean loss: 2.8600\n",
      "Iteration: 3815; Percent done: 95.4%;                    Mean loss: 2.8675\n",
      "Iteration: 3816; Percent done: 95.4%;                    Mean loss: 2.7447\n",
      "Iteration: 3817; Percent done: 95.4%;                    Mean loss: 2.9151\n",
      "Iteration: 3818; Percent done: 95.5%;                    Mean loss: 2.7445\n",
      "Iteration: 3819; Percent done: 95.5%;                    Mean loss: 2.8376\n",
      "Iteration: 3820; Percent done: 95.5%;                    Mean loss: 2.8504\n",
      "Iteration: 3821; Percent done: 95.5%;                    Mean loss: 2.8010\n",
      "Iteration: 3822; Percent done: 95.5%;                    Mean loss: 2.9194\n",
      "Iteration: 3823; Percent done: 95.6%;                    Mean loss: 2.7895\n",
      "Iteration: 3824; Percent done: 95.6%;                    Mean loss: 2.7950\n",
      "Iteration: 3825; Percent done: 95.6%;                    Mean loss: 2.9148\n",
      "Iteration: 3826; Percent done: 95.7%;                    Mean loss: 2.8085\n",
      "Iteration: 3827; Percent done: 95.7%;                    Mean loss: 2.7005\n",
      "Iteration: 3828; Percent done: 95.7%;                    Mean loss: 2.7457\n",
      "Iteration: 3829; Percent done: 95.7%;                    Mean loss: 2.8147\n",
      "Iteration: 3830; Percent done: 95.8%;                    Mean loss: 2.7903\n",
      "Iteration: 3831; Percent done: 95.8%;                    Mean loss: 2.8037\n",
      "Iteration: 3832; Percent done: 95.8%;                    Mean loss: 2.9029\n",
      "Iteration: 3833; Percent done: 95.8%;                    Mean loss: 2.7731\n",
      "Iteration: 3834; Percent done: 95.9%;                    Mean loss: 2.6972\n",
      "Iteration: 3835; Percent done: 95.9%;                    Mean loss: 2.5479\n",
      "Iteration: 3836; Percent done: 95.9%;                    Mean loss: 2.9105\n",
      "Iteration: 3837; Percent done: 95.9%;                    Mean loss: 2.7999\n",
      "Iteration: 3838; Percent done: 96.0%;                    Mean loss: 2.7960\n",
      "Iteration: 3839; Percent done: 96.0%;                    Mean loss: 2.8535\n",
      "Iteration: 3840; Percent done: 96.0%;                    Mean loss: 2.7341\n",
      "Iteration: 3841; Percent done: 96.0%;                    Mean loss: 2.7641\n",
      "Iteration: 3842; Percent done: 96.0%;                    Mean loss: 2.7887\n",
      "Iteration: 3843; Percent done: 96.1%;                    Mean loss: 3.0910\n",
      "Iteration: 3844; Percent done: 96.1%;                    Mean loss: 2.7318\n",
      "Iteration: 3845; Percent done: 96.1%;                    Mean loss: 2.8058\n",
      "Iteration: 3846; Percent done: 96.2%;                    Mean loss: 2.8644\n",
      "Iteration: 3847; Percent done: 96.2%;                    Mean loss: 3.0416\n",
      "Iteration: 3848; Percent done: 96.2%;                    Mean loss: 2.7483\n",
      "Iteration: 3849; Percent done: 96.2%;                    Mean loss: 2.9108\n",
      "Iteration: 3850; Percent done: 96.2%;                    Mean loss: 2.7960\n",
      "Iteration: 3851; Percent done: 96.3%;                    Mean loss: 2.8675\n",
      "Iteration: 3852; Percent done: 96.3%;                    Mean loss: 2.6298\n",
      "Iteration: 3853; Percent done: 96.3%;                    Mean loss: 2.7452\n",
      "Iteration: 3854; Percent done: 96.4%;                    Mean loss: 2.8715\n",
      "Iteration: 3855; Percent done: 96.4%;                    Mean loss: 2.9981\n",
      "Iteration: 3856; Percent done: 96.4%;                    Mean loss: 2.7999\n",
      "Iteration: 3857; Percent done: 96.4%;                    Mean loss: 2.6740\n",
      "Iteration: 3858; Percent done: 96.5%;                    Mean loss: 3.0501\n",
      "Iteration: 3859; Percent done: 96.5%;                    Mean loss: 2.7911\n",
      "Iteration: 3860; Percent done: 96.5%;                    Mean loss: 2.9356\n",
      "Iteration: 3861; Percent done: 96.5%;                    Mean loss: 2.9055\n",
      "Iteration: 3862; Percent done: 96.5%;                    Mean loss: 2.8477\n",
      "Iteration: 3863; Percent done: 96.6%;                    Mean loss: 3.0968\n",
      "Iteration: 3864; Percent done: 96.6%;                    Mean loss: 2.5570\n",
      "Iteration: 3865; Percent done: 96.6%;                    Mean loss: 2.8718\n",
      "Iteration: 3866; Percent done: 96.7%;                    Mean loss: 2.6589\n",
      "Iteration: 3867; Percent done: 96.7%;                    Mean loss: 3.0631\n",
      "Iteration: 3868; Percent done: 96.7%;                    Mean loss: 2.9627\n",
      "Iteration: 3869; Percent done: 96.7%;                    Mean loss: 2.6275\n",
      "Iteration: 3870; Percent done: 96.8%;                    Mean loss: 2.8566\n",
      "Iteration: 3871; Percent done: 96.8%;                    Mean loss: 2.6468\n",
      "Iteration: 3872; Percent done: 96.8%;                    Mean loss: 2.9570\n",
      "Iteration: 3873; Percent done: 96.8%;                    Mean loss: 2.6458\n",
      "Iteration: 3874; Percent done: 96.9%;                    Mean loss: 3.0062\n",
      "Iteration: 3875; Percent done: 96.9%;                    Mean loss: 3.0339\n",
      "Iteration: 3876; Percent done: 96.9%;                    Mean loss: 2.7550\n",
      "Iteration: 3877; Percent done: 96.9%;                    Mean loss: 2.4661\n",
      "Iteration: 3878; Percent done: 97.0%;                    Mean loss: 2.7964\n",
      "Iteration: 3879; Percent done: 97.0%;                    Mean loss: 2.6760\n",
      "Iteration: 3880; Percent done: 97.0%;                    Mean loss: 2.8273\n",
      "Iteration: 3881; Percent done: 97.0%;                    Mean loss: 2.4982\n",
      "Iteration: 3882; Percent done: 97.0%;                    Mean loss: 2.4743\n",
      "Iteration: 3883; Percent done: 97.1%;                    Mean loss: 2.7073\n",
      "Iteration: 3884; Percent done: 97.1%;                    Mean loss: 2.8342\n",
      "Iteration: 3885; Percent done: 97.1%;                    Mean loss: 2.7874\n",
      "Iteration: 3886; Percent done: 97.2%;                    Mean loss: 2.6267\n",
      "Iteration: 3887; Percent done: 97.2%;                    Mean loss: 2.6747\n",
      "Iteration: 3888; Percent done: 97.2%;                    Mean loss: 2.4340\n",
      "Iteration: 3889; Percent done: 97.2%;                    Mean loss: 2.6326\n",
      "Iteration: 3890; Percent done: 97.2%;                    Mean loss: 2.8622\n",
      "Iteration: 3891; Percent done: 97.3%;                    Mean loss: 2.9675\n",
      "Iteration: 3892; Percent done: 97.3%;                    Mean loss: 2.7447\n",
      "Iteration: 3893; Percent done: 97.3%;                    Mean loss: 2.7711\n",
      "Iteration: 3894; Percent done: 97.4%;                    Mean loss: 2.6765\n",
      "Iteration: 3895; Percent done: 97.4%;                    Mean loss: 2.7912\n",
      "Iteration: 3896; Percent done: 97.4%;                    Mean loss: 2.9404\n",
      "Iteration: 3897; Percent done: 97.4%;                    Mean loss: 2.7709\n",
      "Iteration: 3898; Percent done: 97.5%;                    Mean loss: 2.4770\n",
      "Iteration: 3899; Percent done: 97.5%;                    Mean loss: 2.9353\n",
      "Iteration: 3900; Percent done: 97.5%;                    Mean loss: 2.8710\n",
      "Iteration: 3901; Percent done: 97.5%;                    Mean loss: 2.6689\n",
      "Iteration: 3902; Percent done: 97.5%;                    Mean loss: 2.8603\n",
      "Iteration: 3903; Percent done: 97.6%;                    Mean loss: 2.8235\n",
      "Iteration: 3904; Percent done: 97.6%;                    Mean loss: 2.6665\n",
      "Iteration: 3905; Percent done: 97.6%;                    Mean loss: 2.7437\n",
      "Iteration: 3906; Percent done: 97.7%;                    Mean loss: 2.8792\n",
      "Iteration: 3907; Percent done: 97.7%;                    Mean loss: 2.9700\n",
      "Iteration: 3908; Percent done: 97.7%;                    Mean loss: 2.8015\n",
      "Iteration: 3909; Percent done: 97.7%;                    Mean loss: 2.6886\n",
      "Iteration: 3910; Percent done: 97.8%;                    Mean loss: 2.6934\n",
      "Iteration: 3911; Percent done: 97.8%;                    Mean loss: 2.8058\n",
      "Iteration: 3912; Percent done: 97.8%;                    Mean loss: 2.6287\n",
      "Iteration: 3913; Percent done: 97.8%;                    Mean loss: 2.6873\n",
      "Iteration: 3914; Percent done: 97.9%;                    Mean loss: 2.6748\n",
      "Iteration: 3915; Percent done: 97.9%;                    Mean loss: 2.9458\n",
      "Iteration: 3916; Percent done: 97.9%;                    Mean loss: 2.6226\n",
      "Iteration: 3917; Percent done: 97.9%;                    Mean loss: 2.7289\n",
      "Iteration: 3918; Percent done: 98.0%;                    Mean loss: 3.0429\n",
      "Iteration: 3919; Percent done: 98.0%;                    Mean loss: 2.7350\n",
      "Iteration: 3920; Percent done: 98.0%;                    Mean loss: 2.7930\n",
      "Iteration: 3921; Percent done: 98.0%;                    Mean loss: 2.7749\n",
      "Iteration: 3922; Percent done: 98.0%;                    Mean loss: 2.6986\n",
      "Iteration: 3923; Percent done: 98.1%;                    Mean loss: 2.8599\n",
      "Iteration: 3924; Percent done: 98.1%;                    Mean loss: 2.7267\n",
      "Iteration: 3925; Percent done: 98.1%;                    Mean loss: 2.5338\n",
      "Iteration: 3926; Percent done: 98.2%;                    Mean loss: 2.6166\n",
      "Iteration: 3927; Percent done: 98.2%;                    Mean loss: 2.7182\n",
      "Iteration: 3928; Percent done: 98.2%;                    Mean loss: 2.6890\n",
      "Iteration: 3929; Percent done: 98.2%;                    Mean loss: 2.6477\n",
      "Iteration: 3930; Percent done: 98.2%;                    Mean loss: 2.7160\n",
      "Iteration: 3931; Percent done: 98.3%;                    Mean loss: 2.9478\n",
      "Iteration: 3932; Percent done: 98.3%;                    Mean loss: 2.6678\n",
      "Iteration: 3933; Percent done: 98.3%;                    Mean loss: 2.8361\n",
      "Iteration: 3934; Percent done: 98.4%;                    Mean loss: 2.6198\n",
      "Iteration: 3935; Percent done: 98.4%;                    Mean loss: 2.9450\n",
      "Iteration: 3936; Percent done: 98.4%;                    Mean loss: 2.8996\n",
      "Iteration: 3937; Percent done: 98.4%;                    Mean loss: 2.6048\n",
      "Iteration: 3938; Percent done: 98.5%;                    Mean loss: 2.8326\n",
      "Iteration: 3939; Percent done: 98.5%;                    Mean loss: 2.7214\n",
      "Iteration: 3940; Percent done: 98.5%;                    Mean loss: 2.9096\n",
      "Iteration: 3941; Percent done: 98.5%;                    Mean loss: 2.9049\n",
      "Iteration: 3942; Percent done: 98.6%;                    Mean loss: 2.7182\n",
      "Iteration: 3943; Percent done: 98.6%;                    Mean loss: 2.9421\n",
      "Iteration: 3944; Percent done: 98.6%;                    Mean loss: 2.5814\n",
      "Iteration: 3945; Percent done: 98.6%;                    Mean loss: 2.7793\n",
      "Iteration: 3946; Percent done: 98.7%;                    Mean loss: 2.6111\n",
      "Iteration: 3947; Percent done: 98.7%;                    Mean loss: 2.7922\n",
      "Iteration: 3948; Percent done: 98.7%;                    Mean loss: 2.7345\n",
      "Iteration: 3949; Percent done: 98.7%;                    Mean loss: 2.9322\n",
      "Iteration: 3950; Percent done: 98.8%;                    Mean loss: 2.6967\n",
      "Iteration: 3951; Percent done: 98.8%;                    Mean loss: 2.7242\n",
      "Iteration: 3952; Percent done: 98.8%;                    Mean loss: 2.6091\n",
      "Iteration: 3953; Percent done: 98.8%;                    Mean loss: 2.4743\n",
      "Iteration: 3954; Percent done: 98.9%;                    Mean loss: 2.8955\n",
      "Iteration: 3955; Percent done: 98.9%;                    Mean loss: 2.8272\n",
      "Iteration: 3956; Percent done: 98.9%;                    Mean loss: 2.7118\n",
      "Iteration: 3957; Percent done: 98.9%;                    Mean loss: 2.9082\n",
      "Iteration: 3958; Percent done: 99.0%;                    Mean loss: 2.9238\n",
      "Iteration: 3959; Percent done: 99.0%;                    Mean loss: 2.9381\n",
      "Iteration: 3960; Percent done: 99.0%;                    Mean loss: 2.9973\n",
      "Iteration: 3961; Percent done: 99.0%;                    Mean loss: 2.6628\n",
      "Iteration: 3962; Percent done: 99.1%;                    Mean loss: 2.7786\n",
      "Iteration: 3963; Percent done: 99.1%;                    Mean loss: 2.7448\n",
      "Iteration: 3964; Percent done: 99.1%;                    Mean loss: 2.7526\n",
      "Iteration: 3965; Percent done: 99.1%;                    Mean loss: 2.8910\n",
      "Iteration: 3966; Percent done: 99.2%;                    Mean loss: 2.7741\n",
      "Iteration: 3967; Percent done: 99.2%;                    Mean loss: 2.8711\n",
      "Iteration: 3968; Percent done: 99.2%;                    Mean loss: 2.7665\n",
      "Iteration: 3969; Percent done: 99.2%;                    Mean loss: 2.7988\n",
      "Iteration: 3970; Percent done: 99.2%;                    Mean loss: 2.9658\n",
      "Iteration: 3971; Percent done: 99.3%;                    Mean loss: 2.6606\n",
      "Iteration: 3972; Percent done: 99.3%;                    Mean loss: 2.7337\n",
      "Iteration: 3973; Percent done: 99.3%;                    Mean loss: 2.5222\n",
      "Iteration: 3974; Percent done: 99.4%;                    Mean loss: 2.5872\n",
      "Iteration: 3975; Percent done: 99.4%;                    Mean loss: 2.5889\n",
      "Iteration: 3976; Percent done: 99.4%;                    Mean loss: 2.4898\n",
      "Iteration: 3977; Percent done: 99.4%;                    Mean loss: 2.7751\n",
      "Iteration: 3978; Percent done: 99.5%;                    Mean loss: 2.7743\n",
      "Iteration: 3979; Percent done: 99.5%;                    Mean loss: 2.8408\n",
      "Iteration: 3980; Percent done: 99.5%;                    Mean loss: 2.6265\n",
      "Iteration: 3981; Percent done: 99.5%;                    Mean loss: 2.6842\n",
      "Iteration: 3982; Percent done: 99.6%;                    Mean loss: 2.8540\n",
      "Iteration: 3983; Percent done: 99.6%;                    Mean loss: 2.5939\n",
      "Iteration: 3984; Percent done: 99.6%;                    Mean loss: 3.0048\n",
      "Iteration: 3985; Percent done: 99.6%;                    Mean loss: 2.8586\n",
      "Iteration: 3986; Percent done: 99.7%;                    Mean loss: 2.7996\n",
      "Iteration: 3987; Percent done: 99.7%;                    Mean loss: 2.5956\n",
      "Iteration: 3988; Percent done: 99.7%;                    Mean loss: 2.8053\n",
      "Iteration: 3989; Percent done: 99.7%;                    Mean loss: 2.8436\n",
      "Iteration: 3990; Percent done: 99.8%;                    Mean loss: 2.8163\n",
      "Iteration: 3991; Percent done: 99.8%;                    Mean loss: 2.6607\n",
      "Iteration: 3992; Percent done: 99.8%;                    Mean loss: 2.4989\n",
      "Iteration: 3993; Percent done: 99.8%;                    Mean loss: 2.7019\n",
      "Iteration: 3994; Percent done: 99.9%;                    Mean loss: 2.7265\n",
      "Iteration: 3995; Percent done: 99.9%;                    Mean loss: 2.7744\n",
      "Iteration: 3996; Percent done: 99.9%;                    Mean loss: 2.7275\n",
      "Iteration: 3997; Percent done: 99.9%;                    Mean loss: 2.7120\n",
      "Iteration: 3998; Percent done: 100.0%;                    Mean loss: 2.8251\n",
      "Iteration: 3999; Percent done: 100.0%;                    Mean loss: 2.8668\n",
      "Iteration: 4000; Percent done: 100.0%;                    Mean loss: 3.0472\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "# 1\n",
    "model_name = 'chatbot_model'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = .15\n",
    "batch_size = 64\n",
    "\n",
    "# 2\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "\n",
    "if loadFilename:\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "print('Building encoder and decoder ...')\n",
    "\n",
    "# 3\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# 4\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = DecoderRNN(embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# 5\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')\n",
    "\n",
    "# 6 \n",
    "save_dir = '../models/chatbot'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "epochs = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# 7\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# 8\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(),lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# 9\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# 10\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, epochs, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To begin the evaluation, we first switch our model into evaluation mode. As with all other PyTorch models, this is done to prevent any further parameter updates occurring within the evaluation process:\n",
    "\n",
    "```python\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "```\n",
    "\n",
    "2. We also initialize an instance of GreedySearchDecoder in order to be able to perform the evaluation and return the predicted output as text:\n",
    "\n",
    "```python\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "```\n",
    "\n",
    "3. Finally, to run the chatbot, we simply call the runchatbot function, passing it encoder, decoder, searcher, and voc:\n",
    "```py\n",
    "runchatbot(encoder, decoder, searcher, voc)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embedding): Embedding(9719, 500)\n",
       "  (embedding_dropout): Dropout(p=0.15, inplace=False)\n",
       "  (gru): GRU(500, 500, num_layers=2, dropout=0.15)\n",
       "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (out): Linear(in_features=500, out_features=9719, bias=True)\n",
       "  (attn): Attn()\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: i don't know . is it . !\n",
      "Response: i don't know . is it . !\n",
      "Response: i don't know . is it . !\n",
      "Response: i don't know . is it . !\n"
     ]
    }
   ],
   "source": [
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "runChatbot(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that we have created a chatbot capable of simple back and forth conversations. However, we still have a long way to go before our chatbot is able to pass the Turing test and be able to convince us that we are actually talking to a human being. However, considering the relatively small corpus of data we have trained our model on, the use of attention in our sequence-to-sequence model has shown reasonably good results, demonstrating just how versatile these architectures can be.\n",
    "\n",
    "While the best chatbots are trained on vast corpuses of billions of data points, our model has proven reasonably effective with a relatively small one. However, basic attention networks are no longer state-of-the-art and in our next chapter, we will discuss some of the more recent developments for NLP learning that have resulted in even more realistic chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although **sequence-to-sequence models with attention were state-of-the-art in 2017**, machine learning is a rapidly progressing field and since then, there have been multiple improvements made to these models. In the final chapter, we will discuss some of these state-of-the-art models in more detail, as well as cover several other contemporary techniques used in machine learning for NLP, many of which are still in development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "580318c0c86e0542a7f2f9882bbbc393e6c88c07c2a75daeff3d2ea36de686a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
