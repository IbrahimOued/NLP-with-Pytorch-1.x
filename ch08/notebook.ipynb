{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Building a Chatbot Using Attention-Based Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The theory of attention within neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our sequence-to-sequence model for sentence translation (with no attention implemented), we used both encoders and decoders. **The encoder obtained a hidden state from the input sentence**, which was a representation of our sentence. **The decoder then used this hidden state to perform the translation steps**. A basic graphical illustration of this is as follows:\n",
    "\n",
    "![](graphical_repr_seq2seq_model.png)\n",
    "\n",
    "However, **decoding over the entirety of the hidden state is not necessarily the most efficient way of using this task**. This is because the hidden state represents the entirety of the input sentence; however, in some tasks (such as predicting the next word in a sentence), **we do not need to consider the entirety of the input sentence, just the parts that are relevant to the prediction** we are trying to make. We can show that by using attention within our sequence-to-sequence neural network. We can teach our model to only look at the relevant parts of the input in order to make its prediction, resulting in a much more efficient and accurate model.\n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "***I will be traveling to Paris, the capital city of France, on the 2nd of March. My flight leaves from London Heathrow airport and will take approximately one hour.***\n",
    "\n",
    "Let's say that we are training a model to predict the next word in a sentence. We can first input the start of the sentence:\n",
    "\n",
    "***The capital city of France is _____.***\n",
    "\n",
    "We would expect our model to be able to retrieve the word Paris, in this case. If we were to use our basic sequence-to-sequence model, we would transform our entire input into a hidden state, which our model would then try to extract the relevant information out of. This includes all the extraneous information about flights. You may notice here that we only need to look at a small part of our input sentence in order to identify the relevant information required to complete our sentence:\n",
    "\n",
    "***I will be traveling to <font color=pink>Paris, the capital city of France</font>, on the 2nd of March. My flight leaves from London Heathrow airport and will take approximately one hour.***\n",
    "\n",
    "Therefore, if we can train our model to only use the relevant information within the input sentence, we can make more accurate and relevant predictions. We can implement attention within our networks in order to achieve this.\n",
    "\n",
    "There are two main types of attention mechanisms that we can implement: local and global attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparing local and global attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $2$ forms of attention that we can implement within our networks are very similar, but with subtle key differences. We will start by looking at local attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **<font color=pink>local attention</font>**, our model **only looks at a few hidden states from the encoder**. For example, if we are performing a sentence translation task and we are calculating the second word in our translation, the model may wish to only look at the hidden states from the encoder related to the second word in the input sentence. This would mean that our model needs to look at the second hidden state from our encoder $(h_2)$ but maybe also the hidden state before it $(h_1)$.\n",
    "\n",
    "In the following diagram, we can see this in practice:\n",
    "\n",
    "![](local_attention_model.png)\n",
    "\n",
    "We first **start by calculating the aligned position, $p_t$, from our final hidden state, $h_n$**. This tells us **which hidden states we need to be looking at to make our prediction**. We then **calculate our local weights and apply them to our hidden states in order to determine our context vector**. These weights may tell us **to pay more attention to the most relevant hidden state $(h_2)$ but less attention to the preceding hidden state $(h_1)$**.\n",
    "\n",
    "We then **take our context vector and pass it forward to our decoder in order to make its prediction.** In our **non-attention based sequence-to-sequence model, we would have only passed our final hidden state, $h_n$, forward**, but we see here that instead, we **only consider the relevant hidden states that our model deems necessary** to make its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **<font color=pink>global attention</font>** model **works in a very similar way**. **However, instead of only looking at a few of the hidden states, we want to look at all of our model's hidden states**—hence the name global. We can see a graphical illustration of a global attention layer here:\n",
    "\n",
    "![](global_attention_model.png)\n",
    "\n",
    "We can see in the preceding diagram that although this appears very similar to our local attention framework, **our model is now looking at all the hidden states and calculating the global weights across all of them**. This **allows our model to look at any given part of the input sentence that it considers relevant, instead of being limited to a local area determined by the local attention methodology**. Our model may wish to only look at a small, local area, but this is within the capabilities of the model. **An easy way to think of the global attention framework is that it is essentially learning a mask that only allows through hidden states that are relevant to our prediction:**\n",
    "\n",
    "![](combined_model.png)\n",
    "\n",
    "We can see in the preceding diagram that **by learning which hidden states to pay attention to, our model controls which states are used in the decoding step to determine our predicted output**. Once we decide which hidden states to pay attention to, we can combine them using a number of different methods—either by concatenating or taking the weighted dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a chatbot using sequence-to-sequence neural networks with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Acquiring our dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processing our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
      "\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
      "\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 Let's read the dataset\n",
    "import os\n",
    "corpus = \"movie_corpus\"\n",
    "corpus_name = \"movie_corpus\"\n",
    "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
    "with open(datafile, 'rb') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines[:3]:\n",
    "        print(str(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating the vocabulary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past, our corpus has comprised of several dictionaries consisting of the unique words in our corpus and lookups between word and indices. However, we can do this in a far more elegant way **by creating a vocabulary class that consists of all of the elements required**:\n",
    "\n",
    "1. We start by creating our `vocabulary` class. We initialize this class with empty dictionaries-`word2index` and `word2count`. We also initialize this `index2word` dictionary with placeholders for out padding tokens, as well as our `Start-of-Sentence (SOS)` and `End-of-Sentence (EOS)` tokens. We keep a running count of the number of words in our vocabulary, too (which is $3$ to start with as our corpus already contains the three tokens mentioned). These are the default values for an empty vocabulary; however, they will be populated as we read our data in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocabularity:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we create the functions that we create the functions that we will use to populate our vocabulary. `addWord` takes a word as input. If this is a new word that is not already in our vocabulary, we add this word to our indices, set the count of this word to $1$, and increment the total number of words in our vocabulary by $1$. If the word in question is already in our vocabulary, we simply increment the count of this word by $1$.\n",
    "\n",
    "```python\n",
    "def addWord(self, w):\n",
    "    if w not in self.word2index:\n",
    "        self.word2index[w] = self.num_words\n",
    "        self.word2count[w] = 1\n",
    "        self.index2word[self.num_words] = w\n",
    "        self.num_words += 1\n",
    "    else:\n",
    "        self.word2count[w] += 1\n",
    "```\n",
    "\n",
    "3. We also use the `addSentence` function to apply the `addWord` function to all the words within a given sentence:\n",
    "\n",
    "```python\n",
    "def addSentence(self, sent):\n",
    "    for word in sent.split(''):\n",
    "        self.addWord(word)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can do to speed up the training of our model is reduce the size of our vocabulary. This means that any embedding layers will be much smaller and the total number of learned parameters within our model can be fewer. An easy way to do this is to remove any low-frequency words from our vocabulary. Any words occurring just once or twice in our dataset are unlikely to have huge predictive power, and so removing them from our corpus and replacing them with blank tokens in our final model could reduce the time taken for our model to train and reduce overfitting without having much of a negative impact on our model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To remove low-frequency words from our vocabulary, we can implement a trim function. The function first loops through the word count dictionary and if the occurrence of the word is greater than the minimum required count, it is appended to a new list:\n",
    "\n",
    "```python\n",
    "def trim(self, min_cnt):\n",
    "    if self.trimmed:\n",
    "        return\n",
    "    self.trimmed = True\n",
    "    words_to_keep = []\n",
    "    for k in self.word2count.items():\n",
    "        if v >= min_cnt:\n",
    "            words_to_keep.append(k)\n",
    "    print('Words to keep: {} / {} = {:.2%}'.format(len(words_to_keep), len(self.word2index), len(words_to_keep)/len(words_to_keep)))\n",
    "```\n",
    "5. Finally, our indices are rebuilt from the `new_to_keep` list. We set all the indice to their initial empty values and then repopulate them by looping through our kept words with the `addWord` function:\n",
    "\n",
    "```python\n",
    "self.word2index = {}\n",
    "self.word2count = {}\n",
    "self.index2word = {PAD_token: \"PAD\", sos_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "\n",
    "self.num_words = 3\n",
    "for w in words_to_keep:\n",
    "    self.addWord(w)\n",
    "```\n",
    "\n",
    "We have defined a vocabulary class that can be easily poplulated with our input sentences, Next, we actually need to load in our dataset to create our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Here the complete code of the vocabulary class***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocabularity:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "    \n",
    "    # 2\n",
    "    def addWord(self, w):\n",
    "        if w not in self.word2index:\n",
    "            self.word2index[w] = self.num_words\n",
    "            self.word2count[w] = 1\n",
    "            self.index2word[self.num_words] = w\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[w] += 1\n",
    "\n",
    "    # 3\n",
    "    def addSentence(self, sent):\n",
    "        for word in sent.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    # 4\n",
    "    def trim(self, min_cnt):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        words_to_keep = []\n",
    "        for k in self.word2count.items():\n",
    "            if k >= min_cnt:\n",
    "                words_to_keep.append(k)\n",
    "        print('Words to keep: {} / {} = {:.2%}'.format(len(words_to_keep), len(self.word2index), len(words_to_keep)/len(words_to_keep)))\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "\n",
    "        self.num_words = 3\n",
    "        for w in words_to_keep:\n",
    "            self.addWord(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start loading in the data using the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 The first step for reading in our data is to perform any\n",
    "# necessary steps to clean the data and make it more\n",
    "# human-readable. We start by converting it from Unicode\n",
    "# into ASCII format. We can easily use a function to do this:\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# 2 Next, we want to process our input strings so that they are\n",
    "# all in lowercase and do not contain any trailing whitespace or\n",
    "# punctuation, except the most basic characters. We can do this\n",
    "# by using a series of regular expressions\n",
    "def cleanStrings(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# 3 Finally, we apply this function within a wider funtion\n",
    "# readVocs. This functions reads our data file into lines and\n",
    "# then applies the cleanString function to every line. It also\n",
    "# creates an instance of the vocabulary class that we created earlier\n",
    "# meaning, this function outputs both our data data and vocabulary:\n",
    "def readVocs(datafile, corpus_name):\n",
    "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[cleanStrings(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Vocabularity(corpus_name)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we filter our input pairs by their maximum length. This is again done to reduce the potential dimensionality of our model. Predicting sentences that are hundreds of words long would require a very deep architecture. In the interest of training time, we want to limit our training data here to instances where the input and output are less than $10$ words long.\n",
    "\n",
    "4. To do this, we create a couple of filter functions. The first one, `filterPair`, returns a Boolean value based on whether the current line has an input and output length that is less than the maximum length. Our second function, `filterPairs`, simply applies this condition to all the pairs within our dataset, only keeping the ones that meet this condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p, max_length):\n",
    "    return len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length\n",
    "\n",
    "def filterPairs(pairs, max_length):\n",
    "    return [pair for pair in pairs if filterPair(pair, max_length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now, we just need to create one final function that applies all the previous functions we have put together and run it to create our vocabulary and data pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221282 Sentence pairs\n",
      "70015 Sentence pairs after trimming\n",
      "27501 Distinct words in vocabulary\n"
     ]
    }
   ],
   "source": [
    "def loadData(corpus, corpus_name, datafile, max_length):\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(str(len(pairs)) + \" Sentence pairs\")\n",
    "    pairs = filterPairs(pairs, max_length)\n",
    "    print(str(len(pairs)) + \" Sentence pairs after trimming\")\n",
    "    for p in pairs:\n",
    "        voc.addSentence(p[0])\n",
    "        voc.addSentence(p[1])\n",
    "    print(str(voc.num_words) + \" Distinct words in vocabulary\")\n",
    "    return voc, pairs\n",
    "\n",
    "max_length = 10\n",
    "voc, pairs = loadData(corpus, corpus_name, datafile, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our input dataset consists of over $200,000$ pairs. When we filter this to sentences where both the input and output are less than $10$ words long, this reduces to just $64,000$ pairs consisting of $18,000$ distinct word (previous result):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Pairs:\n",
      "['three minutes to go !', 'yes .']\n",
      "['yes .', \"what d'ya want to do to kill time ?\"]\n",
      "['another fifteen seconds to go .', 'do something ! stall them !']\n",
      "['yes, sir, name, please ?', 'food !']\n",
      "['food !', 'do you have a reservation ?']\n",
      "['do you have a reservation ?', 'food ! !']\n",
      "['grrrhmmnnnjkjmmmnn !', 'franz ! help ! lunatic !']\n",
      "[\"what o'clock is it, mr noggs ?\", \"eleven o'clock, my lorj 42\"]\n",
      "['stuart ?', 'yes .']\n",
      "['yes .', 'how quickly can you move your artillery forward ?']\n"
     ]
    }
   ],
   "source": [
    "#  6 We can print a selection of our processed input/output\n",
    "# pairs in order to verify that our functions have all worked correctly:\n",
    "print(\"Example Pairs:\")\n",
    "for pair in pairs[-10:]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we have successfully split our dataset into input and output pairs upon which we can train our network.\n",
    "\n",
    "Finally, before we begin building the model, we must remove the rare words from our corpus and data pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing rare words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, including words that only occur a few times within our dataset will increase the dimensionality of our model, increasing our model's complexity and the time it will take to train the model. Therefore, it is preferred to remove them from our training data to keep our model as streamlined and efficient as possible.\n",
    "\n",
    "You may recall earlier that we built a trim function into our vocabulary, which will allow us to remove infrequently occurring words from our vocabulary. We can now create a function to remove these rare words and call the trim method from our vocabulary as our first step. You will see that this removes a large percentage of words from our vocabulary, indicating that the majority of the words in our vocabulary occur infrequently. This is expected as the distribution of words within any language model will follow a long-tail distribution. We will use the following steps to remove the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from 70015 pairs to 70015, 100.00% of total\n"
     ]
    }
   ],
   "source": [
    "# 1 We first calculate the percentage of words that we will keep within our model\n",
    "def removeRareWords(voc, all_pairs, minimum):\n",
    "    voc.trim(minimum)\n",
    "    # 2 we loop through all the words in the input and output sentences.\n",
    "    # If for a given pair either the input or output sentence has a word\n",
    "    # that isn't in our new trimmed corpus, we drop this pair from our dataset.\n",
    "    # We print the output and see that even though we have dropped over half\n",
    "    # of our vocabulary, we only drop around 17% of our training pairs. This\n",
    "    # again reflects how our corpus of words is distributed over our individual\n",
    "    # training pairs:\n",
    "    pairs_to_keep = []\n",
    "    for p in all_pairs:\n",
    "        keep = True\n",
    "        for word in p[0].split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "        for word in p[1].split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "        if keep:\n",
    "            pairs_to_keep.append(p)\n",
    "\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.2%} of total\".format(len(all_pairs), len(pairs_to_keep), len(pairs_to_keep)/ len(all_pairs)))\n",
    "    return pairs_to_keep\n",
    "minimum_count = 3\n",
    "pairs = removeRareWords(voc, pairs, minimum_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transforming sentence pairs to tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that **our model will not take raw text as input**, but rather, tensor representations of sentences. We will also **not process our sentences one by one, but instead in smaller batches**. For this, **we require both our input and output sentences to be transformed into tensors**, where the width of the tensor represents the size of the batch that we wish to train on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ibrao\\OneDrive\\Documents\\Deep Learning\\NLP with Pytorch 1.x\\ch08\\notebook.ipynb Cellule 35\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# 7 This function should be all we need to transform our training pairs into\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# tensors for training our model. We can validate that this is working correctly\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# by performing a single iteration of our batch2Train function on a random\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# selection of our data. We set our batch size to 5 and run this once:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m test_batch_size \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m batches \u001b[39m=\u001b[39m batch2Train(voc, [random\u001b[39m.\u001b[39;49mchoice(pairs) \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(test_batch_size)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m input_variable, lengths, target_variable, mask, max_target_len \u001b[39m=\u001b[39m batches\n",
      "\u001b[1;32mc:\\Users\\ibrao\\OneDrive\\Documents\\Deep Learning\\NLP with Pytorch 1.x\\ch08\\notebook.ipynb Cellule 35\u001b[0m in \u001b[0;36mbatch2Train\u001b[1;34m(voc, batch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     input_batch\u001b[39m.\u001b[39mappend(p[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     output_batch\u001b[39m.\u001b[39mappend(p[\u001b[39m1\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     inp, lengths \u001b[39m=\u001b[39m inputVar(input_batch, voc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     output, mask, max_target_len \u001b[39m=\u001b[39m outputVar(output_batch, voc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mreturn\u001b[39;00m inp, lengths, output, mask, max_target_len\n",
      "\u001b[1;32mc:\\Users\\ibrao\\OneDrive\\Documents\\Deep Learning\\NLP with Pytorch 1.x\\ch08\\notebook.ipynb Cellule 35\u001b[0m in \u001b[0;36minputVar\u001b[1;34m(l, voc)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minputVar\u001b[39m(l, voc):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     indexes_batch \u001b[39m=\u001b[39m [indexFromSentence(voc, sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m l]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     padList \u001b[39m=\u001b[39m zeroPad(indexes_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     padTensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(padList)\n",
      "\u001b[1;32mc:\\Users\\ibrao\\OneDrive\\Documents\\Deep Learning\\NLP with Pytorch 1.x\\ch08\\notebook.ipynb Cellule 35\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minputVar\u001b[39m(l, voc):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     indexes_batch \u001b[39m=\u001b[39m [indexFromSentence(voc, sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m l]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     padList \u001b[39m=\u001b[39m zeroPad(indexes_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     padTensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(padList)\n",
      "\u001b[1;32mc:\\Users\\ibrao\\OneDrive\\Documents\\Deep Learning\\NLP with Pytorch 1.x\\ch08\\notebook.ipynb Cellule 35\u001b[0m in \u001b[0;36mindexFromSentence\u001b[1;34m(voc, sentence)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mindexFromSentence\u001b[39m(voc, sentence):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [voc\u001b[39m.\u001b[39mword2index[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m [EOS_token]]\n",
      "\u001b[1;32mc:\\Users\\ibrao\\OneDrive\\Documents\\Deep Learning\\NLP with Pytorch 1.x\\ch08\\notebook.ipynb Cellule 35\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mindexFromSentence\u001b[39m(voc, sentence):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch08/notebook.ipynb#Y101sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [voc\u001b[39m.\u001b[39;49mword2index[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m [EOS_token]]\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "# 1 We start by creating helper functions, which we can use to transform\n",
    "# out pairs into tensors. We first create a indexFromSentence function,\n",
    "# which grabs the index of each word in the sentence from the vocabulart and\n",
    "# appends EOS token to the end\n",
    "def indexFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ') + [EOS_token]]\n",
    "\n",
    "# 2 Secondly, we create a zeroPad function which pads any tensors with\n",
    "# zeros so that all of the sentences within the tensor are effectively the\n",
    "# same length\n",
    "def zeroPad(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "# 3 Then, to generate our input tensor, we apply both of these functions\n",
    "#  First, we get the indices of our input sentence, then apply padding,\n",
    "# and then transform the output into LongTensor. We will also obtain the\n",
    "# lengths of each of our input sentences out output this as a tensor:\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    return padTensor, lengths\n",
    "\n",
    "# 4 Within our model, our padded tokens should gererally be ignored.\n",
    "# We don't want to train our model on these padded tokens, so we create a\n",
    "# boolean mask to ignore these tokens. To do so, we use a getMask function,\n",
    "# which we apply to our output tensor. This simply returns 1 if the output consists\n",
    "# of a word and 0 if it consists of a padding token\n",
    "def getMask(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# 5 We then apply this to our outputVar function, this is identical to the inputVar\n",
    "# function except that along with the indexed output tensor and the tensor of lengths,\n",
    "# we also return the boolean mask of our output tensor. This boolean mask just returns\n",
    "# True when there is a word within the output tensor and False when there is a padding\n",
    "# token. We also return the maximum length of sentences within our output tensor:\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    mask = torch.BoolTensor(getMask(padList))\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    return padTensor, mask, max_target_len\n",
    "\n",
    "# 6 Finally in order to create our input and output batches concurrently, we loop\n",
    "# through the pairs in our batch and create input and output tensors for both pairs\n",
    "# using the functions we created previously. We then return all the necessary variables:\n",
    "def batch2Train(voc, batch):\n",
    "    batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "\n",
    "    for p in batch:\n",
    "        input_batch.append(p[0])\n",
    "        output_batch.append(p[1])\n",
    "        inp, lengths = inputVar(input_batch, voc)\n",
    "        output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# 7 This function should be all we need to transform our training pairs into\n",
    "# tensors for training our model. We can validate that this is working correctly\n",
    "# by performing a single iteration of our batch2Train function on a random\n",
    "# selection of our data. We set our batch size to 5 and run this once:\n",
    "\n",
    "test_batch_size = 5\n",
    "\n",
    "batches = batch2Train(voc, [random.choice(pairs) for _ in range(test_batch_size)])\n",
    "\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "580318c0c86e0542a7f2f9882bbbc393e6c88c07c2a75daeff3d2ea36de686a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
