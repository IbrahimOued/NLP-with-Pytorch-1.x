{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Building a Chatbot Using Attention-Based Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The theory of attention within neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our sequence-to-sequence model for sentence translation (with no attention implemented), we used both encoders and decoders. **The encoder obtained a hidden state from the input sentence**, which was a representation of our sentence. **The decoder then used this hidden state to perform the translation steps**. A basic graphical illustration of this is as follows:\n",
    "\n",
    "![](graphical_repr_seq2seq_model.png)\n",
    "\n",
    "However, **decoding over the entirety of the hidden state is not necessarily the most efficient way of using this task**. This is because the hidden state represents the entirety of the input sentence; however, in some tasks (such as predicting the next word in a sentence), **we do not need to consider the entirety of the input sentence, just the parts that are relevant to the prediction** we are trying to make. We can show that by using attention within our sequence-to-sequence neural network. We can teach our model to only look at the relevant parts of the input in order to make its prediction, resulting in a much more efficient and accurate model.\n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "***I will be traveling to Paris, the capital city of France, on the 2nd of March. My flight leaves from London Heathrow airport and will take approximately one hour.***\n",
    "\n",
    "Let's say that we are training a model to predict the next word in a sentence. We can first input the start of the sentence:\n",
    "\n",
    "***The capital city of France is _____.***\n",
    "\n",
    "We would expect our model to be able to retrieve the word Paris, in this case. If we were to use our basic sequence-to-sequence model, we would transform our entire input into a hidden state, which our model would then try to extract the relevant information out of. This includes all the extraneous information about flights. You may notice here that we only need to look at a small part of our input sentence in order to identify the relevant information required to complete our sentence:\n",
    "\n",
    "***I will be traveling to <font color=pink>Paris, the capital city of France</font>, on the 2nd of March. My flight leaves from London Heathrow airport and will take approximately one hour.***\n",
    "\n",
    "Therefore, if we can train our model to only use the relevant information within the input sentence, we can make more accurate and relevant predictions. We can implement attention within our networks in order to achieve this.\n",
    "\n",
    "There are two main types of attention mechanisms that we can implement: local and global attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparing local and global attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $2$ forms of attention that we can implement within our networks are very similar, but with subtle key differences. We will start by looking at local attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **<font color=pink>local attention</font>**, our model **only looks at a few hidden states from the encoder**. For example, if we are performing a sentence translation task and we are calculating the second word in our translation, the model may wish to only look at the hidden states from the encoder related to the second word in the input sentence. This would mean that our model needs to look at the second hidden state from our encoder $(h_2)$ but maybe also the hidden state before it $(h_1)$.\n",
    "\n",
    "In the following diagram, we can see this in practice:\n",
    "\n",
    "![](local_attention_model.png)\n",
    "\n",
    "We first **start by calculating the aligned position, $p_t$, from our final hidden state, $h_n$**. This tells us **which hidden states we need to be looking at to make our prediction**. We then **calculate our local weights and apply them to our hidden states in order to determine our context vector**. These weights may tell us **to pay more attention to the most relevant hidden state $(h_2)$ but less attention to the preceding hidden state $(h_1)$**.\n",
    "\n",
    "We then **take our context vector and pass it forward to our decoder in order to make its prediction.** In our **non-attention based sequence-to-sequence model, we would have only passed our final hidden state, $h_n$, forward**, but we see here that instead, we **only consider the relevant hidden states that our model deems necessary** to make its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **<font color=pink>global attention</font>** model **works in a very similar way**. **However, instead of only looking at a few of the hidden states, we want to look at all of our model's hidden states**—hence the name global. We can see a graphical illustration of a global attention layer here:\n",
    "\n",
    "![](global_attention_model.png)\n",
    "\n",
    "We can see in the preceding diagram that although this appears very similar to our local attention framework, **our model is now looking at all the hidden states and calculating the global weights across all of them**. This **allows our model to look at any given part of the input sentence that it considers relevant, instead of being limited to a local area determined by the local attention methodology**. Our model may wish to only look at a small, local area, but this is within the capabilities of the model. **An easy way to think of the global attention framework is that it is essentially learning a mask that only allows through hidden states that are relevant to our prediction:**\n",
    "\n",
    "![](combined_model.png)\n",
    "\n",
    "We can see in the preceding diagram that **by learning which hidden states to pay attention to, our model controls which states are used in the decoding step to determine our predicted output**. Once we decide which hidden states to pay attention to, we can combine them using a number of different methods—either by concatenating or taking the weighted dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a chatbot using sequence-to-sequence neural networks with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Acquiring our dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processing our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
      "\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
      "\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 Let's read the dataset\n",
    "import os\n",
    "corpus = \"movie_corpus\"\n",
    "corpus_name = \"movie_corpus\"\n",
    "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
    "with open(datafile, 'rb') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines[:3]:\n",
    "        print(str(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating the vocabulary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past, our corpus has comprised of several dictionaries consisting of the unique words in our corpus and lookups between word and indices. However, we can do this in a far more elegant way **by creating a vocabulary class that consists of all of the elements required**:\n",
    "\n",
    "1. We start by creating our `vocabulary` class. We initialize this class with empty dictionaries-`word2index` and `word2count`. We also initialize this `index2word` dictionary with placeholders for our `padding tokens`, as well as our `Start-of-Sentence (SOS)` and `End-of-Sentence (EOS)` tokens. We keep a running count of the number of words in our vocabulary, too (which is $3$ to start with as our corpus already contains the three tokens mentioned). These are the default values for an empty vocabulary; however, they will be populated as we read our data in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, we create the functions that we will use to populate our vocabulary. `addWord` takes a word as input. **If this is a new word that is not already in our vocabulary, we add this word to our indices**, **set the count of this word** to $1$, and **increment the total number of words in our vocabulary** by $1$. **If the word in question is already in our vocabulary**, we simply **increment the count of this word** by $1$.\n",
    "\n",
    "```python\n",
    "    def addWord(self, w):\n",
    "        if w not in self.word2index:\n",
    "            self.word2index[w] = self.num_words\n",
    "            self.word2count[w] = 1\n",
    "            self.index2word[self.num_words] = w\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[w] += 1\n",
    "```\n",
    "\n",
    "3. We also use the `addSentence` function to apply the `addWord` function to all the words within a given sentence:\n",
    "\n",
    "```python\n",
    "    def addSentence(self, sent):\n",
    "        for word in sent.split(''):\n",
    "            self.addWord(word)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can do to **speed up the training of our model** is **reduce the size of our vocabulary**. This means that **any embedding layers will be much smaller and the total number of learned parameters within our model can be fewer**. An easy way to do this is to **remove any low-frequency words from our vocabulary**. Any words occurring just once or twice in our dataset are unlikely to have huge predictive power, and so removing them from our corpus and replacing them with blank tokens in our final model could reduce the time taken for our model to train and reduce overfitting without having much of a negative impact on our model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To remove low-frequency words from our vocabulary, we can implement a `trim()` function. The function first loops through the word count dictionary and if the occurrence of the word is greater than the minimum required count, it is appended to a new list:\n",
    "\n",
    "```python\n",
    "    def trim(self, min_cnt):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        words_to_keep = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_cnt:\n",
    "                words_to_keep.append(k)\n",
    "        print('Words to keep: {} / {} = {:.2%}'.format(len(words_to_keep), len(self.word2index), len(words_to_keep)/len(self.word2index)))\n",
    "```\n",
    "5. Finally, our indices are rebuilt from the new `words_to_keep` list. We set all the indice to their initial empty values and then repopulate them by looping through our kept words with the `addWord` function:\n",
    "\n",
    "```python\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", sos_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "\n",
    "        self.num_words = 3\n",
    "        for w in words_to_keep:\n",
    "            self.addWord(w)\n",
    "```\n",
    "\n",
    "We have defined a vocabulary class that can be easily poplulated with our input sentences, Next, we actually need to load in our dataset to create our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Here the complete code of the vocabulary class***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values for our empty vocabulary\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "# 1 We create the vocabulary class\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "    \n",
    "    # 2 The function use to populate our vocabulary\n",
    "    def addWord(self, w):\n",
    "        if w not in self.word2index:\n",
    "            self.word2index[w] = self.num_words\n",
    "            self.word2count[w] = 1\n",
    "            self.index2word[self.num_words] = w\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[w] += 1\n",
    "\n",
    "    # 3 To apply addWord to all the words of a given sentence\n",
    "    def addSentence(self, sent):\n",
    "        for word in sent.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    # 4 To remove low-frequency words from our vocabulary\n",
    "    def trim(self, min_cnt):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        words_to_keep = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_cnt:\n",
    "                words_to_keep.append(k)\n",
    "        print('Words to keep: {} / {} = {:.2%}'.format(len(words_to_keep), len(self.word2index), len(words_to_keep)/len(self.word2index)))\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "\n",
    "        self.num_words = 3\n",
    "        for w in words_to_keep:\n",
    "            self.addWord(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start loading in the data using the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 The first step for reading in our data is to perform any\n",
    "# necessary steps to clean the data and make it more\n",
    "# human-readable. We start by converting it from Unicode\n",
    "# into ASCII format. We can easily use a function to do this:\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# 2 Next, we want to process our input strings so that they are\n",
    "# all in lowercase and do not contain any trailing whitespace or\n",
    "# punctuation, except the most basic characters. We can do this\n",
    "# by using a series of regular expressions\n",
    "def cleanStrings(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# 3 Finally, we apply this function within a wider funtion\n",
    "# readVocs. This functions reads our data file into lines and\n",
    "# then applies the cleanString function to every line. It also\n",
    "# creates an instance of the vocabulary class that we created earlier\n",
    "# meaning, this function outputs both our data and vocabulary:\n",
    "def readVocs(datafile, corpus_name):\n",
    "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[cleanStrings(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Vocabulary(corpus_name)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we **filter our input pairs by their maximum length**. This is again done **to reduce the potential dimensionality of our model**. **Predicting sentences that are hundreds of words long would require a very deep architecture**. In the interest of training time, we want to limit our training data here to instances where the input and output are less than $10$ words long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. To do this, we create a couple of filter functions. The first one,\n",
    "# `filterPair`, returns a Boolean value based on whether the current\n",
    "# line has an input and output length that is less than the maximum\n",
    "# length. Our second function, `filterPairs`, simply applies this condition\n",
    "# to all the pairs within our dataset, only keeping the ones that meet this condition:\n",
    "def filterPair(p, max_length):\n",
    "    return len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length\n",
    "\n",
    "def filterPairs(pairs, max_length):\n",
    "    return [pair for pair in pairs if filterPair(pair, max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221282 Sentence pairs\n",
      "70015 Sentence pairs after trimming\n",
      "27501 Distinct words in vocabulary\n"
     ]
    }
   ],
   "source": [
    "# 5. Now, we just need to create one final function that applies all the previous\n",
    "# functions we have put together and run it to create our vocabulary and data pairs:\n",
    "def loadData(corpus, corpus_name, datafile, max_length):\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(str(len(pairs)) + \" Sentence pairs\")\n",
    "    pairs = filterPairs(pairs, max_length)\n",
    "    print(str(len(pairs)) + \" Sentence pairs after trimming\")\n",
    "    for p in pairs:\n",
    "        voc.addSentence(p[0])\n",
    "        voc.addSentence(p[1])\n",
    "    print(str(voc.num_words) + \" Distinct words in vocabulary\")\n",
    "    return voc, pairs\n",
    "\n",
    "max_length = 10\n",
    "voc, pairs = loadData(corpus, corpus_name, datafile, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our input dataset consists of over $200,000$ pairs. When we filter this to sentences where both the input and output are less than $10$ words long, this reduces to just $64,000$ pairs consisting of $18,000$ distinct word (previous result):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Pairs:\n",
      "['three minutes to go !', 'yes .']\n",
      "['yes .', \"what d'ya want to do to kill time ?\"]\n",
      "['another fifteen seconds to go .', 'do something ! stall them !']\n",
      "['yes, sir, name, please ?', 'food !']\n",
      "['food !', 'do you have a reservation ?']\n",
      "['do you have a reservation ?', 'food ! !']\n",
      "['grrrhmmnnnjkjmmmnn !', 'franz ! help ! lunatic !']\n",
      "[\"what o'clock is it, mr noggs ?\", \"eleven o'clock, my lorj 42\"]\n",
      "['stuart ?', 'yes .']\n",
      "['yes .', 'how quickly can you move your artillery forward ?']\n"
     ]
    }
   ],
   "source": [
    "#  6 We can print a selection of our processed input/output\n",
    "# pairs in order to verify that our functions have all worked correctly:\n",
    "print(\"Example Pairs:\")\n",
    "for pair in pairs[-10:]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we have successfully split our dataset into input and output pairs upon which we can train our network.\n",
    "\n",
    "Finally, before we begin building the model, we must remove the rare words from our corpus and data pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing rare words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, including words that only occur a few times within our dataset will increase the dimensionality of our model, increasing our model's complexity and the time it will take to train the model. Therefore, it is preferred to remove them from our training data to keep our model as streamlined and efficient as possible.\n",
    "\n",
    "You may recall earlier that we built a `trim()` function into our vocabulary, which will allow us to remove infrequently occurring words from our vocabulary. We can now create a function to remove these rare words and call the `trim()` method from our vocabulary as our first step. You will see that this removes a large percentage of words from our vocabulary, indicating that the majority of the words in our vocabulary occur infrequently. This is expected as the distribution of words within any language model will follow a long-tail distribution. We will use the following steps to remove the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words to keep: 9716 / 27498 = 35.33%\n",
      "Trimmed from 70015 pairs to 52223, 74.59% of total\n"
     ]
    }
   ],
   "source": [
    "# 1 We first calculate the percentage of words that we will keep within our model\n",
    "def removeRareWords(voc, all_pairs, minimum):\n",
    "    voc.trim(minimum)\n",
    "    # 2 we loop through all the words in the input and output sentences.\n",
    "    # If for a given pair either the input or output sentence has a word\n",
    "    # that isn't in our new trimmed corpus, we drop this pair from our dataset.\n",
    "    # We print the output and see that even though we have dropped over half\n",
    "    # of our vocabulary, we only drop around 17% of our training pairs. This\n",
    "    # again reflects how our corpus of words is distributed over our individual\n",
    "    # training pairs:\n",
    "    pairs_to_keep = []\n",
    "    for p in all_pairs:\n",
    "        keep = True\n",
    "        for word in p[0].split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "        for word in p[1].split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep = False\n",
    "                break\n",
    "        if keep:\n",
    "            pairs_to_keep.append(p)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.2%} of total\".format(len(all_pairs), len(pairs_to_keep), len(pairs_to_keep)/ len(all_pairs)))\n",
    "    return pairs_to_keep\n",
    "\n",
    "minimum_count = 3\n",
    "pairs = removeRareWords(voc, pairs, minimum_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transforming sentence pairs to tensors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that **our model will not take raw text as input**, but rather, **tensor representations of sentences**. We will also **not process our sentences one by one, but instead in smaller batches**. For this, **we require both our input and output sentences to be transformed into tensors**, where the **width of the tensor represents the size of the batch** that we wish to train on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "# 1 We start by creating helper functions, which we can use to transform\n",
    "# out pairs into tensors. We first create a indexFromSentence function,\n",
    "# which grabs the index of each word in the sentence from the vocabulary and\n",
    "# appends EOS token to the end\n",
    "def indexFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "# 2 Secondly, we create a zeroPad function which pads any tensors with\n",
    "# zeros so that all of the sentences within the tensor are effectively the\n",
    "# same length\n",
    "def zeroPad(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "# 3 Then, to generate our input tensor, we apply both of these functions\n",
    "#  First, we get the indices of our input sentence, then apply padding,\n",
    "# and then transform the output into LongTensor. We will also obtain the\n",
    "# lengths of each of our input sentences out output this as a tensor:\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    return padTensor, lengths\n",
    "\n",
    "# 4 Within our model, our padded tokens should gererally be ignored.\n",
    "# We don't want to train our model on these padded tokens, so we create a\n",
    "# boolean mask to ignore these tokens. To do so, we use a getMask function,\n",
    "# which we apply to our output tensor. This simply returns 1 if the output consists\n",
    "# of a word and 0 if it consists of a padding token\n",
    "def getMask(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# 5 We then apply this to our outputVar function, this is identical to the inputVar\n",
    "# function except that along with the indexed output tensor and the tensor of lengths,\n",
    "# we also return the boolean mask of our output tensor. This boolean mask just returns\n",
    "# True when there is a word within the output tensor and False when there is a padding\n",
    "# token. We also return the maximum length of sentences within our output tensor:\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPad(indexes_batch)\n",
    "    mask = torch.BoolTensor(getMask(padList))\n",
    "    padTensor = torch.LongTensor(padList)\n",
    "    return padTensor, mask, max_target_len\n",
    "\n",
    "# 6 Finally in order to create our input and output batches concurrently, we loop\n",
    "# through the pairs in our batch and create input and output tensors for both pairs\n",
    "# using the functions we created previously. We then return all the necessary variables:\n",
    "def batch2Train(voc, batch):\n",
    "    batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "\n",
    "    for p in batch:\n",
    "        input_batch.append(p[0])\n",
    "        output_batch.append(p[1])\n",
    "        inp, lengths = inputVar(input_batch, voc)\n",
    "        output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# 7 This function should be all we need to transform our training pairs into\n",
    "# tensors for training our model. We can validate that this is working correctly\n",
    "# by performing a single iteration of our batch2Train function on a random\n",
    "# selection of our data. We set our batch size to 5 and run this once:\n",
    "\n",
    "test_batch_size = 5\n",
    "\n",
    "batches = batch2Train(voc, [random.choice(pairs) for _ in range(test_batch_size)])\n",
    "\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can **validate that our input tensor has been created correctly**. Note how the **sentences end with padding** ($0$ tokens) where the **sentence length is less than the maximum length for the tensor** (in this instance, $9$). The **width of the tensor also corresponds to the batch size** (in this case, $5$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  58, 1734,  189,   33,  420],\n",
       "        [ 325,   15,  680,  117, 5311],\n",
       "        [  47,   16,  148,  489, 3474],\n",
       "        [ 195,  718,  201,   16,   15],\n",
       "        [  78,   78,  602,   10,    2],\n",
       "        [2241, 1647,  275,    2,    0],\n",
       "        [7678, 1734,   15,    0,    0],\n",
       "        [ 249,   15,    2,    0,    0],\n",
       "        [  10,    2,    0,    0,    0],\n",
       "        [   2,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can also validate the corresponding output data and mask. Notice how the False values in the mask overlap with the padding tokens (the zeroes) in our output tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 270,    5,  122,  579,  288],\n",
       "        [ 373,  312,  357,  917,   10],\n",
       "        [  75,  827,   94,   16,    2],\n",
       "        [  47,   10,  553,  488,    0],\n",
       "        [ 469,    2,   96,   10,    0],\n",
       "        [  47,    0,    7,    2,    0],\n",
       "        [1736,    0,  197,    0,    0],\n",
       "        [2048,    0,  931,    0,    0],\n",
       "        [  10,    0,   10,    0,    0],\n",
       "        [   2,    0,    2,    0,    0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True, False,  True,  True, False],\n",
       "        [ True, False,  True, False, False],\n",
       "        [ True, False,  True, False, False],\n",
       "        [ True, False,  True, False, False],\n",
       "        [ True, False,  True, False, False]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained, cleaned, and transformed our data, we are ready to begin training the attention-based model that will form the basis of our chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start, as with our other sequence-to-sequence models, by creating our encoder. This will transform the initial tensor representation of our input sentence into hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Constructing the encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As with all our PyTorch models, we start by creating an `Encoder` class that inherits from `nn.Module`.\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0:\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "```\n",
    "Next we create our `RNN` module. In this chatbot, we will be using a `Gated Recurrent Unit (GRU)` instead of the `Long Short-Term Memory (LSTM)` models we saw before. **GRUs are slightly less complex than LSTMs as although they still control the flow of information through the RNN**, they **don't have separate forget and update gates** like the LSTM. We use GRUs in this instance for a few main reasons:\n",
    "\n",
    "a- GRUs have proven to be **more computationally efficient as there are fewer parameters to learn**. This means that our model will train much more quickly with GRUs than with LSTMs\n",
    "\n",
    "b- GRUs have proven to **have similar performance levels over short sequences of data as LSTMs**. LSTMs are more useful when learning longer sequences of data. In this instance we are only using input sentences with $10$ words or less, so GRUs should produce similar results.\n",
    "\n",
    "c- GRUs have proven to be **more effective at learning from small datasets than LSTMs**. As the size of our training data is small relative to the complexity of the task we're trying to learn, we should opt to use GRUs.\n",
    "\n",
    "2. We now define our GRU, taking into account the size of our input, the number of layers, and whether we should implement dropout:\n",
    "\n",
    "```python\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout)), bidirectional=True)\n",
    "```\n",
    "Notice here how **we implement bidirectionality into our model**. You will recall from previous chapters that **a bidirectional RNN allows us to learn from a sentence moving sequentially forward through a sentence, as well as moving sequentially backward**. This allows us to better capture the context of each word in the sentence relative to those that appear before and after it. Bidirectionality in our GRU means our encoder looks something like this:\n",
    "\n",
    "![](encoder_label_gru_bidirectional.png)\n",
    "\n",
    "> We maintain two hidden states, as well as outputs at each step, within our input sentence.\n",
    "\n",
    "3. Next, we need to create a forward pass for our encoder. We do this by first embedding our input sentences and then using the `pack_padded_sequence` function on our embeddings. This function \"packs\" our padded sequence so that all of our inputs are of the same length. We then pass out the packed sequences through our GRU to perform a forward pass:\n",
    "\n",
    "```python\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "```\n",
    "\n",
    "4. After this, we unpack our padding and sum the GRU outpus. We can then return this summed output, along with our final hidden state, to complete our forward pass\n",
    "\n",
    "```python\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + a outputs[:, : ,self.hidden_size:]\n",
    "        return outputs, hidden\n",
    "```\n",
    "\n",
    "Here is the complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirection=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = nn.utils.rnn.packed_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:,:,:self.hidden_size] + outputs[:,:, self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, we will move on to creating an attention module in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Constructing the attention module**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by creating a class for the attention model\n",
    "\n",
    "```python\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "```\n",
    "\n",
    "2. Then create the `dot_score()` function within this class. This function simply calculates the dot product of our encoder output with the output of our hidden state by our encoder. While there are other ways of transforming these two tensors into a single representation, using a dot product is one of the simplest\n",
    "\n",
    "```python\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "```\n",
    "\n",
    "3. We then use this function within our forward pass. First, calculate the attention weigths/energies based on the `dot_score` method, then transpose the results, and return the softmax transformed probability scores:\n",
    "\n",
    "```python\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "```\n",
    "The complete code of the Attention class is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use this attention module within our decoder to create an attention-focused decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct the decoder, as follows:\n",
    "\n",
    "1. We begin by creating our DecoderRNN class, inheriting from nn.Module and defining the initialization parameters:\n",
    "```python\n",
    "class DecoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "```\n",
    "2. We then create our layers within this module. We will create an embedding layer and a corresponding dropout layer. We use GRUs again for our decoder; however, this time, we do not need to make our GRU layer bidirectional as we will be decoding the output from our encoder sequentially. We will also create two linear layers—one regular layer for calculating our output and one layer that can be used for concatenation. This layer is twice the width of the regular hidden layer as it will be used on two concatenated vectors, each with a length of `hidden_size`. We also initialize an instance of our attention module from the last section in order to be able to use it within our `Decoder` class:\n",
    "\n",
    "```python\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,  dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(hidden_size)\n",
    "```\n",
    "\n",
    "3. After defining all of our layers, we need to create a forward pass for the decoder. Notice how the forward pass will be used one step (word) at a time. We start by getting the embedding of the current input word and making a forward pass through the GRU layer to get our output and hidden states:\n",
    "\n",
    "```python\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "```\n",
    "4. Next, we use the attention module to get the attention weights from the GRU output. These weights are then multiplied by the encoder outputs to effectively give us a weighted sum of our attention weights and our encoder output:\n",
    "\n",
    "```python\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "```\n",
    "\n",
    "5. We then concatenate our weighted context vector with the output of our GRU and apply a tanh function to get out final concatenated output:\n",
    "\n",
    "```python\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "```\n",
    "\n",
    "6. For the final step within our decoder, we simply use this final concatenated output to predict the next word and apply a `softmax` function. The forward pass finally returns this output, along with the final hidden state. This forward pass will be iterated upon, with the next forward pass using the next word in the sentence and this new hidden state:\n",
    "\n",
    "```python\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "```\n",
    "The complete code for the Decoder class is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, output_size, n_layers=1, dropout=.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Defining the training process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the training process is to **define the measure of loss for our models**. As our input tensors may consist of padded sequences, owing to our input sentences all being of different lengths, we cannot simply calculate the difference between the true output and the predicted output tensors. To account for this, we will define a loss function that applies a Boolean mask over our outputs and only calculates the loss of the non-padded tokens:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "580318c0c86e0542a7f2f9882bbbc393e6c88c07c2a75daeff3d2ea36de686a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
