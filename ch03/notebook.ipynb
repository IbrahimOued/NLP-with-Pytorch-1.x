{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Fundamentals of Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 : NLP and Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways of representing text in deep learning. While we have covered basic bag-of-words (BoW) representations, unsurprisingly, there is a far more sophisticated way of representing text data known as **embeddings**. While a **BoW vector acts only as a count of words within a sentence**, **embeddings help to numerically define the actual meaning of certain words**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words do not have a natural way of representing their meaning. In images, we already have representations in rich vectors (containing the values of each pixel within the image). **When parts of language are represented in a high-dimensional vector format, they are known as embeddings**. Through analysis of a corpus of words, and **by determining which words appear frequently together**, we can obtain **an $n$-length vector for each word, which better represents the semantic relationship of each word to all other words**. We saw previously that we can easily represent words as one-hot encoded vectors:\n",
    "\n",
    "![](ohe_vectors.png)\n",
    "\n",
    "On the other hand, embeddings are vectors of length $n$ (in the following example, $n = 3$) that can take any value\n",
    "\n",
    "![](vectors_n_equal_3.png)\n",
    "\n",
    "These embeddings **represent the word's vector in $n$-dimensional space (where $n$ is the *length of the embedding vectors*), and words with similar vectors within this space are considered *to be more similar in meaning***. While these embeddings can be of any size, they are generally of much lower dimensionality than the BoW representation. The BOW vectors are generally very sparse, consisting mostly of zeros, whereas embeddings are rich in data and every dimension contributes to the overall representation of the word. The lower dimensionality and the fact that they are not sparse makes performing deep learning on embeddings much more efficient than performing it on BOW representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GLoVe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Vectors for Word Representation (GLoVe) embeddings is a set of pre-calculated word embeddings, It can be downloaded [here](https://nlp.stanford.edu/projects/glove/) and it will help us demonstrate how embeddings work.\n",
    "\n",
    "These embeddings are calculated on a very large corpus of NLP data and are trained on a word co-occurrence matrix. This is based on the notion that words that appear together are more likely to have similar meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !iwr -outf glove.6B.50d.zip http://nlp.stanford.edu/data/glove.6B.50d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5897  , -0.55043 , -1.0106  ,  0.41226 ,  0.57348 ,  0.23464 ,\n",
       "       -0.35773 , -1.78    ,  0.10745 ,  0.74913 ,  0.45013 ,  1.0351  ,\n",
       "        0.48348 ,  0.47954 ,  0.51908 , -0.15053 ,  0.32474 ,  1.0789  ,\n",
       "       -0.90894 ,  0.42943 , -0.56388 ,  0.69961 ,  0.13501 ,  0.16557 ,\n",
       "       -0.063592,  0.35435 ,  0.42819 ,  0.1536  , -0.47018 , -1.0935  ,\n",
       "        1.361   , -0.80821 , -0.674   ,  1.2606  ,  0.29554 ,  1.0835  ,\n",
       "        0.2444  , -1.1877  , -0.60203 , -0.068315,  0.66256 ,  0.45336 ,\n",
       "       -1.0178  ,  0.68267 , -0.20788 , -0.73393 ,  1.2597  ,  0.15425 ,\n",
       "       -0.93256 , -0.15025 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 We first create a simple function to load our GLoVe vectors\n",
    "import numpy as np\n",
    "\n",
    "def loadGlove(path):\n",
    "    file = open(path, 'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for l in file:\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "        value = np.array([float(val) for val in line[1:]])\n",
    "        model[word]  = value\n",
    "    return model\n",
    "\n",
    "glove = loadGlove(path='./glove.6B.50d.txt')\n",
    "\n",
    "# 2 We can access a single vector by juste calling it\n",
    "# from the dict\n",
    "glove['python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this **returns a $50$-dimensional vector embedding for the word *Python***. We will now introduce the concept of ***cosine similarity*** to **compare how similar two vectors are**. Vectors will have a similarity of $1$ if **the angle in the $n$-dimensional space between them is $0^{\\circ}$**. **Values with high cosine similarity can be considered similar**, even if they are not equal. This can be calculated using the following formula, where $A$ and $B$ are the two embedding vectors being compared:\n",
    "\n",
    "$\\frac{\\sum A . B}{\\sqrt{\\sum A^2} \\times \\sqrt{\\sum B^2}}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92180053]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# 3 We can calculatee easily using the cosine_similarity() of sklearn\n",
    "cosine_similarity(glove['cat'].reshape(1, -1), glove['dog'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19825255]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 However cat and piano are quite dissimilar as they\n",
    "# are two seemingly unrelated items\n",
    "cosine_similarity(glove['cat'].reshape(1, -1), glove['piano'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Embeddings operation**s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since embeddings are vectors, we can perform operations on them. For example, let's say we take the embeddings for the following sorts and we calculate the following\n",
    "\n",
    "$Queen-Woman+Man$\n",
    "\n",
    "With this, we can approximate the embedding for $king$. This essentially replaces the Woman vector component from $Queen$ with the $Man$ vector to arrive at this approximation. We can graphically illustrate this as follows:\n",
    "\n",
    "![](embeddings_operation.png)\n",
    "\n",
    "Note that in this example, we illustrate this graphically in two dimensions. In the case of our embeddings, this is happening in a $50$-dimensional space. While this is not exact, we can verify that our calculated vector is indeed similar to the GLoVe vector for King:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85888392]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_king_embedding = glove['queen'] - glove['woman'] + glove['man']\n",
    "cosine_similarity(predicted_king_embedding.reshape(1, -1), glove['king'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While GLoVe embeddings are very useful pre-calculated embeddings, it is actually possible for us to calculate our own embeddings. This may be useful when we are analyzing a particularly unique corpus. For example, **the language used on Twitter may differ from the language used on Wikipedia**, so embeddings trained on one may not be useful for the other. We will now demonstrate **how we can calculate our own embeddings using a continuous bag-of-words**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **continuous bag-of-words (CBOW)** model forms part of **Word2Vec** – a model created by Google in order to **obtain vector representations of words**. By running these models over a very large corpus, we are able to obtain detailed representations of words that represent their semantic and contextual similarity to one another. The **Word2Vec** model consists of two main components:\n",
    "\n",
    "* **CBOW**: This model attempts to predict the target word in a document, given the surrounding words\n",
    "* **Skip-gram**: This is the opposite of CBOW; this model attempts to predict the surrounding words, given the target word.\n",
    "\n",
    "Consider the following sentence:\n",
    "\n",
    "***PyTorch is a deep learning framework***\n",
    "\n",
    "Let's say we want to predict the word *deep*, given the context words:\n",
    "\n",
    "***PyTorch is a {target_word} learning framework***\n",
    "\n",
    "We could look at this in a number of ways:\n",
    "\n",
    "![](table%20of%20context%20and%20representation.png)\n",
    "\n",
    "For our CBOW model, we will use a **window of length $2$**, which means for our model's $(X, y)~input/output$ pairs, we use $([n-2, n-1, n+1, n+2, n])$, where $n$ is our target word being predicted.\n",
    "\n",
    "Using these as our model inputs, we will train a model that includes an embedding layer. This embedding layer automatically forms an $n$-dimensional representation of the words in our corpus. However, to begin with, this layer is initialized with random weights. These parameters are what will be learned using our model so that after our model has finished training, this embedding layer can be used can be used to encode our corpus in an embedded vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CBOW architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our model takes an input of $4$ words ($2$ before our target word and $2$ after) and trains it against an output (our target word). The following representation is an illustration of how this might look:\n",
    "\n",
    "![](cbow%20architecture.png)\n",
    "\n",
    "Our input words are first fed through an embedding layer, represented as a tensor of size $(n,l)$, where $n$ is the **specified length of our embeddings** and $l$ is **the number of words in our corpus**. This is because **every word within the corpus has its own unique tensor representation**.\n",
    "\n",
    "Using our combined (summed) embeddings from our $4$ context words, this is then **fed into a fully connected layer in order to learn the final classification** of our target word against our embedded representation of our context words\n",
    "\n",
    "> Note that our predicted/target word is encoded as a vector that's the length of our corpus\n",
    "\n",
    "Because our model effectively predicts the probability of each word in the corpus to be the target word, and the final classification is the one with the highest probability. We then obtain a loss, backpropagate this through our network, and update the parameters on the fully connected layer, as well as the embeddings themselves.\n",
    "\n",
    "The reason this methodology works is because our learned embeddings represent semantic similarity. Let's say we train our model on the following:\n",
    "\n",
    "$X = [\"is\", \"a\", \"learning\", \"framework\"];~y = \"deep\"$\n",
    "\n",
    "What our model is essentially learning is that **the combined embedding representation of our target words is semantically similar to our target word**. If we repeat this over a large enough corpus of words, we will find that our word embeddings begin to resemble our previously seen GLoVe embeddings, where semantically similar words appear to one another within the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building CBOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['members', 'of', 'alphago', 'team'], 'the')\n"
     ]
    }
   ],
   "source": [
    "# 1 We first define some text and perform some basic text\n",
    "# cleaning, removing basic punctuation and converting it all to\n",
    "# lowercase\n",
    "\n",
    "text = \"\"\"\n",
    "For us, the members of the AlphaGo team, the AlphaGo story was the adventure of a\n",
    "lifetime. It began, as many great adventures do, with a small step—training a simple\n",
    "convolutional neural network on records of Go games played by strong human play-\n",
    "ers. This led to pivotal breakthroughs in the recent development of machine learning,\n",
    "as well as a series of unforgettable events, including matches against the formidable\n",
    "Go professionals Fan Hui, Lee Sedol, and Ke Jie. We’re proud to see the lasting\n",
    "impact of these matches on the way Go is played around the world, as well as their role\n",
    "in making more people aware of, and interested in, the field of artificial intelligence.\n",
    "But why, you might ask, should we care about games? Just as children use games to\n",
    "learn about aspects of the real world, so researchers in machine learning use them to\n",
    "train artificial software agents. In this vein, the AlphaGo project is part of DeepMind’s\n",
    "strategy to use games as simulated microcosms of the real world. This helps us study\n",
    "artificial intelligence and train learning agents with the goal of one day building gen-\n",
    "eral purpose learning systems capable of solving the world’s most complex problems.\n",
    "AlphaGo works in a way that is similar to the two modes of thinking that Nobel\n",
    "laureate Daniel Kahnemann describes in his book on human cognition, Thinking Fast\n",
    "and Slow.\n",
    "\"\"\"\n",
    "\n",
    "text = text.replace(',', '').replace('.', '').lower().split()\n",
    "\n",
    "# 2 We start by defining our coprus and its lenght\n",
    "corpus = set(text)\n",
    "corpus_length = len(corpus)\n",
    "\n",
    "# 3 Note that we use a set instead of a list as we are only\n",
    "# converned with the unique words within our text. We then\n",
    "# build our corpus index and our inverse corpus index. Our\n",
    "# corpus index will allow us to obtain the index of a word\n",
    "# given the word itself, which will be useful when encoding\n",
    "# our words for entry into our network. Our inverse corpus\n",
    "# index allows us to obtain a word, given the index value,\n",
    "# which will be used to convert our predictions back into words:\n",
    "word_dict = {}\n",
    "inverse_word_dict = {}\n",
    "for i, word in enumerate(corpus):\n",
    "    word_dict[word] = i\n",
    "    inverse_word_dict[i] = word\n",
    "\n",
    "# 4 Next, we encode our data. We loop through our corpus\n",
    "# and for each target word, we capture the context words\n",
    "# (the two words before and the two words after). We append\n",
    "# this with the target word itself to our dataset. Note how\n",
    "# we begin this process from the third word in our corpus\n",
    "# (index = 2) and stop it two steps before the end of the\n",
    "# corpus. This is because the two words at the beginning\n",
    "# won't have two words before them and, similarly, the two\n",
    "# words at the end won't have two words after them:\n",
    "data = []\n",
    "\n",
    "for i in range(2, len(text) - 2):\n",
    "\n",
    "    sentence = [text[i-2], text[i-1], text[i+1], text[i+2]]\n",
    "    target = text[i]\n",
    "    data.append((sentence, target))\n",
    "\n",
    "print(data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We then define the length of our embeddings. While this can technically be any number you wish, there are some tradeoffs to consider. While higher-dimensional embeddings can lead to a more detailed representation of the words, the feature space also becomes sparser, which means high-dimensional embeddings are only appropriate for large corpuses. Furthermore, larger embeddings mean more parameters to learn, so increasing the embedding size can increase training time significantly. We are only training on a very small dataset, so we have opted to use embeddings of size $20$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_length = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our **CBOW model in PyTorch**. We define our embeddings layer so that **it takes a vector of corpus length in and outputs a single embedding**. We define our linear layer as a fully connected layer that takes an embedding in and outputs a vector of 64. We define our final layer as a classification layer that is the same length as our text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We define our forward pass by obtaining and summing the embeddings for all input context words. This then passes through the fully connected layer with ReLU activation functions and finally into the classification layer, which predicts which word in the corpus corresponds to the summed embeddings of the context words the most:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 78, 105,  60, 101, 110,  89])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, corpus_length, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(corpus_length, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, corpus_length)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1, -1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    # 7 We can also define a get_word_embedding() function, which will\n",
    "    # allow us to extract embeddings for a given word after our model\n",
    "    # has been trained\n",
    "    def get_word_embedding(self, word):\n",
    "        word = torch.LongTensor([word_dict[word]])\n",
    "        return self.embeddings(word).view(1, -1)\n",
    "\n",
    "# 8 Now we are ready to train our model. We first create an instance of our\n",
    "# model and define the loss function and optimizer\n",
    "model = CBOW(corpus_length, embedding_length)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.01)\n",
    "\n",
    "# 9 We then create a helper function that makes our input context words\n",
    "# gets the word indexes for each of these, and transforms them into a tensor\n",
    "# of length 4, which forms the input to our neural network:\n",
    "def make_sentence_vector(sentence, word_dict):\n",
    "    idxs = [word_dict[w] for w in sentence]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "print(make_sentence_vector(['the', 'alphago', 'project', 'is', 'part', 'of'], word_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train our network. We loop through $100$ epochs and for each pass, we loop through all our context words, that is, target word pairs. For each of these pairs, we load the context sentence using `make_sentence_vector()` and use our current model state to obtain predictions. We evaluate these predictions against our actual target in order to obtain our loss. We backpropagate to calculate the gradients and step through our optimizer to update the weights. Finally, we sum all our losses for the epoch and print this out. Here, we can see that our loss is decreasing, showing that our model is learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1148.0550537109375\n",
      "Epoch: 1, Loss: 1023.6575927734375\n",
      "Epoch: 2, Loss: 934.0814208984375\n",
      "Epoch: 3, Loss: 850.8849487304688\n",
      "Epoch: 4, Loss: 768.6152954101562\n",
      "Epoch: 5, Loss: 684.1392211914062\n",
      "Epoch: 6, Loss: 598.3729858398438\n",
      "Epoch: 7, Loss: 513.0980224609375\n",
      "Epoch: 8, Loss: 430.1632080078125\n",
      "Epoch: 9, Loss: 352.50628662109375\n",
      "Epoch: 10, Loss: 282.7103271484375\n",
      "Epoch: 11, Loss: 221.9185028076172\n",
      "Epoch: 12, Loss: 171.78469848632812\n",
      "Epoch: 13, Loss: 131.42916870117188\n",
      "Epoch: 14, Loss: 99.86688232421875\n",
      "Epoch: 15, Loss: 76.31681060791016\n",
      "Epoch: 16, Loss: 59.3114013671875\n",
      "Epoch: 17, Loss: 47.06007766723633\n",
      "Epoch: 18, Loss: 38.15354537963867\n",
      "Epoch: 19, Loss: 31.66030502319336\n",
      "Epoch: 20, Loss: 26.807104110717773\n",
      "Epoch: 21, Loss: 23.121313095092773\n",
      "Epoch: 22, Loss: 20.26970863342285\n",
      "Epoch: 23, Loss: 17.99311637878418\n",
      "Epoch: 24, Loss: 16.175214767456055\n",
      "Epoch: 25, Loss: 14.655794143676758\n",
      "Epoch: 26, Loss: 13.392498970031738\n",
      "Epoch: 27, Loss: 12.318419456481934\n",
      "Epoch: 28, Loss: 11.393003463745117\n",
      "Epoch: 29, Loss: 10.598590850830078\n",
      "Epoch: 30, Loss: 9.893559455871582\n",
      "Epoch: 31, Loss: 9.274552345275879\n",
      "Epoch: 32, Loss: 8.72702407836914\n",
      "Epoch: 33, Loss: 8.23669719696045\n",
      "Epoch: 34, Loss: 7.796017169952393\n",
      "Epoch: 35, Loss: 7.399237155914307\n",
      "Epoch: 36, Loss: 7.037268161773682\n",
      "Epoch: 37, Loss: 6.7085161209106445\n",
      "Epoch: 38, Loss: 6.408211708068848\n",
      "Epoch: 39, Loss: 6.131194591522217\n",
      "Epoch: 40, Loss: 5.876676559448242\n",
      "Epoch: 41, Loss: 5.641684532165527\n",
      "Epoch: 42, Loss: 5.423459053039551\n",
      "Epoch: 43, Loss: 5.220358848571777\n",
      "Epoch: 44, Loss: 5.031856536865234\n",
      "Epoch: 45, Loss: 4.856032848358154\n",
      "Epoch: 46, Loss: 4.69083833694458\n",
      "Epoch: 47, Loss: 4.536556243896484\n",
      "Epoch: 48, Loss: 4.391793251037598\n",
      "Epoch: 49, Loss: 4.254697799682617\n",
      "Epoch: 50, Loss: 4.126087665557861\n",
      "Epoch: 51, Loss: 4.004498481750488\n",
      "Epoch: 52, Loss: 3.8895084857940674\n",
      "Epoch: 53, Loss: 3.780693769454956\n",
      "Epoch: 54, Loss: 3.6773102283477783\n",
      "Epoch: 55, Loss: 3.579526901245117\n",
      "Epoch: 56, Loss: 3.4862093925476074\n",
      "Epoch: 57, Loss: 3.3974974155426025\n",
      "Epoch: 58, Loss: 3.312718391418457\n",
      "Epoch: 59, Loss: 3.232264518737793\n",
      "Epoch: 60, Loss: 3.154967784881592\n",
      "Epoch: 61, Loss: 3.0813894271850586\n",
      "Epoch: 62, Loss: 3.0108649730682373\n",
      "Epoch: 63, Loss: 2.943300724029541\n",
      "Epoch: 64, Loss: 2.8788156509399414\n",
      "Epoch: 65, Loss: 2.8165464401245117\n",
      "Epoch: 66, Loss: 2.7570488452911377\n",
      "Epoch: 67, Loss: 2.699831962585449\n",
      "Epoch: 68, Loss: 2.64471697807312\n",
      "Epoch: 69, Loss: 2.5919384956359863\n",
      "Epoch: 70, Loss: 2.5407297611236572\n",
      "Epoch: 71, Loss: 2.4917941093444824\n",
      "Epoch: 72, Loss: 2.4444103240966797\n",
      "Epoch: 73, Loss: 2.398907423019409\n",
      "Epoch: 74, Loss: 2.354651927947998\n",
      "Epoch: 75, Loss: 2.3118817806243896\n",
      "Epoch: 76, Loss: 2.2710065841674805\n",
      "Epoch: 77, Loss: 2.2310681343078613\n",
      "Epoch: 78, Loss: 2.1925528049468994\n",
      "Epoch: 79, Loss: 2.155318021774292\n",
      "Epoch: 80, Loss: 2.1192493438720703\n",
      "Epoch: 81, Loss: 2.0842576026916504\n",
      "Epoch: 82, Loss: 2.050360918045044\n",
      "Epoch: 83, Loss: 2.0174508094787598\n",
      "Epoch: 84, Loss: 1.9855766296386719\n",
      "Epoch: 85, Loss: 1.954614281654358\n",
      "Epoch: 86, Loss: 1.9245927333831787\n",
      "Epoch: 87, Loss: 1.8953553438186646\n",
      "Epoch: 88, Loss: 1.8670003414154053\n",
      "Epoch: 89, Loss: 1.8393540382385254\n",
      "Epoch: 90, Loss: 1.8125370740890503\n",
      "Epoch: 91, Loss: 1.786561131477356\n",
      "Epoch: 92, Loss: 1.7610360383987427\n",
      "Epoch: 93, Loss: 1.7363423109054565\n",
      "Epoch: 94, Loss: 1.7122409343719482\n",
      "Epoch: 95, Loss: 1.688857913017273\n",
      "Epoch: 96, Loss: 1.6659213304519653\n",
      "Epoch: 97, Loss: 1.6436477899551392\n",
      "Epoch: 98, Loss: 1.6219367980957031\n",
      "Epoch: 99, Loss: 1.6006876230239868\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for sentence, target in data:\n",
    "        model.zero_grad()\n",
    "        sentence_vector = make_sentence_vector(sentence, word_dict)\n",
    "        log_probs = model(sentence_vector)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_dict[target]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.data\n",
    "    print('Epoch: ' + str(epoch)+ ', Loss: ' + str(epoch_loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has been trained, we can make predictions. We define a couple of functions to allow us to do so. `get_predicted_result()` returns the predicted word from the array of predictions, while our `predict_sentence()` function makes a prediction based on the context words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. We split our sentences into individual words and transform them into and input vector. We then create our prediction array by feeding this into our model and get our final predicted word by using the `get_predicted_result()` function. We also print the $2$ words before and after the predicted target word for context. We can run a couple of predictions to validate our model is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preceding words : ['professionals']\n",
      "Predicted word : go\n",
      "Following words: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_predicted_result(input, inverse_word_dict):\n",
    "    index = np.argmax(input)\n",
    "    return inverse_word_dict[index]\n",
    "\n",
    "def predict_sentence(sentence):\n",
    "    sentence_split = sentence.replace('.', '').lower().split()\n",
    "    sentence_vector = make_sentence_vector(sentence_split, word_dict)\n",
    "    prediction_array = model(sentence_vector).data.numpy()\n",
    "    print(\"Preceding words : {}\".format(sentence_split[:2]))\n",
    "    print(\"Predicted word : {}\".format(get_predicted_result(prediction_array[0], inverse_word_dict)))\n",
    "    print(\"Following words: {}\\n\".format(sentence_split[2:]))\n",
    "\n",
    "predict_sentence(\"professionals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Now that we have a trained model, we are able to use the `get_word_embedding()` function in order to return the 20 dimensions word embedding for any word in our corpus. If we needed our embeddings for another NLP task, we could actually extract the weights from the whole embedding layer and use this in our new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1237,  1.8168, -0.7407, -1.3450, -0.6330,  0.1734,  1.6975,  0.3242,\n",
      "         -1.0633, -0.8235, -1.3310, -1.9816, -0.2267,  0.6849,  0.5219, -0.7008,\n",
      "          0.3234, -0.2631, -0.6461, -0.1032]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.get_word_embedding('professionals'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have demonstrated how to train a **CBOW** model for creating word embeddings. In reality, to create reliable embeddings for a corpus, we would require a very large dataset to be able to truly capture the semantic relationship between all the words. Because of this, it may be preferable to use pre-trained embeddings such as GLoVe, which have been trained on a very large corpus of data, for your models, but there may be cases where it would be preferable to train a brand new set of embeddings from scratch; for example, when analyzing a corpus of data that doesn't resemble normal NLP (for example, Twitter data where users may speak in short abbreviations and not use full sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring $n$-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not only our context words that influence the meaning of words in a sentence, but the order of those words as well. Consider the following sentences:\n",
    "\n",
    "***The cat sat on the dog***\n",
    "\n",
    "***The dog sat on the cat***\n",
    "\n",
    "If you were to transform these two sentences into a bag-of-words representation, we would see that they are identical (in fact, they are the complete opposite!). This clearly demonstrates that **the meaning of a sentence is not just the words it contains**, **but the order in which they occur**. One simple way of attempting to capture the order of words within a sentence is by using $n$-grams.\n",
    "\n",
    "If we perform a count on our sentences, but instead of counting individual words, we now count the distinct two-word pairings that occur within the sentences, this is known as using bi-grams\n",
    "\n",
    "![](bi_gram.png)\n",
    "\n",
    "We can represent this as follows:\n",
    "\n",
    "***The cat sat on the dog*** $ \\to [1,1,1,0,1,1]$\n",
    "\n",
    "***The dog sat on the cat*** $ \\to [1,1,0,1,1,1]$\n",
    "\n",
    "**We can use $n$-grams as inputs into our deep learning models instead of just a singular word, but when using $n$-gram models, it is worth noting that your feature space can become very large very quickly and may make machine learning very slow**. If a dictionary contains all the words in the English language, a dictionary containing all distinct pairs of words would be several orders of magnitude larger!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **$n$-gram language modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think of a language as being represented by parts of smaller word pairs (bigrams) instead of single words, we can begin to model language as a probabilistic model where the probability that a word appears in a sentence depends on the words that appeared before it.\n",
    "\n",
    "In a **unigram** model, we assume that all the words have a finite probability of appearing based on the distribution of the words in a corpus or document\n",
    "\n",
    "*My name is my name*\n",
    "\n",
    "Based on this sentence, we can generate a distribution of words whereby each word has a given probability of occurring based on its frequency within the document:\n",
    "\n",
    "![](unigram.png)\n",
    "\n",
    "We could then draw words randomly from this distribution in order to generate new sentences:\n",
    "\n",
    "*Name is Name my my*\n",
    "\n",
    "This sentence doesn't make any sense, illustrating the problems of using a unigram model. Because the probability of each word occurring is independent of all the other words in the sentence, there is no consideration given to the order or context of the words appearing. This is where n-gram models are useful.\n",
    "\n",
    "We will now consider using a **bigram** language model. This calculation takes the probability of a word occurring, given the word that appears before it:\n",
    "\n",
    "$p(W_n|W_{n-1}) = \\frac{p(W_{n-1}, W_n)}{p(W_{n-1})}$\n",
    "\n",
    "This means that **the probability of a word occurring, given the previous word, is the probability of the word $n$-gram occurring divided by the probability of the previous word occurring**. Let's say we are trying to predict the next word in the following sentence:\n",
    "\n",
    "***My favourite language is ___***\n",
    "\n",
    "Along with this, we're given the following n-gram and word probabilities:\n",
    "\n",
    "![](probabilities.png)\n",
    "\n",
    "With this, we could calculate the probability of Python occurring, given the probability of the previous word is occurring is only $20\\%$, whereas the probability of English occurring is only $10\\%$. We could expand this model further to use a trigram or any $n$-gram representation of words as we deem appropriate. We have demonstrated that $n$-gram language modeling can be used to introduce further information about word's relationships to one another into our models, rather than naively assuming that words are independently distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's way of pre-processing text for entry into our models. Tokenization splits our sentences up into smaller parts. This could involve **splitting a sentence up into its individual words** or **splitting a whole document up into individual sentences**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'single', 'sentence', '.']\n",
      "['this', 'is', 'a', 'single', 'sentence']\n",
      "['This document is the first sentence.', 'THis is the second sentence.', 'A document    contains many sentences.']\n",
      "[['This', 'document', 'is', 'the', 'first', 'sentence', '.'], ['THis', 'is', 'the', 'second', 'sentence', '.'], ['A', 'document', 'contains', 'many', 'sentences', '.']]\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n",
      "['This', 'document', 'first', 'sentence', '.', 'THis', 'second', 'sentence', '.', 'A', 'document', 'contains', 'many', 'sentences', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# 1 We first take a basic sentence and split this up\n",
    "# into individual words using the word tokenizer in NLTK\n",
    "text = 'This is a single sentence.'\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# 2 Note how a period is considered a token as it is a part of\n",
    "# natural language. Depending on what we want to do with the text\n",
    "# we may wish to keep or dispose of the punctuation\n",
    "no_punctuation = [word.lower() for word in tokens if word.isalpha()]\n",
    "print(no_punctuation)\n",
    "\n",
    "# 3 We can also tokenize documents into individuals sentences\n",
    "# using sentence_tokenizer\n",
    "text = \"This document is the first sentence. THis is the second sentence. A document\\\n",
    "    contains many sentences.\"\n",
    "\n",
    "print(sent_tokenize(text))\n",
    "\n",
    "# 4 Alternatively, we can combine the 2 split into individual\n",
    "# sentences of words\n",
    "print([word_tokenize(sentence) for sentence in sent_tokenize(text)])\n",
    "\n",
    "# 5 One other optional step in the process of tokenization, which\n",
    "# is the removal of stopwords.\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:20])\n",
    "\n",
    "# 6 Let's easily remove these stopwords from our words using basic list\n",
    "# comprehension\n",
    "tokens = [token for token in word_tokenize(text) if token not in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some NLP tasks (such as predicting the next word in the sentence) require stopwords, others (such as judging the sentiment of a film review) do not as the stopwords do not contribute much toward the overall meaning of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging and chunking for parts of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have covered several approaches for representing words and sentences, including bag-of-words, embeddings, and $n$-grams. However, **these representations fail to capture the structure of any given sentence**. Within natural language, different words can have different functions within a sentence. Consider the following:\n",
    "\n",
    "*The big dog is sleeping on the bed*\n",
    "\n",
    "We can \"tag\" the various words of this text, depending on the function of each word in the sentence. So, the preceding sentence becomes as follows:\n",
    "\n",
    "The $\\rarr$ big $\\rarr$ dog $\\rarr$ is $\\rarr$ sleeping $\\rarr$ on $\\rarr$ the $\\rarr$ bed\n",
    "\n",
    "Determiner $\\rarr$ Adjective $\\rarr$ Noun $\\rarr$ Verb $\\rarr$ Verb $\\rarr$ Preposition $\\rarr$ Determiner $\\rarr$ Noun\n",
    "\n",
    "These different parts of speech can be used to better understand the structure of sentences. For example, ***adjectives* often precede *nouns* in English**. We can use these parts of speech and their relationships to one another in our models. For example, if we are **predicting the next word in the sentence and the context word is an adjective**, **we know the probability of the next *word* being a noun is high**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tagging**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech tagging is the act of assigning these part of speech tags to the various words within the sentence. Fortunately, NTLK has a built-in tagging functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('sleeping', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('bed', 'NN')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "sentence = \"The big dog is sleeping on the bed\"\n",
    "token = word_tokenize(sentence)\n",
    "pos_tag(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decode the meaning of this tag by calling `upenn_tagset()` on the code. In this case, we can see that \"VBG\" corresponds to a verb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n"
     ]
    }
   ],
   "source": [
    "from nltk.help import upenn_tagset\n",
    "upenn_tagset(\"VBG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking expands upon our initial parts of speech tagging and aims to structure our sentences in small chunks, where each of these chunks represent a small part of speech.\n",
    "\n",
    "We may wish to split our text up into **entities**, where each entity is a separate object or thing. For example, the red book refers not to three separate entities, but to a single entity described by three words.\n",
    "\n",
    "We must first define a grammar pattern to match using regular expressions. The pattern in question looks for **noun phrases (NP)**, where a noun phrase is defined as a **determiner (DT)**, followed by an **optional adjective (JJ)**, followed by a **noun (NN)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression = ('NP: {<DT>?<JJ>*<NN>}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the RegexpParser() function, we can match occurrences of this expression and tag them as noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT big/JJ dog/NN)\n",
      "  is/VBZ\n",
      "  sleeping/VBG\n",
      "  on/IN\n",
      "  (NP the/DT bed/NN))\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "tagged = pos_tag(token)\n",
    "REchunkParser = RegexpParser(expression)\n",
    "tree = REchunkParser.parse(tagged)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is yet another technique we can learn about to better represent natural language. **It is often used in text mining and information retrieval to match documents based on search terms, but can also be used in combination with embeddings to better represent sentences in embedding form**.\n",
    "\n",
    "***This is a small giraffe***\n",
    "\n",
    "Let's say we want a single embedding to represent the meaning of this sentence. One thing we could do is simply average the individual embeddings of each of the five words in this sentence:\n",
    "\n",
    "![](words_embeddings_tfidf.png)\n",
    "\n",
    "However, this methodology assigns equal weight to all the words in the sentence. We might want to assign more weight to the rarer words (like *girage* here). This methodology is known as **Term Frequency – Inverse Document Frequency (TD-IDF)**. We will now demonstrate how we can calculate TF-IDF weightings for our documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF consists of two separate parts: term frequency and inverse document frequency. Term frequency is a document-specific measure counting the frequency of a given word within the document being analyzed:\n",
    "\n",
    "$tf(w, d) = \\frac{\\text{count of word w in document d}}{\\text{words in document d}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we divide this measure by the total number of words in the document as a longer document is more likely to contain any given word. **If a word appears many times in a document, it will receive a higher term frequency. However, this is the opposite of what we wish our TF-IDF weighting to do as we want to give a higher weight to occurrences of rare words within our document**. This is where IDF comes into play.\n",
    "\n",
    "Document frequency measures the number of documents within the entire corpus of documents where the word is being analyzed, and inverse document frequency calculates the ratio of the total documents to the document frequency:\n",
    "\n",
    "$df(w) = \\text{count of w accross all documents}$\\\n",
    "$idf(w) = \\frac{N}{df(w)}$\n",
    "\n",
    "If we have a corpus of $100$ documents and our word appears $5$ times across them, we will have an **inverse document frequency** of $20$. This means that **a higher weight is given to words with lower occurrences across all documents**.\n",
    "\n",
    "Now, consider a corpus of $100,000$ documents. If a word appears just $1$ once, it will have an **IDF of** $100,000$, whereas a word occurring $2$ twice would have an **IDF of** $50,000$.\n",
    "\n",
    "These very large and volatile IDFs aren't ideal for our calculations, **so we must first normalize them with $logs$**.\n",
    "> Note how we add $1$ within our calculations to prevent division by $0$ if we calculate TF-IDF for a word that doesn't appear in our corpus:\n",
    "\n",
    "$idf(w) = log(\\frac{N}{df(w) + 1})$\n",
    "\n",
    "This makes our final TF-IDF equation look as follows:\n",
    "\n",
    "$tfidf(w, d) = tf(w, d) *  log(\\frac{N}{df(w) + 1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementing TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024390243902439025"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 1 Import the dataset\n",
    "emma = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
    "emma_sentences = []\n",
    "emma_word_set = []\n",
    "\n",
    "for sentence in emma:\n",
    "    emma_sentences.append([word.lower() for word in sentence if word.isalpha()])\n",
    "    for word in sentence:\n",
    "        if word.isalpha():\n",
    "            emma_word_set.append(word.lower())\n",
    "\n",
    "emma_word_set = set(emma_word_set)\n",
    "\n",
    "# 2 Next, we create a function that will return our Term Frequencies\n",
    "# for a given word in our document\n",
    "def TermFreq(document, word):\n",
    "    doc_length = len(document)\n",
    "    occurances = len([w for w in document if w == word])\n",
    "    return occurances / doc_length\n",
    "\n",
    "TermFreq(emma_sentences[5], 'ago')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ago - 0.2183214869867708\n",
      "indistinct - 0.2183214869867708\n"
     ]
    }
   ],
   "source": [
    "# 3 Next, we calculate our Document Frequency\n",
    "# In order to do this efficiently, we first\n",
    "# need to pre-compute a Document Frequency\n",
    "# dictionary. This loops through all the data\n",
    "# and counts the number of documents each word\n",
    "# in our corpus appears in. We pre-compute\n",
    "# this so we that do not have to perform this\n",
    "# loop every time we wish to calculate Document\n",
    "# Frequency for a given word\n",
    "def build_DF_dict():\n",
    "    output = {}\n",
    "    for word in emma_word_set:\n",
    "        output[word] = 0\n",
    "        for doc in emma_sentences:\n",
    "            if word in doc:\n",
    "                output[word] += 1\n",
    "    return output\n",
    "\n",
    "df_dict = build_DF_dict()\n",
    "df_dict['ago']\n",
    "\n",
    "# 4 Here, we can see that the word ago appears\n",
    "# within our document 32 times. Using this\n",
    "# dictionary, we can very easily calculate our\n",
    "# Inverse Document Frequency by dividing the total\n",
    "# number of documents by our Document Frequency\n",
    "# and taking the logarithm of this value. Note\n",
    "# how we add one to the Document Frequency to\n",
    "# avoid a divide by zero error when the word\n",
    "# doesn't appear in the corpus\n",
    "def InverseDocumentFreq(words):\n",
    "    N = len(emma_sentences)\n",
    "    try:\n",
    "        df = df_dict[word] + 1\n",
    "    except:\n",
    "        df = 1\n",
    "    return np.log(N/df)\n",
    "\n",
    "InverseDocumentFreq('ago')\n",
    "\n",
    "# 5 Finally we simply combile the Term Frequency and\n",
    "# Inverse Document Frequency to get the TF-IDF\n",
    "# weighting for each word/document pair:\n",
    "def TFIDF(doc, word):\n",
    "    tf = TermFreq(doc, word)\n",
    "    idf = InverseDocumentFreq(word)\n",
    "    return tf*idf\n",
    "\n",
    "print('ago - ' + str(TFIDF(emma_sentences[5],'ago')))\n",
    "print('indistinct - ' + str(TFIDF(emma_sentences[5],'indistinct')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that although the words ago and `indistinct` appear only once in the given document, `indistinct` **occurs less frequently throughout the whole corpus, meaning it receives a higher TF-IDF weighting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating TF-IDF weighted embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.32575634e-01  3.16596488e-01 -1.80050732e-01 -3.82070951e-01\n",
      "   4.98493527e-01  5.33804805e-01 -5.46517073e-01  9.12476195e-02\n",
      "  -1.31538483e-01 -2.71967805e-02  2.99867317e-02  2.64278024e-02\n",
      "  -2.06519756e-01 -1.54796634e-01  4.28036366e-01 -5.74977317e-02\n",
      "  -2.65928778e-01  1.60373902e-02 -2.84913561e-01 -2.01252268e-01\n",
      "  -5.96390732e-02  5.72458220e-01  2.06195927e-01 -1.54312293e-01\n",
      "   2.52049805e-01 -1.64638200e+00 -3.42686049e-01  1.02592522e-01\n",
      "   1.42848000e-01 -1.09779902e-01  2.89345488e+00  7.36985634e-02\n",
      "  -3.73648780e-03 -2.76292784e-01  1.50580049e-01  9.80399951e-02\n",
      "   2.24408780e-03  2.83664024e-01  3.92979024e-02 -2.98091634e-01\n",
      "  -1.17309171e-01  2.08815776e-01  6.89953902e-03  2.92777244e-02\n",
      "   5.54180122e-02 -2.20519707e-01 -2.82007805e-01 -4.34917439e-01\n",
      "  -9.69051537e-02 -1.67569878e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 1 Import GLoVe embeddings\n",
    "def loadGlove(path):\n",
    "    file = open(path, 'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for l in file:\n",
    "        line = l.split()\n",
    "        word = line[0]\n",
    "        value = np.array([float(val) for val in line[1:]])\n",
    "        model[word]  = value\n",
    "    return model\n",
    "\n",
    "glove = loadGlove(path='./glove.6B.50d.txt')\n",
    "\n",
    "# 2 We then calculate an unweighted mean average\n",
    "# of all the individual embeddings in our document\n",
    "# to get a vector representation of the sentence\n",
    "# as a whole. We simply loop through all the words \n",
    "# n our document, extract the embedding from the\n",
    "# GLoVe dictionary, and calculate the average\n",
    "# over all these vectors:\n",
    "embeddings = []\n",
    "\n",
    "for word in emma_sentences[5]:\n",
    "    embeddings.append(glove[word])\n",
    "\n",
    "mean_embedding = np.mean(embeddings, axis = 0).reshape(1, -1)\n",
    "print(mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03384888  0.04561131 -0.02508487 -0.05546237  0.0651311   0.07019455\n",
      "  -0.06298467  0.02670422 -0.01072827 -0.00508234  0.00517652  0.00817101\n",
      "  -0.01604324 -0.01483237  0.04946372 -0.01076198 -0.05021479  0.00040191\n",
      "  -0.01920397 -0.01341318 -0.01123547  0.08492142  0.02142466 -0.01588025\n",
      "   0.04405683 -0.17856836 -0.03999452  0.01601948  0.02088402 -0.01340125\n",
      "   0.2829529   0.00694315  0.00485215 -0.02633143  0.01534283  0.01608815\n",
      "   0.00316191  0.03238881  0.0082704  -0.04192922 -0.0058766   0.01992215\n",
      "  -0.00304265 -0.00353939  0.01174628 -0.03416807 -0.02939215 -0.06798914\n",
      "  -0.00774682 -0.01807456]]\n"
     ]
    }
   ],
   "source": [
    "# 3. We repeat this process to calculate our TF-IDF\n",
    "# weighted document vector, but this time, we\n",
    "# multiply our vectors by their TF-IDF weighting\n",
    "# before we average them:\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for word in emma_sentences[5]:\n",
    "    tfidf = TFIDF(emma_sentences[5], word)\n",
    "    embeddings.append(glove[word]* tfidf)\n",
    "\n",
    "tfidf_weighted_embedding = np.mean(embeddings, axis = 0).reshape(1, -1)\n",
    "\n",
    "print(tfidf_weighted_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.986523]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 We can then compare the TF-IDF weighted\n",
    "# embedding with our average embedding to see\n",
    "# how similar they are. We can do this using\n",
    "# cosine similarity, as follows:\n",
    "cosine_similarity(mean_embedding, tfidf_weighted_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "580318c0c86e0542a7f2f9882bbbc393e6c88c07c2a75daeff3d2ea36de686a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
