{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Getting started with Pytorch 1.x for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a Python-based machine learning library. It consists of two main features: its ability to efficiently perform tensor operations with hardware acceleration (using GPUs) and its ability to build deep neural networks. PyTorch also uses dynamic computational graphs instead of static ones, which sets it apart from similar libraries such as TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, it's important that you are fully aware of the properties of a tensor. Tensors have a property known as an **order**, which essentially determines the dimensionality of a tensor. An order one tensor with a single dimension, which is equivalent to a vector or list of numbers. An order 2 tensor is a tensor with 2 dimensions, equivalent to a matrix, whereas a tensor of order 3 consists of three dimensions. There is no limit to the maximum order a tensor can have within PyTorch:\n",
    "\n",
    "![](tensors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n",
      "tensor([3., 8.])\n"
     ]
    }
   ],
   "source": [
    "# 1 To define a Tensor in PyTorch, we can do the following:\n",
    "import torch\n",
    "x = torch.tensor([1.,2.])\n",
    "print(x)\n",
    "# 2 We can perform basic operations\n",
    "# such as multiplication using standard Python operators\n",
    "x = torch.tensor([1., 2.])\n",
    "y = torch.tensor([3., 4.])\n",
    "print(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# 3 We can also select individual elements from a tensor, as follows:\n",
    "x = torch.tensor([[1., 2.],[5., 3.],[0., 4.]])\n",
    "print(x[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# 4 However, note that unlike a NumPy array, selecting\n",
    "# an individual element from a tensor object returns\n",
    "# another tensor. In order to return an individual\n",
    "# value from a tensor, you can use the .item() function:\n",
    "print(x[0][1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can check the size of any tensor by typing the following:\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is one of the main frameworks used in deep learning today. There are other widely used frameworks available too, such as TensorFlow, Theano, and Caffe. While these are very similar in many ways, there are some key differences in how they operate. These include the following:\n",
    "\n",
    "How the models are computed\n",
    "The way in which the computational graphs are compiled\n",
    "The ability to create dynamic computational graphs with variable layers\n",
    "Differences in syntax\n",
    "Arguably, the main difference between PyTorch and other frameworks is in the way that the models themselves are computed. PyTorch uses an automatic differentiation method called autograd, which allows computational graphs to be defined and executed dynamically. This is in contrast to other frameworks such as TensorFlow, which is a static framework. In these static frameworks, computational graphs must be defined and compiled before finally being executed. While using pre-compiled models may lead to efficient implementations in production, they do not offer the same level of flexibility in research and explorational projects.\n",
    "\n",
    "Frameworks such as PyTorch do not need to pre-compile computational graphs before the model can be trained. The dynamic computational graphs used by PyTorch mean that graphs are compiled as they are executed, which allows graphs to be defined on the go. The dynamic approach to model construction is particularly useful in the field of NLP. Let's consider two sentences that we wish to perform sentiment analysis on:\n",
    "\n",
    "![](pytorch%20model%20construction.png)\n",
    "\n",
    "We can represent each of these sentences as a sequence of individual word vectors, which would then form our input to our neural network. However, as we can see, each of our inputs is of a different size. Within a fixed computation graph, these varying input sizes could be a problem, but for frameworks like PyTorch, models are able to adjust dynamically to account for the variation in input structure. This is one reason why PyTorch is often preferred for NLP-related deep learning.\n",
    "\n",
    "Another major difference between PyTorch and other deep learning frameworks is syntax. PyTorch is often preferred by developers with experience in Python as it is considered to be very Pythonic in nature. PyTorch integrates well with other aspects of the Python ecosystem and it is very easy to learn if you have prior knowledge of Python. We will demonstrate PyTorch syntax now by coding up our own neural network from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple neural network in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "# 1 Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "train_labels = train['label'].values\n",
    "train = train.drop(\"label\",axis=1).values.reshape(len(train),1,28,28)\n",
    "\n",
    "# Notice that we reshaped our input to (1, 1, 28, 28),\n",
    "# which is a tensor of 1,000 images, each consisting of 28x28 pixels.\n",
    "\n",
    "# 2 Next, we convert our training data and training labels into\n",
    "# pytorch tensors so they can be fed into the neural net\n",
    "X = torch.Tensor(train.astype(float))\n",
    "y = torch.Tensor(train_labels).long()\n",
    "# Note the data types of these two tensors. A float tensor comprises\n",
    "# 32-bit floating-point numbers, while a long tensor consists of 64-bit\n",
    "# integers. Our X features must be floats in order for PyTorch to be able\n",
    "# to compute gradients, while our labels must be integers within this\n",
    "# classification model (as we're trying to predict values of 1, 2, 3,\n",
    "# and so on), so a prediction of 1.5 wouldn't make sense.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can start to construct our actual neural network classifier\n",
    "```py\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 392)\n",
    "        self.fc2 = nn.Linear(392, 196)\n",
    "        self.fc3 = nn.Linear(196, 98)\n",
    "        self.fc4 = nn.Linear(98, 10)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our classifier as if we were building a normal class in Python, inheriting from `nn.Module `in PyTorch. Within our init method, we define each of the layers of our neural network. Here, we define fully connected linear layers of varying sizes.\n",
    "\n",
    "Our first layer takes $784$ inputs as this is the size of each of our images to classify $(28\\times28)$. We then see that the output of one layer must have the same value as the input of the next one, which means our first fully connected layer outputs $392$ units and our second layer takes $392$ units as input. This is repeated for each layer, with them having half the number of units each time until we reach our final fully connected layer, which outputs $10$ units. This is the length of our classification layer.\n",
    "\n",
    "Our network now looks something like this:\n",
    "\n",
    "![](neural_net.png)\n",
    "\n",
    "Here, we can see that our final layer outputs $10$ units. This is because we wish to predict whether each image is a digit between $0$ and $9$, which is $10$ different possible classifications in total. Our output is a vector of length $10$ and contains predictions for each of the $10$ possible values of the image. When making a final classification, we take the digit classification that has the highest value as the model's final prediction. For example, for a given prediction, our model might predict the image is type 1 with a probability of $10\\%$, type 2 with a probability of $10\\%$, and type 3 with a probability of $80\\%$. We would, therefore, take type 3 as the prediction as it was predicted with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing dropout\n",
    "\n",
    "Within the init method of our MNISTClassifier class, we also define a dropout method in order to help regularize the network:\n",
    "```py\n",
    "self.dropout = nn.Dropout(p=0.2)\n",
    "```\n",
    "\n",
    "**Dropout** is a **way of regularizing our neural networks to prevent overfitting**. On each training epoch, for each node in a layer that has dropout applied, there is a probability (here, defined as $p = 20\\%$) that each node within the layer will not be used in training/backpropagation. This means that when training, our network becomes robust toward overfitting since each node will not be used in every iteration of the training process. This prevents our network from becoming too reliant on predictions from specific nodes within our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the forward pass withing our classifier\n",
    "```py\n",
    "\n",
    "def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "```\n",
    "\n",
    "The `forward()` method within our classifier is **where we apply our activation functions and define where dropout is applied within our network**. Our forward method defines the path our input will take through the network. It first takes our input, x, and reshapes it for use within the network, transforming it into a one-dimensional vector. We then pass it through our first fully connected layer and wrap it in a `ReLU` activation function to make it non-linear. We also wrap it in our dropout, as defined in our `init` method. We repeat this process for all the other layers in the network.\n",
    "\n",
    "For our final prediction layer, we wrap it in a log `softmax` layer. We will use this to easily calculate our loss function, as we will see next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the model parameters\n",
    "\n",
    "Setting the model parameters\n",
    "Next, we define our model parameters:\n",
    "```py\n",
    "model = MNISTClassifier()\n",
    "loss_function = nn.NLLLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "We initialize an instance of our `MNISTClassifier` class as a model. We also define our loss as a `Negative Log Likelihood Loss`:\n",
    "\n",
    "$Loss(y) = -log(y)$\n",
    "\n",
    "Let's assume our image is of a number $7$. If we predict class $7$ with probability $1$, our loss will be $-log(1) = 0$, but if we only predict class $7$ with probability 0.7, our loss will be $-log(0.7) = 0.3$. This means that our loss approaches infinity the further away from the correct prediction we are:\n",
    "\n",
    "![](loss.png)\n",
    "\n",
    "This is then summed over all the correct classes in our dataset to compute the total loss. Note that we defined a log softmax when building the classifier as this already applies a softmax function (restricting the predicted output to be between 0 and 1) and takes the log. This means that $log(y)$ is already calculated, so all we need to do to compute the total loss on the network is calculate the negative sum of the outputs.\n",
    "\n",
    "We will also define our optimizer as an Adam optimizer. An optimizer controls the **learning rate** within our model. The learning rate of a model defines how big the parameter updates are during each epoch of training. The larger the size of the learning rate, the larger the size of the parameter updates during gradient descent. An optimizer dynamically controls this learning rate so that when a model is initialized, the parameter updates are large. However, as the model learns and moves closer to the point where loss is minimized, the optimizer controls the learning rate, so the parameter updates become smaller and the local minimum can be located more precisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our network\n",
    "\n",
    "Finally, we can actually start training our network:\n",
    "\n",
    "1. First, create a loop that runs once for each epoch of our training. Here, we will run our training loop for 50 epochs. We first take our input tensor of images and our output tensor of labels and transform them into PyTorch variables. A variable is a PyTorch object that contains a backward() method that we can use to perform backpropagation through our network:\n",
    "\n",
    "```python\n",
    "for epoch in range(50):\n",
    "    images = Variable(X)\n",
    "    labels = Variable(y)\n",
    "\n",
    "```\n",
    "\n",
    "2. Next, we call `zero_grad()` on our optimizer to set our calculated gradients to zero. Within PyTorch, gradients are calculated cumulatively on each backpropagation. While this is useful in some models, such as when training RNNs, for our example, we wish to calculate the gradients from scratch after each epoch, so we make sure to reset the gradients to zero after each pass:\n",
    "```py\n",
    "    opt.zero_grad()\n",
    "```\n",
    "3. Next, we use our model's current state to make predictions on our dataset. This is effectively our forward pass as we then use these predictions to calculate our loss:\n",
    "```py\n",
    "    outputs = model(images)\n",
    "```\n",
    "\n",
    "4. Using the outputs and the true labels of our dataset, we calculate the total loss of our model using the defined loss function, which in this case is the negative log likelihood. On calculating this loss, we can then make a backward() call to backpropagate our loss through the network. We then use step() using our optimizer in order to update our model parameters accordingly:\n",
    "```py\n",
    "    loss = loss_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "```\n",
    "\n",
    "5. Finally, after each epoch is complete, we print the total loss. We can observe this to make sure our model is learning:\n",
    "```py\n",
    "print ('Epoch [%d/%d] Loss: %.4f' %(epoch+1, 50, loss.data.item()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has been trained, we can use this to make predictions on unseen data. We begin by reading in our test set of data (which was not used to train our model):\n",
    "```py\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_labels = test['label'].values\n",
    "test = test.drop(\"label\",axis=1).values.reshape(len(test), 128,28)\n",
    "X_test = torch.Tensor(test.astype(float))\n",
    "y_test = torch.Tensor(test_labels).long()\n",
    "```\n",
    "Here, we perform the same steps we performed when we loaded our training set of data: we reshape our test data and transform it into PyTorch tensors. Next, to predict using our trained model, we simply run the following command:\n",
    "\n",
    "```py\n",
    "preds = model(X_test)\n",
    "```\n",
    "\n",
    "In the same way that we calculated our outputs on the forward pass of our training data in our model, we now pass our test data through the model and obtain predictions. We can view the predictions for one of the images like so:\n",
    "\n",
    "```py\n",
    "print(preds[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that our prediction is a vector of length 10, with a prediction for each of the possible classes (digits between 0 and 9). The one with the highest predicted value is the one our model chooses as its prediction. In this case, it is the 10th unit of our vector, which equates to the digit 9. Note that since we used log softmax earlier, our predictions are logs and not raw probabilities. To convert these back into probabilities, we can just transform them using x.\n",
    "\n",
    "We can now construct a summary DataFrame containing our true test data labels, as well as the labels our model predicted:\n",
    "\n",
    "```python\n",
    "_, predictionlabel = torch.max(preds.data, 1)\n",
    "predictionlabel = predictionlabel.tolist()\n",
    "predictionlabel = pd.Series(predictionlabel)\n",
    "test_labels = pd.Series(test_labels)\n",
    "pred_table = pd.concat([predictionlabel, test_labels], axis=1)\n",
    "pred_table.columns =['Predicted Value', 'True Value']\n",
    "display(pred_table.head())\n",
    "```\n",
    "> Note how the `torch.max()` function automatically selects the prediction with the highest value. We can see here that, based on a small selection of our data, our model appears to be making some good predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some predictions from our model, we can use these predictions to evaluate how good our model is. One rudimentary way of evaluating model performance is accuracy, as discussed in the previous chapter. Here, we simply calculate our correct predictions (where the predicted image label is equal to the actual image label) as a percentage of the total number of predictions our model made:\n",
    "\n",
    "```py\n",
    "preds = len(predictionlabel)\n",
    "correct = len([1 for x,y in zip(predictionlabel, test_labels) if x==y])\n",
    "print((correct/preds)*100)\n",
    "```\n",
    "This results in the following output: $89.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Your first neural network was able to correctly identify almost 90% of unseen digit images. As we progress, we will see that there are more sophisticated models that may lead to improved performance. However, for now, we have demonstrated that creating a simple deep neural network is very simple using PyTorch. This can be coded up in just a few lines and leads to performance above and beyond what is possible with basic machine learning models such as regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Loss: 8.2165\n",
      "Epoch [2/100] Loss: 6.2187\n",
      "Epoch [3/100] Loss: 4.2620\n",
      "Epoch [4/100] Loss: 3.4075\n",
      "Epoch [5/100] Loss: 2.2780\n",
      "Epoch [6/100] Loss: 1.9502\n",
      "Epoch [7/100] Loss: 1.6316\n",
      "Epoch [8/100] Loss: 1.4636\n",
      "Epoch [9/100] Loss: 1.2973\n",
      "Epoch [10/100] Loss: 1.2291\n",
      "Epoch [11/100] Loss: 1.0734\n",
      "Epoch [12/100] Loss: 1.0030\n",
      "Epoch [13/100] Loss: 0.8864\n",
      "Epoch [14/100] Loss: 0.8959\n",
      "Epoch [15/100] Loss: 0.8175\n",
      "Epoch [16/100] Loss: 0.7573\n",
      "Epoch [17/100] Loss: 0.7174\n",
      "Epoch [18/100] Loss: 0.6808\n",
      "Epoch [19/100] Loss: 0.5734\n",
      "Epoch [20/100] Loss: 0.5536\n",
      "Epoch [21/100] Loss: 0.5247\n",
      "Epoch [22/100] Loss: 0.5284\n",
      "Epoch [23/100] Loss: 0.4776\n",
      "Epoch [24/100] Loss: 0.4149\n",
      "Epoch [25/100] Loss: 0.4050\n",
      "Epoch [26/100] Loss: 0.4315\n",
      "Epoch [27/100] Loss: 0.3674\n",
      "Epoch [28/100] Loss: 0.3520\n",
      "Epoch [29/100] Loss: 0.3183\n",
      "Epoch [30/100] Loss: 0.2830\n",
      "Epoch [31/100] Loss: 0.2981\n",
      "Epoch [32/100] Loss: 0.2431\n",
      "Epoch [33/100] Loss: 0.2448\n",
      "Epoch [34/100] Loss: 0.2557\n",
      "Epoch [35/100] Loss: 0.2427\n",
      "Epoch [36/100] Loss: 0.2244\n",
      "Epoch [37/100] Loss: 0.1975\n",
      "Epoch [38/100] Loss: 0.1892\n",
      "Epoch [39/100] Loss: 0.1815\n",
      "Epoch [40/100] Loss: 0.1717\n",
      "Epoch [41/100] Loss: 0.1598\n",
      "Epoch [42/100] Loss: 0.1602\n",
      "Epoch [43/100] Loss: 0.1299\n",
      "Epoch [44/100] Loss: 0.1383\n",
      "Epoch [45/100] Loss: 0.1362\n",
      "Epoch [46/100] Loss: 0.1352\n",
      "Epoch [47/100] Loss: 0.1168\n",
      "Epoch [48/100] Loss: 0.1249\n",
      "Epoch [49/100] Loss: 0.1016\n",
      "Epoch [50/100] Loss: 0.1068\n",
      "Epoch [51/100] Loss: 0.1041\n",
      "Epoch [52/100] Loss: 0.0829\n",
      "Epoch [53/100] Loss: 0.0746\n",
      "Epoch [54/100] Loss: 0.0862\n",
      "Epoch [55/100] Loss: 0.0752\n",
      "Epoch [56/100] Loss: 0.0679\n",
      "Epoch [57/100] Loss: 0.0667\n",
      "Epoch [58/100] Loss: 0.0598\n",
      "Epoch [59/100] Loss: 0.0776\n",
      "Epoch [60/100] Loss: 0.0677\n",
      "Epoch [61/100] Loss: 0.0621\n",
      "Epoch [62/100] Loss: 0.0680\n",
      "Epoch [63/100] Loss: 0.0471\n",
      "Epoch [64/100] Loss: 0.0506\n",
      "Epoch [65/100] Loss: 0.0467\n",
      "Epoch [66/100] Loss: 0.0599\n",
      "Epoch [67/100] Loss: 0.0475\n",
      "Epoch [68/100] Loss: 0.0522\n",
      "Epoch [69/100] Loss: 0.0476\n",
      "Epoch [70/100] Loss: 0.0386\n",
      "Epoch [71/100] Loss: 0.0423\n",
      "Epoch [72/100] Loss: 0.0416\n",
      "Epoch [73/100] Loss: 0.0399\n",
      "Epoch [74/100] Loss: 0.0383\n",
      "Epoch [75/100] Loss: 0.0299\n",
      "Epoch [76/100] Loss: 0.0325\n",
      "Epoch [77/100] Loss: 0.0272\n",
      "Epoch [78/100] Loss: 0.0288\n",
      "Epoch [79/100] Loss: 0.0346\n",
      "Epoch [80/100] Loss: 0.0302\n",
      "Epoch [81/100] Loss: 0.0343\n",
      "Epoch [82/100] Loss: 0.0325\n",
      "Epoch [83/100] Loss: 0.0248\n",
      "Epoch [84/100] Loss: 0.0235\n",
      "Epoch [85/100] Loss: 0.0218\n",
      "Epoch [86/100] Loss: 0.0359\n",
      "Epoch [87/100] Loss: 0.0222\n",
      "Epoch [88/100] Loss: 0.0318\n",
      "Epoch [89/100] Loss: 0.0190\n",
      "Epoch [90/100] Loss: 0.0190\n",
      "Epoch [91/100] Loss: 0.0181\n",
      "Epoch [92/100] Loss: 0.0222\n",
      "Epoch [93/100] Loss: 0.0151\n",
      "Epoch [94/100] Loss: 0.0166\n",
      "Epoch [95/100] Loss: 0.0188\n",
      "Epoch [96/100] Loss: 0.0134\n",
      "Epoch [97/100] Loss: 0.0121\n",
      "Epoch [98/100] Loss: 0.0197\n",
      "Epoch [99/100] Loss: 0.0285\n",
      "Epoch [100/100] Loss: 0.0116\n",
      "tensor([-1.4314e+01, -2.1727e+01, -1.7158e+01, -2.1550e+01, -1.8464e+01,\n",
      "        -2.1339e+01, -3.2871e+01, -8.7693e+00, -1.5636e+01, -1.5615e-04],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Value</th>\n",
       "      <th>True Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted Value  True Value\n",
       "0                9           9\n",
       "1                8           5\n",
       "2                6           2\n",
       "3                4           4\n",
       "4                1           1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 1 Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "train_labels = train['label'].values\n",
    "train = train.drop(\"label\",axis=1).values.reshape(len(train),1,28,28)\n",
    "\n",
    "# Notice that we reshaped our input to (1, 1, 28, 28),\n",
    "# which is a tensor of 1,000 images, each consisting of 28x28 pixels.\n",
    "\n",
    "# 2 Next, we convert our training data and training labels into\n",
    "# pytorch tensors so they can be fed into the neural net\n",
    "X = torch.Tensor(train.astype(float))\n",
    "y = torch.Tensor(train_labels).long()\n",
    "# Note the data types of these two tensors. A float tensor comprises\n",
    "# 32-bit floating-point numbers, while a long tensor consists of 64-bit\n",
    "# integers. Our X features must be floats in order for PyTorch to be able\n",
    "# to compute gradients, while our labels must be integers within this\n",
    "# classification model (as we're trying to predict values of 1, 2, 3,\n",
    "# and so on), so a prediction of 1.5 wouldn't make sense.\n",
    "\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 392)\n",
    "        self.fc2 = nn.Linear(392, 196)\n",
    "        self.fc3 = nn.Linear(196, 98)\n",
    "        self.fc4 = nn.Linear(98, 10)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Setting model parameters\n",
    "\n",
    "model = MNISTClassifier()\n",
    "loss_function = nn.NLLLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training the network\n",
    "\n",
    "for epoch in range(100):\n",
    "    images = Variable(X)\n",
    "    labels = Variable(y)\n",
    "    opt.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = loss_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print ('Epoch [%d/%d] Loss: %.4f' %(epoch+1, 100, loss.data.item()))\n",
    "\n",
    "# Making predictions\n",
    "\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "test_labels = test['label'].values\n",
    "test = test.drop(\"label\",axis=1).values.reshape(len(test), 1, 28, 28)\n",
    "X_test = torch.Tensor(test.astype(float))\n",
    "y_test = torch.Tensor(test_labels).long()\n",
    "\n",
    "\n",
    "preds = model(X_test)\n",
    "print(preds[0])\n",
    "\n",
    "_, predictionlabel = torch.max(preds.data, 1)\n",
    "predictionlabel = predictionlabel.tolist()\n",
    "predictionlabel = pd.Series(predictionlabel)\n",
    "test_labels = pd.Series(test_labels)\n",
    "pred_table = pd.concat([predictionlabel, test_labels], axis=1)\n",
    "pred_table.columns =['Predicted Value', 'True Value']\n",
    "display(pred_table.head())\n",
    "\n",
    "\n",
    "# Evaluating our model\n",
    "preds = len(predictionlabel)\n",
    "correct = len([1 for x,y in zip(predictionlabel, test_labels) if x==y])\n",
    "print((correct/preds)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP for Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create *a basic bag-of-words classifier in order to classify the language of a given sentence*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this example, we'll take a selection of sentences in Spanish and english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split each sentence into a list of words and take the language of each sentence as a label. We take a section of sentences to train our model on and keep a small section to one side as our test set. We do this so that we can evaluate the performance of our model after it has been trained:\n",
    "```python\n",
    "(\"This is my favourite chapter\".lower().split(), \"English\"),\n",
    "(\"Estoy en la biblioteca\".lower().split(), \"Spanish\")\n",
    "```\n",
    "> Note that we also transform each word into lowercase, which stops words being double counted in our bag-of-words. If we have the word **book** and the word **Book**, we want these to be counted as the same word, so we transform these into lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build our word index, which is simply a dictionary of all the words in our corpus, and then create a unique index value for each word. This can be easily done with a short `for` loop\n",
    "\n",
    "```python\n",
    "word_dict = {}\n",
    "i = 0\n",
    "for words, language in training_data + test_data:\n",
    "    for word in words:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = i\n",
    "            i+=1\n",
    "print(word_dict)\n",
    "```\n",
    "> Note that here, we looped through all our training data and test data. If we just created our word index on training data, when it came to evaluating our test set, we would have new words that were not seen in the original training, so we wouldn't be able to create a true bag-of-words representation for these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now we build our classifier in a similar fashion to how we build our neural netword in the previous section.\n",
    "\n",
    "Here, we define our classifier so that it consists of a single linear layer with log softmax activation functions approximating a logistic regression. We could easily extend this to operate as a neural network by adding extra linear layers here, but a single layer of parameters will serve our purpose. Pay close attention to the input and output sizes of our linear layer:\n",
    "\n",
    "```python\n",
    "corpus_size = len(word_dict)\n",
    "languages = 2\n",
    "label_index = {\"Spanish\": 0, \"English\": 1}\n",
    "\n",
    "class BagofWordsClassifier(nn.Module):  \n",
    "    def __init__(self, languages, corpus_size):\n",
    "        super(BagofWordsClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(corpus_size, languages)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "```\n",
    "**The input is of length**`corpus_size`, which is just the total count of unique words in our corpus. This is because each input to our model will be a bag-of-words representation, consisting of the counts of words in each sentence, with a count of 0 if a given word does not appear in our sentence. Our output is of size 2, which is our number of languages to predict. Our final predictions will consist of a probability that our sentence is English versus the probability that our sentence is Spanish, with our final prediction being the one with the highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Next, we define some utility functions. We first define make_bow_vector, which takes the sentence and transforms it into a bag-of-words representation. We first create a vector consisting of all zeros. We then loop through them and for each word in the sentence, we increment the count of that index within the bag-of-words vector by one. We finally reshape this vector using with .view() for entry into our classifier:\n",
    "\n",
    "```python\n",
    "def make_bow_vector(sentence, word_index):\n",
    "    word_vec = torch.zeros(corpus_size)\n",
    "    for word in sentence:\n",
    "        word_vec[word_dict[word]] += 1\n",
    "    return word_vec.view(1, -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Similarly, we define `make_target`, which simply takes the label of the sentence (Spanish or English) and returns its relevant index (0 or 1):\n",
    "\n",
    "```python\n",
    "def make_target(label, label_index):\n",
    "    return torch.LongTensor([label_index[label]])\n",
    "```\n",
    "\n",
    "6. We can now create an instance of our model, ready for training. We also define our loss function as Negative Log Likelihood as we are using a log softmax function, and then define our optimizer in order to use standard stochastic gradient descent (SGD):\n",
    "\n",
    "```py\n",
    "model = BagofWordsClassifier(languages, corpus_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "```\n",
    "\n",
    "Now, we are ready to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We evaluate our model on a couple of sentences from our test data that our model was not trained on. Here, we first set torch.no_grad(), which deactivates the autograd engine as there is no longer any need to calculate gradients as we are no longer training our model. Next, we take our test sentence and transform it into a bag-of-words vector and feed it into our model to obtain predictions.\n",
    "\n",
    "```py\n",
    "for epoch in range(100):\n",
    "    for sentence, label in training_data:\n",
    "        model.zero_grad()\n",
    "        bow_vec = make_bow_vector(sentence, word_dict)\n",
    "        target = make_target(label, label_index)\n",
    "        log_probs = model(bow_vec)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: ',str(epoch+1),', Loss: ' + str(loss.item()))\n",
    "```\n",
    "\n",
    "2. We then simply print the sentence, the true label of the sentence, and then the predicted probabilities. Note that we transform the predicted values from log probabilities back into probabilities. We obtain two probabilities for each prediction, but if we refer back to the label index, we can see that the first probability (index 0) corresponds to Spanish, whereas the other one corresponds to English\n",
    "\n",
    "```python\n",
    "def make_predictions(data):\n",
    "    with torch.no_grad():\n",
    "        sentence = data[0]\n",
    "        label = data[1]\n",
    "        bow_vec = make_bow_vector(sentence, word_dict)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(sentence)\n",
    "        print(label + ':')\n",
    "        print(np.exp(log_probs))\n",
    "\n",
    "make_predictions(test_data[0])\n",
    "make_predictions(test_data[1])\n",
    "```\n",
    "\n",
    "Here, we can see that for both our predictions, our model predicts the correct answer, but why is this? What exactly has our model learned? We can see that our first test sentence contains the word estoy, which was previously seen in a Spanish sentence within our training set. Similarly, we can see that the word book was seen within our training set in an English sentence. Since our model consists of a single layer, the parameters on each of our nodes are easy to interpret.\n",
    "\n",
    "3. Here, we define a function that takes a word as input and returns the weights on each of the parameters within the layer. For a given word, we get the index of this word from our dictionary and then select these parameters from the same index within the model. Note that our model returns two parameters as we are making two predictions; that is, the model's contribution to the Spanish prediction and the model's contribution to the English prediction:\n",
    "\n",
    "```py\n",
    "def return_params(word):\n",
    "    index = word_dict[word]\n",
    "    for p in model.parameters():\n",
    "        dims = len(p.size())\n",
    "        if dims == 2:\n",
    "            print(word + ':')\n",
    "            print('Spanish Parameter = ' + str(p[0][index].item()))\n",
    "            print('English Parameter = ' + str(p[1][index].item()))\n",
    "            print('\\n')\n",
    "\n",
    "return_params('estoy')\n",
    "return_params('book')\n",
    "```\n",
    "\n",
    "We can show that our model has only learned based on what it has been trained on. If we try to predict a word the model hasn't been trained on, we can see it is unable to make an accurate decision. In this case, our model thinks that the English word \"not\" is Spanish:\n",
    "\n",
    "```py\n",
    "new_sentence = ([\"not\"],\"English\")\n",
    "make_predictions(new_sentence)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 , Loss: 1.0719650983810425\n",
      "Epoch:  11 , Loss: 0.0\n",
      "Epoch:  21 , Loss: 0.0\n",
      "Epoch:  31 , Loss: 0.0\n",
      "Epoch:  41 , Loss: 0.0\n",
      "Epoch:  51 , Loss: 0.0\n",
      "Epoch:  61 , Loss: 0.0\n",
      "Epoch:  71 , Loss: 0.0\n",
      "Epoch:  81 , Loss: 0.0\n",
      "Epoch:  91 , Loss: 0.0\n",
      "----------------MAKING PREDICTION-------------\n",
      "\n",
      "['in', 'the', 'case', 'of', 'alphago,', 'the', 'slow', 'mode', 'of', 'thinking', 'is', 'carried', 'out', 'by', 'a', 'planning', 'algorithm', 'called', 'monte', 'carlo', 'tree', 'search,', 'which', 'plans', 'from', 'a', 'given', 'position', 'by', 'expanding', 'the', 'game', 'tree', 'that', 'represents', 'possible', 'future', 'moves', 'and', 'counter', 'moves.', 'but', 'with', 'roughly', '(1', 'followed', 'by', '170', '0s)', 'many', 'possible', 'go', 'positions,', 'searching', 'through', 'every', 'sequence', 'of', 'a', 'game', 'proves', 'impossible.', 'to', 'get', 'around', 'this', 'and', 'to', 'reduce', 'the', 'size', 'of', 'the', 'search', 'space,', 'we', 'paired', 'the', 'monte', 'carlo', 'tree', 'search', 'with', 'a', 'deep', 'learning']\n",
      "English:\n",
      "tensor([[2.9544e-09, 1.0000e+00]])\n",
      "['quelle', 'licence', 'correspond', 'à', 'vos', 'souhaits', 'et/ou', 'contraintes', 'de', 'partage', '?', 'etc.', 'mise', 'à', 'jour', 'lors', 'de', 'la', 'production', 'd’un', 'nouveau', 'jeu', 'de', 'données,', 'lors', 'du', 'dépôt', 'd’une', 'demande', 'de', 'brevet,', 'etc.', 'le', 'dmp', 'est', 'un', 'document', 'évolutif.', 'des', 'mises', 'à', 'jour', 'et', 'des', 'livrables', 'précis', 'peuvent', 'être', 'définis', 'selon', 'les', 'financeurs', 'et/ou', 'projets.', 'pour', 'respecter', 'les', 'échéances']\n",
      "French:\n",
      "tensor([[1.0000e+00, 7.8730e-09]])\n",
      "----------------END PREDICTION----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "french_text = \"\"\"\n",
    "Le Data Management Plan ou Plan de gestion de données est un document synthétique qui aide à organiser et\n",
    "anticiper toutes les étapes du cycle de vie de la donnée. Il explique pour chaque jeu de données comment seront\n",
    "gérées les données d’un projet, depuis leur création ou collecte jusqu’à leur partage et leur archivage.\n",
    "Une bonne pratique de recherche\n",
    "La réalisation d’un DMP\n",
    "Suivant un calendrier\n",
    "Exemples\n",
    "Au moyen d’outils dédiés\n",
    "De plus en plus utilisée\n",
    "Exemples\n",
    "Orientée libre accès\n",
    "Un DMP peut être établi aussi bien dans une optique de partage des données que pour des données en accès\n",
    "restreint ou fermé, total ou partiel. Le DMP mentionnera dans ce cas les raisons de non partage.\n",
    "Le contexte politique va en faveur de l’Open Data.\n",
    "Recommandé par les financeurs, le DMP devient l’outil\n",
    "de gestion incontournable des projets de recherche.\n",
    "Pour optimiser la gestion de vos données\n",
    "L’Europe, les Etats-Unis ou encore l’Australie\n",
    "ont adopté le DMP. Les projets rentrant dans\n",
    "le pilote du programme Horizon 2020 doivent\n",
    "obligatoirement en établir un.\n",
    "Le DMP est très lié au principe du libre accès aux\n",
    "données de recherche. En fonction de votre choix et de\n",
    "vos contraintes en matière de partage, des critères\n",
    "sont à définir.\n",
    "Pour vous poser les bonnes questions\n",
    "Quelles métadonnées renseigner ? Faut-il des\n",
    "outils spécifiques pour accéder aux données ?\n",
    "\"\"\"\n",
    "\n",
    "fr_test_text = \"\"\"\n",
    "Quelle licence correspond à vos souhaits\n",
    "et/ou contraintes de partage ? Etc. Mise à jour lors de\n",
    "la production d’un nouveau jeu de données, lors du dépôt d’une demande\n",
    "de brevet, etc. Le DMP est un document évolutif. Des mises à jour et\n",
    "des livrables précis peuvent être définis selon les\n",
    "financeurs et/ou projets. Pour respecter les échéances\n",
    "\"\"\"\n",
    "\n",
    "english_text = \"\"\"\n",
    "For us, the members of the AlphaGo team, the AlphaGo story was the adventure of a\n",
    "lifetime. It began, as many great adventures do, with a small step—training a simple\n",
    "convolutional neural network on records of Go games played by strong human play-\n",
    "ers. This led to pivotal breakthroughs in the recent development of machine learning,\n",
    "as well as a series of unforgettable events, including matches against the formidable\n",
    "Go professionals Fan Hui, Lee Sedol, and Ke Jie. We’re proud to see the lasting\n",
    "impact of these matches on the way Go is played around the world, as well as their role\n",
    "in making more people aware of, and interested in, the field of artificial intelligence.\n",
    "But why, you might ask, should we care about games? Just as children use games to\n",
    "learn about aspects of the real world, so researchers in machine learning use them to\n",
    "train artificial software agents. In this vein, the AlphaGo project is part of DeepMind’s\n",
    "strategy to use games as simulated microcosms of the real world. This helps us study\n",
    "artificial intelligence and train learning agents with the goal of one day building gen-\n",
    "eral purpose learning systems capable of solving the world’s most complex problems.\n",
    "AlphaGo works in a way that is similar to the two modes of thinking that Nobel\n",
    "laureate Daniel Kahnemann describes in his book on human cognition, Thinking Fast\n",
    "and Slow.\n",
    "\"\"\"\n",
    "\n",
    "eng_test_text = \"\"\"\n",
    "In the case of AlphaGo, the slow mode of thinking is carried out by a planning\n",
    "algorithm called Monte Carlo Tree Search, which plans from a given position by expanding\n",
    "the game tree that represents possible future moves and counter moves. But with\n",
    "roughly (1 followed by 170 0s) many possible Go positions, searching through\n",
    "every sequence of a game proves impossible. To get around this and to reduce the size\n",
    "of the search space, we paired the Monte Carlo Tree Search with a deep learning\n",
    "\"\"\"\n",
    "\n",
    "### Setting up the classifier\n",
    "# 1 Splitting sentences\n",
    "training_data = [(english_text.lower().split(), \"English\"), (french_text.lower().split(), \"French\")]\n",
    "test_data = [(eng_test_text.lower().split(), \"English\"), (fr_test_text.lower().split(), \"French\")]\n",
    "\n",
    "\n",
    "# 2 Build our word index which is simply a dictionary of all the words\n",
    "# in our corpus\n",
    "word_dict = {}\n",
    "i = 0\n",
    "for words, language in training_data + test_data:\n",
    "    for word in words:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = i\n",
    "            i+=1\n",
    "# print(word_dict)\n",
    "\n",
    "# 3 Build our classifier\n",
    "corpus_size = len(word_dict)\n",
    "languages = 2\n",
    "label_index = {\"French\": 0, \"English\": 1}\n",
    "\n",
    "class BagofWordsClassifier(nn.Module):\n",
    "    def __init__(self, languages, corpus_size):\n",
    "        super(BagofWordsClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(corpus_size, languages)\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "\n",
    "# 4 we define some utility functions. We first define\n",
    "# make_bow_vector, which takes the sentence and transforms\n",
    "# it into a bag-of-words representation\n",
    "\n",
    "def make_bow_vector(sentence, word_index):\n",
    "    word_vec = torch.zeros(corpus_size)\n",
    "    for word in sentence:\n",
    "        word_vec[word_dict[word]] += 1\n",
    "    return word_vec.view(1, -1)\n",
    "# 5 Similary we define make_target, which\n",
    "# simply takes the label of the sentence\n",
    "# (Spanish or English) and returns its relevant index (0 or 1):\n",
    "def make_target(label, label_index):\n",
    "    return torch.LongTensor([label_index[label]])\n",
    "\n",
    "# 6 Create an instance of our model\n",
    "model = BagofWordsClassifier(languages, corpus_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "### Training the classifier\n",
    "for epoch in range(100):\n",
    "    for sentence, label in training_data:\n",
    "        model.zero_grad()\n",
    "        bow_vec = make_bow_vector(sentence, word_dict)\n",
    "        target = make_target(label, label_index)\n",
    "        log_probs = model(bow_vec)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: ',str(epoch+1),', Loss: ' + str(loss.item()))\n",
    "\n",
    "def make_predictions(data):\n",
    "    with torch.no_grad():\n",
    "        sentence = data[0]\n",
    "        label = data[1]\n",
    "        bow_vec = make_bow_vector(sentence, word_dict)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(sentence)\n",
    "        print(label+':')\n",
    "        print(np.exp(log_probs))\n",
    "print(\"----------------MAKING PREDICTION-------------\\n\")\n",
    "make_predictions(test_data[0])\n",
    "make_predictions(test_data[1])\n",
    "\n",
    "print(\"----------------END PREDICTION----------------\\n\")\n",
    "def return_params(word):\n",
    "    index = word_dict[word]\n",
    "    for p in model.parameters():\n",
    "        dims = len(p.size())\n",
    "        if dims == 2:\n",
    "            print(word + ':')\n",
    "            print('French Parameter = ' + str(p[0][index].item()))\n",
    "            print('English Parameter = ' + str(p[1][index].item()))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm:\n",
      "French Parameter = 0.023485127836465836\n",
      "English Parameter = -0.048045434057712555\n",
      "\n",
      "\n",
      "nouveau:\n",
      "French Parameter = 0.022577587515115738\n",
      "English Parameter = 0.02954227104783058\n",
      "\n",
      "\n",
      "['around']\n",
      "English:\n",
      "tensor([[0.4943, 0.5057]])\n"
     ]
    }
   ],
   "source": [
    "return_params('algorithm')\n",
    "return_params('nouveau')\n",
    "\n",
    "new_sentence = ([\"around\"],\"English\")\n",
    "make_predictions(new_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "580318c0c86e0542a7f2f9882bbbc393e6c88c07c2a75daeff3d2ea36de686a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
