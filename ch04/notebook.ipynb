{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 : Text Preprocessing, Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike tokenization, which reduces a document into individual words, s**temming and lemmatization are attempts to reduce these words further to their lexical roots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing HTML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, in Python, there is a package called `BeautifulSoup` that allows us to remove all HTML in a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <b> This text is in bold</br>, <i> This text is in italics </i>\n",
      "Output:  This text is in bold,  This text is in italics \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "input_text = \"<b> This text is in bold</br>, <i> This text is in italics </i>\"\n",
    "output_text = BeautifulSoup(input_text, \"html.parser\").get_text()\n",
    "print('Input: ' + input_text)\n",
    "print('Output: ' + output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Converting text into lowercase**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done very easily within Python using the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['Cat', 'cat', 'CAT']\n",
      "Output: ['cat', 'cat', 'cat']\n"
     ]
    }
   ],
   "source": [
    "input_text = ['Cat','cat','CAT']\n",
    "output_text = [x.lower() for x in input_text]\n",
    "print('Input: ' + str(input_text))\n",
    "print('Output: ' + str(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing punctuation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, depending on the type of model being constructed, we may wish to remove punctuation from our input text. This is particularly useful in models where we are aggregating word counts, such as in a bag-of-words representation. The presence of a full stop or a comma within the sentence doesn't add any useful information about the semantic content of the sentence. However, more complicated models that take into account the position of punctuation within the sentence may actually use the position of the punctuation to infer a different meaning. A classic example is as follows:\n",
    "\n",
    "*The panda eats shoots and leaves*\n",
    "\n",
    "*The panda eats, shoots, and leaves*\n",
    "\n",
    "Here, the addition of a comma transforms the sentence describing a panda's eating habits into a sentence describing an armed robbery of a restaurant by a panda!\n",
    "\n",
    "We can do this in Python by using the `re` library, to match any punctuation using a regular expression, and the `sub()` method, to replace any matched punctuation with an empty character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: This ,sentence.'' contains-£ no:: punctuation?\n",
      "Output: This sentence contains no punctuation\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "input_text = \"This ,sentence.'' contains-£ no:: punctuation?\"\n",
    "output_text = re.sub(r'[^\\w\\s]', '', input_text)\n",
    "print('Input: ' + input_text)\n",
    "print('Output: ' + output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be instances where we may not wish to directly remove punctuation. A good example would be the use of the ampersand (&), which in almost every instance is used interchangeably with the word \"and\". Therefore, rather than completely removing the ampersand, we may instead opt to replace it directly with the word \"and\". We can easily implement this in Python using the `.replace()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Cats & dogs\n",
      "Output: Cats and dogs\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Cats & dogs\"\n",
    "output_text = input_text.replace(\"&\", \"and\")\n",
    "print('Input: ' + input_text)\n",
    "print('Output: ' + output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also worth considering specific circumstances where punctuation may be essential for the representation of a sentence. One crucial example is email addresses. Removing the @ from email addresses doesn't make the address any more readable:\n",
    "\n",
    "name@gmail.com\n",
    "\n",
    "Removing the punctuation returns this:\n",
    "\n",
    "namegmailcom\n",
    "\n",
    "So, in instances like this, it may be preferable to remove the whole item altogether, according to the requirements and purpose of your NLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Replacing numbers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, with numbers, we also want to standardize our outputs. Numbers can be written as digits $(9, 8, 7)$ or as actual words *(nine, eight, seven)*. It may be worth transforming these all into a single, standardized representation so that $1$ and *one* are not treated as separate entities. We can do this in Python using the following methodology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`Field` default cannot be set in `Annotated` for 'num_Annotated[str, FieldInfo(min_length=1, extra={})]'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ibrao\\OneDrive\\Documents\\Deep Learning\\NLP with Pytorch 1.x\\ch04\\notebook.ipynb Cellule 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch04/notebook.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39minflect\u001b[39;00m \u001b[39mimport\u001b[39;00m engine\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch04/notebook.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_digit\u001b[39m(digit):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch04/notebook.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     i \u001b[39m=\u001b[39m engine()\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\inflect\\__init__.py:2046\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   2042\u001b[0m Word \u001b[39m=\u001b[39m Annotated[\u001b[39mstr\u001b[39m, Field(min_length\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\n\u001b[0;32m   2043\u001b[0m Falsish \u001b[39m=\u001b[39m Any  \u001b[39m# ideally, falsish would only validate on bool(value) is False\u001b[39;00m\n\u001b[1;32m-> 2046\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mengine\u001b[39;00m:\n\u001b[0;32m   2047\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2049\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassical_dict \u001b[39m=\u001b[39m def_classical\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\inflect\\__init__.py:3781\u001b[0m, in \u001b[0;36mengine\u001b[1;34m()\u001b[0m\n\u001b[0;32m   3777\u001b[0m         num \u001b[39m=\u001b[39m ONE_DIGIT_WORD\u001b[39m.\u001b[39msub(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munitsub, num, \u001b[39m1\u001b[39m)\n\u001b[0;32m   3778\u001b[0m     \u001b[39mreturn\u001b[39;00m num\n\u001b[0;32m   3780\u001b[0m \u001b[39m@validate_arguments\u001b[39;49m(config\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(arbitrary_types_allowed\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))  \u001b[39m# noqa: C901\u001b[39;49;00m\n\u001b[1;32m-> 3781\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mnumber_to_words\u001b[39;49m(  \u001b[39m# noqa: C901\u001b[39;49;00m\n\u001b[0;32m   3782\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   3783\u001b[0m     num: Union[Number, Word],\n\u001b[0;32m   3784\u001b[0m     wantlist: \u001b[39mbool\u001b[39;49m \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   3785\u001b[0m     group: \u001b[39mint\u001b[39;49m \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[0;32m   3786\u001b[0m     comma: Union[Falsish, \u001b[39mstr\u001b[39;49m] \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   3787\u001b[0m     andword: \u001b[39mstr\u001b[39;49m \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mand\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   3788\u001b[0m     zero: \u001b[39mstr\u001b[39;49m \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mzero\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   3789\u001b[0m     one: \u001b[39mstr\u001b[39;49m \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mone\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   3790\u001b[0m     decimal: Union[Falsish, \u001b[39mstr\u001b[39;49m] \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mpoint\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   3791\u001b[0m     threshold: Optional[\u001b[39mint\u001b[39;49m] \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   3792\u001b[0m ) \u001b[39m-\u001b[39;49m\u001b[39m>\u001b[39;49m Union[\u001b[39mstr\u001b[39;49m, List[\u001b[39mstr\u001b[39;49m]]:\n\u001b[0;32m   3793\u001b[0m     \u001b[39m\"\"\"\u001b[39;49;00m\n\u001b[0;32m   3794\u001b[0m \u001b[39m    Return a number in words.\u001b[39;49;00m\n\u001b[0;32m   3795\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3808\u001b[0m \u001b[39m    parameters not remembered from last call. Departure from Perl version.\u001b[39;49;00m\n\u001b[0;32m   3809\u001b[0m \u001b[39m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_number_args \u001b[39m=\u001b[39;49m {\u001b[39m\"\u001b[39;49m\u001b[39mandword\u001b[39;49m\u001b[39m\"\u001b[39;49m: andword, \u001b[39m\"\u001b[39;49m\u001b[39mzero\u001b[39;49m\u001b[39m\"\u001b[39;49m: zero, \u001b[39m\"\u001b[39;49m\u001b[39mone\u001b[39;49m\u001b[39m\"\u001b[39;49m: one}\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pydantic\\decorator.py:36\u001b[0m, in \u001b[0;36mpydantic.decorator.validate_arguments.validate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pydantic\\decorator.py:126\u001b[0m, in \u001b[0;36mpydantic.decorator.ValidatedFunction.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pydantic\\decorator.py:259\u001b[0m, in \u001b[0;36mpydantic.decorator.ValidatedFunction.create_model\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pydantic\\main.py:972\u001b[0m, in \u001b[0;36mpydantic.main.create_model\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pydantic\\main.py:204\u001b[0m, in \u001b[0;36mpydantic.main.ModelMetaclass.__new__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pydantic\\fields.py:488\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pydantic\\fields.py:419\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pydantic\\fields.py:534\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.prepare\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pydantic\\fields.py:633\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField._type_analysis\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pydantic\\fields.py:776\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField._create_sub_type\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ibrao\\miniconda3\\envs\\pytorch\\lib\\site-packages\\pydantic\\fields.py:451\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField._get_field_info\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: `Field` default cannot be set in `Annotated` for 'num_Annotated[str, FieldInfo(min_length=1, extra={})]'"
     ]
    }
   ],
   "source": [
    "from inflect import engine\n",
    "\n",
    "def to_digit(digit):\n",
    "    i = engine()\n",
    "    if digit.isdigit():\n",
    "        output = i.number_to_words(digit)\n",
    "    else:\n",
    "        output = digit\n",
    "    return output\n",
    "\n",
    "input_text = [\"1\",\"two\",\"3\"]\n",
    "output_text = [to_digit(x) for x in input_text]\n",
    "print('Input: ' + str(input_text))\n",
    "print('Output: ' + str(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we have successfully converted our digits into text.\n",
    "\n",
    "However, in a similar fashion to processing email addresses, processing phone numbers may not require the same representation as regular numbers. This is illustrated in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_digit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ibrao\\OneDrive\\Documents\\Deep Learning\\NLP with Pytorch 1.x\\ch04\\notebook.ipynb Cellule 20\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch04/notebook.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_text \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m0800118118\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch04/notebook.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output_text \u001b[39m=\u001b[39m [to_digit(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m input_text]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch04/notebook.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInput: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(input_text))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch04/notebook.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOutput: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(output_text))\n",
      "\u001b[1;32mc:\\Users\\ibrao\\OneDrive\\Documents\\Deep Learning\\NLP with Pytorch 1.x\\ch04\\notebook.ipynb Cellule 20\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch04/notebook.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_text \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m0800118118\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch04/notebook.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output_text \u001b[39m=\u001b[39m [to_digit(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m input_text]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch04/notebook.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInput: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(input_text))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ibrao/OneDrive/Documents/Deep%20Learning/NLP%20with%20Pytorch%201.x/ch04/notebook.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOutput: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(output_text))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'to_digit' is not defined"
     ]
    }
   ],
   "source": [
    "input_text = [\"0800118118\"]\n",
    "output_text = [to_digit(x) for x in input_text]\n",
    "\n",
    "print('Input: ' + str(input_text))\n",
    "print('Output: ' + str(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization is the process by which we arrive at these root words. **Stemming** is an algorithmic process in which the ends of words are cut off to arrive at a common root, whereas lemmatization uses a true vocabulary and structural analysis of the word itself to arrive at the true roots, or **lemmas**, of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the algorithmic process by which we trim the ends off words in order to arrive at their lexical roots, or **stems**. To do this, we can use different **stemmers** that each follow a particular algorithm in order to return the stem of a word. In English, one of the most common stemmers is the **Porter Stemmer**.\n",
    "\n",
    "The **Porter Stemmer** is an algorithm with a large number of logical rules that can be used to return the stem of a word. We will first show how to implement a Porter Stemmer in Python using NLTK before moving on and discussing the algorithm in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see -> see\n",
      "saw -> saw\n",
      "cat -> cat\n",
      "cats -> cat\n",
      "stem -> stem\n",
      "stemming -> stem\n",
      "lemma -> lemma\n",
      "lemmatization -> lemmat\n",
      "known -> known\n",
      "knowing -> know\n",
      "time -> time\n",
      "timing -> time\n",
      "football -> footbal\n",
      "footballers -> footbal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'thecatanddogarerun'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# 1 First we create an instance of the Porter Stemmer\n",
    "porter = PorterStemmer()\n",
    "# 2 We then simply call this instance of the stemmer\n",
    "# on individual words and print the results\n",
    "word_list = [\"see\",\"saw\",\"cat\", \"cats\", \"stem\", \"stemming\",\n",
    "            \"lemma\",\"lemmatization\",\"known\",\"knowing\",\"time\",\n",
    "            \"timing\",\"football\", \"footballers\"]\n",
    "for word in word_list:\n",
    "    print(word + ' -> ' + porter.stem(word))\n",
    "\n",
    "# 3 We can also apply stemming to an entier sentence\n",
    "# first by tokenizing the sentence and then by stemming each term\n",
    "# individually\n",
    "def sentenceStemmer(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    stems = [porter.stem(word) for word in tokens]\n",
    "    return \"\".join(stems)\n",
    "\n",
    "sentenceStemmer('The cats and dogs are running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization differs from stemming in that it reduces words to their **lemma** instead of their stem. While the stem of a word is processed and reduced to a string, **a word's lemma is its true lexical root**. So, while the stem of the word *ran* will just be *ran*, its lemma is the true lexical root of the word, which would be *run*. We will look at using the WordNet Lemmatizer within NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse\n",
      "wolf\n",
      "mouse\n",
      "cactus\n"
     ]
    }
   ],
   "source": [
    "# We will first create an instance of our lemmatizer and call it on a\n",
    "# selection of words\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print(wordnet_lemmatizer.lemmatize('horses'))\n",
    "print(wordnet_lemmatizer.lemmatize('wolves'))\n",
    "print(wordnet_lemmatizer.lemmatize('mice'))\n",
    "print(wordnet_lemmatizer.lemmatize('cacti'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can already begin to see the advantages of using lemmatization over stemming. Since the WordNet Lemmatizer is built on a database of all the words in the English language, it knows that mice is the plural version of mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "madeupwords\n",
      "madeupword\n"
     ]
    }
   ],
   "source": [
    "print(wordnet_lemmatizer.lemmatize('madeupwords'))\n",
    "print(porter.stem('madeupwords'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that, in this instance, our stemmer is able to generalize better to previously unseen words. Therefore, using a lemmatizer may be a problem if we're lemmatizing sources where language doesn't necessarily match up with real English language, such as social media sites where people may frequently abbreviate language.\n",
    "\n",
    "If we call our lemmatizer on two verbs, we will see that this doesn't reduce them to their expected common lemma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "print(wordnet_lemmatizer.lemmatize('run'))\n",
    "print(wordnet_lemmatizer.lemmatize('ran'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because our lemmatizer relies on the context of words to be able to return the lemmas. Recall from our POS analysis that we can easily return the context of a word in a sentence and determine whether a given word is a noun, verb, or adjective. For now, let's manually specify that our words are verbs. We can see that this now correctly returns the lemma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(wordnet_lemmatizer.lemmatize('ran', pos='v'))\n",
    "print(wordnet_lemmatizer.lemmatize('run', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('cats', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('dogs', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('running', 'VBG')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'The cats and dogs are running'\n",
    "\n",
    "def return_word_pos_tuples(sentence):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "return_word_pos_tuples(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "his means that in order to return the correct lemmatization of any given sentence, we must first perform POS tagging to obtain the context of the words in the sentence, then pass this through the lemmatizer to obtain the lemmas of each of the words in the sentence. We first create a function that will return our POS tagging for each word in the sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen both lemmatization and stemming in action, the question still remains as to under which circumstances we should use both of these techniques. We saw that both techniques attempt to reduce each word to its root. In stemming, this may just be a reduced form of the target room, whereas in lemmatization, it reduces to a true English language word root.\n",
    "\n",
    "Because lemmatization requires cross-referencing the target word within the WordNet corpus, as well as performing part-of-speech analysis to determine the form of the lemma, this may take a significant amount of processing time if a large number of words have to be lemmatized. This is in contrast to stemming, which uses a detailed but relatively fast algorithm to stem words. Ultimately, as with many problems in computing, it is a question of trading off speed versus detail. When choosing which of these methods to incorporate in our deep learning pipeline, the trade-off may be between speed and accuracy. If time is of the essence, then stemming may be the way to go. On the other hand, if you need your model to be as detailed and as accurate as possible, then lemmatization will likely result in the superior model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "580318c0c86e0542a7f2f9882bbbc393e6c88c07c2a75daeff3d2ea36de686a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
