{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Text Translation Using Sequence-to-Sequence Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again be using these familiar RNNs, but instead of just building a simple RNN model, we will use RNNs as part of a larger, more complex model in order to perform sequence-to-sequence translation. By using the underpinnings of RNNs that we learned about in the previous chapters, we can show how these concepts can be extended in order to create a variety of models that can be fit for purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory of sequence-of-sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence-to-sequence models are very similar to the conventional neural net structures we have seen so far. **The main difference is that for a model's output, we expect another sequence, rather than a binary or multi-classification prediction**. This is **particularly useful in tasks such as translation, where we may wish to convert a whole sentence into another language**.\\\n",
    "In the following example, we can see that our English-to-Spanish translation maps word to word:\n",
    "\n",
    "![](eng_to_spanish.png)\n",
    "\n",
    "The **first word in our input sentence maps nicely to the first word in our output sentence**. If **this were the case for all languages, we could simply pass each word in our sentence one by one through our trained model to get an output sentence**, and there would be no need for any sequence-to-sequence modeling, as shown here:\n",
    "\n",
    "![](eng_to_spanish_translation.png)\n",
    "\n",
    "However, we know from our experience with NLP that language is not as simple as this! Single words in one language may map to multiple words in other languages, and the order in which these words occur in a grammatically correct sentence may not be the same. Therefore, **we need a model that can capture the context of a whole sentence and output a correct translation, not a model that aims to directly translate individual words. This is where sequence-to-sequence modeling becomes essential**, as seen here:\n",
    "\n",
    "![](seq_2_seq_modeling_translation_for_translation.png)\n",
    "\n",
    "**To train a sequence-to-sequence model that captures the context of the input sentence and translates this into an output sentence**, we will essentially **train two smaller models** that allow us to do this:\n",
    "* An **encoder** model, which **captures the context of our sentence and outputs it as a single context vector**\n",
    "* A **decoder**, which **takes the context vector representation of our original sentence and translates this into a different language**\n",
    "\n",
    "So, in reality, our full sequence-to-sequence translation model will actually look something like this:\n",
    "\n",
    "![](full_seq_2_seq.png)\n",
    "\n",
    "By **splitting our models into individual encoder and decoder elements, we are effectively modularizing our models**. This means that **if we wish to train multiple models to translate from English into different languages, we do not need to retrain the whole model each time**. We **only need to train multiple different decoders to transform our context vector into our output sentences**. Then, when making predictions, we can simply swap out the decoder that we wish to use for our translation:\n",
    "\n",
    "![](detailed_model_layout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the encoder element of our seq2seq model is to **be able to fully capture the context of our input sentence and represent it as a vector**. We can do this by using RNNs or, more specifically, LSTMs. **RNNs take a sequential input and maintain a hidden state throughout this sequence**. **Each new word in the sequence updates the hidden state. Then, at the end of the sequence, we can use the model's final hidden state as our input into our next layer**.\n",
    "\n",
    "In the **case of our encoder, the hidden state represents the context vector representation of our whole sentence**, meaning **we can use the hidden state output of our RNN to represent the entirety of the input sentence**:\n",
    "\n",
    "![](examining_the_encoder.png)\n",
    "\n",
    "We use our final state $h_n$ as our **context vector**, which we will then **decode using a trained decoder**. It is also woth observing that in the context of our seq2seq models, we append *start* and *end* tokens to the beginning and end of our input sequence, respectively. This is because our inputs and outputs do not have a finite length and our model needs to be able to learn when a sentence should end. Our input sentence will always and end with an \"end\" token. Which signals to the encoder that the hidden state, at this point, will be used as the final context vector representation for this input sentence. Similarly, in the decoder step, we will see that our decoder will keep generating words until it predicts an \"end\" token. This allows our decoder to generate actual output sentences, as opposed to a sequence of tokens of infinite length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our decoder **takes the final hidden state from our encoder layer and decodes this into a sentence in another language**. Our **decoder is an RNN, similar to that of our encoder**, but **while our encoder updates its hidden state given its current hidden state and the current word in the sentence**, our **decoder updates its hidden state and outputs a token at each iteration**, **given the current hidden state and the previous predicted word in the sentence**. This can be seen in the following diagram:\n",
    "\n",
    "![](examining_the_decoder.png)\n",
    "\n",
    "First, our model takes the **context vector as the final hidden state from our encoder step**, $h_0$. Our **model then aims to predict the next word in the sentence**, **given the current hidden state, and then the previous word in the sentence**. We know our sentence must begin with a *\"start\"* token so, at our first step, our model tries to predict the first word in the sentence given the previous hidden state, $h_0$, and the previous word in the sentence (in this instance, the *\"start\"* token). Our model makes a prediction (*\"pienso\"*) and then updates the hidden state to reflect the new state of the model, $h_1$. Then, at the next step, **our model uses the new hidden state and the last predicted word to predict the next word in the sentence**. This continues until the model predicts the *\"end\"* token, at which point our model stops generating output words.\n",
    "\n",
    "The intuition behind this model is in line with what we have learned about language representations thus far. **Words in any given sentence are dependent on the words that come before it. So, to predict any given word in a sentence without considering the words that have been predicted before it, this would not make sense as words in any given sentence are not independent from one another**.\n",
    "\n",
    "We learn our model parameters as we have done previously: by making a forward pass, calculating the loss of our target sentence against the predicted sentence, and backpropagating this loss through the network, updating the parameters as we go. However, learning using this process can be very slow because, to begin with, our model will have very little predictive power. Since our predictions for the words in our target sentence are not independent of one another, if we predict the first word in our target sentence incorrectly, subsequent words in our output sentence are also unlikely to be correct. To help with this process, we can use a technique known as **teacher forcing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using teacher forcing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As **our model does not make good predictions initially**, we will find that **any initial errors are multiplied exponentially**. If our first predicted word in the sentence is incorrect, then the rest of the sentence will likely be incorrect as well. This is because **the predictions our model makes are dependent on the previous predictions it makes**. This means that **any losses our model has can be multiplied exponentially**. **Due to this, we may face the exploding gradient problem, making it very difficult for our model to learn anything**:\n",
    "\n",
    "![](using_teaching_forcing.png)\n",
    "\n",
    "However, by using **teacher forcing**, we **train our model using the correct previous target word so that one wrong prediction does not inhibit our model's ability to learn from the correct predictions**. This means that**if our model makes an incorrect prediction at one point in the sentence, it can still make correct predictions using subsequent words**. **While our model will still have incorrectly predicted words and will have losses by which we can update our gradients, now, we do not suffer from exploding gradients, and our model will learn much more quickly**:\n",
    "\n",
    "![](updating_for_loss.png)\n",
    "\n",
    "You can **consider teacher forcing as a way of helping our model learn independently of its previous predictions at each time step**. This is so the losses that are incurred by a mis-prediction at an early time step are not carried over to later time steps.\n",
    "\n",
    "By **combining the encoder and decoder steps and applying teacher forcing to help our model learn, we can build a sequence-to-sequence model that will allow us to translate sequences of one language into another**. In the next section, we will illustrate how we can build this from scratch using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a sequence-to-sequence model for text translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparing the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multi30k dataset in Torchtext consists of approximately 30,000 sentences with corresponding translations in multiple languages. For this translation task, our input sentences will be in English and our output sentences will be in German. Our fully trained model will, therefore, allow us to translate English sentences into German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core__sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 We will start by extracting our data and preprocessing it. \n",
    "# We will once again use spacy, which contains a built-in\n",
    "# dictionary of vocabulary that we can use to tokenize our data:\n",
    "import spacy\n",
    "spacy_german = spacy.load('de_core_news_sm')\n",
    "spacy_english = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 2 Next, we create a function for each of our languages to\n",
    "# tokenize our sentences. Note that our tokenizer for our input\n",
    "# English sentence reverses the order of the tokens:\n",
    "def tokenize_german(text):\n",
    "    return [token.text for token in spacy_german.tokenizer(text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return [token.text for token in spacy_english.tokenizer(text)][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **reversing the order of our input sentence is not compulsory, it has been shown to improve the model’s ability to learn**. If our model consists of two RNNs joined together, we can show that the information flow within our model is improved when reversing the input sentence. For example, let’s take a basic input sentence in English but not reverse it, as follows:\n",
    "\n",
    "![](reversing_the_input_words.png)\n",
    "\n",
    "Here, we can see that in order to predict the first output word, $y_0$, correctly, our first English word from $x_0$ must travel through three RNN layers before the prediction is made.  In terms of learning, this means that our gradients must be backpropagated through three RNN layers, while maintaining the flow of information through the network. Now, let’s compare this to a situation where we reverse our input sentence:\n",
    "\n",
    "![](reversing_the_input_sentence.png)\n",
    "> There might be an error in this picture\n",
    "\n",
    "We can now see that the **distance between the true first word in our input sentence and the corresponding word in the output sentence is just one RNN layer**. This means that **the gradients only need to be backpropagated to one layer**, meaning **the flow of information and the ability to learn is much greater for our network** compared to when the distance between these two words was three layers.\n",
    "\n",
    "If we were to **calculate the total distances between the input words and their output counterparts for the reversed and non-reversed variants, we would see that they are the same**. However, we have seen previously that **the most important word in our output sentence is the first one. This is because the words in our output sentences are dependent on the words that come before them**.\n",
    "\n",
    "If we were to **predict the first word in the output sentence incorrectly, then chances are the rest of the words in our sentences would be predicted incorrectly too**. However, **by predicting the first word correctly, we maximize our chances of predicting the whole sentence correctly**. Therefore, by **minimizing the distance between the first word in our output sentence and its input counterpart, we can increase our model’s ability to learn this relationship**. This increases the chances of this prediction being correct, thus maximizing the chances of our entire output sentence being predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n",
      "['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'm', 'i', 'sind', 'männer', 'weiße', 'junge', 'zwei']\n"
     ]
    }
   ],
   "source": [
    "# 3 With our tokenizers constructed, we now need to define the fields for our tokenization.\n",
    "# Notice here how we append start and end tokens to our sequences so that our model\n",
    "# knows when to begin and end the sequence’s input and output.\n",
    "# We also convert all our input sentences into lowercase for the sake of simplicity:\n",
    "import torch\n",
    "from torchtext.data import Field\n",
    "from torchtext import datasets\n",
    "\n",
    "SOURCE = Field(tokenize=tokenize_german, init_token='<sos>', eos_token='<eos>', lower='True')\n",
    "TARGET = Field(tokenize=tokenize_english, init_token='<sos>', eos_token='<eos>', lower='True')\n",
    "\n",
    "# 4 With our fields defined, our tokenization becomes a simpler one liner.\n",
    "# The dataset containing 30,000 sentences has built-in training, validation,\n",
    "# and test sets that we can use for our model:\n",
    "train_data, valid_data, test_data = datasets.Multi30k.splits(exts=('.en', '.de'), fields = (SOURCE, TARGET))\n",
    "\n",
    "# 5 We can examine individual sentences using the examples property of our dataset\n",
    "# objects. Here, we can see that the source (src) property contains our reversed input\n",
    "# sentence in English and that our target (trg) contains our non-reversed output sentence in German:\n",
    "print(train_data.examples[0].src)\n",
    "print(train_data.examples[0].trg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now, we can examine the size of each of our datasets. Here, we can see that our\n",
    "training dataset consists of $29,000$ examples and that each of our validation and \n",
    "test sets consist of $1,014$ and $1,000$ examples, respectively. In the past, we have\n",
    "used $80\\%/20\\%$ splits for the training and validation data. However, in instances\n",
    "like this, where our input and output fields are very sparse and our training\n",
    "set is of a limited size, it is often beneficial to train on as much data as\n",
    "there is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size : 29000\n",
      "Validation dataset size : 1014\n",
      "Test dataset size : 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset size : \" + str(len(train_data.examples)))\n",
    "print(\"Validation dataset size : \" + str(len(valid_data.examples)))\n",
    "print(\"Test dataset size : \" + str(len(test_data.examples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Now, we can build our vocabularies and check their size. Our vocabularies should consist of every unique word that was found within our dataset. We can see that our German vocabulary is considerably larger than our English vocabulary. Our vocabularies are significantly smaller than the true size of each vocabulary for each language (every word in the English dictionary). Therefore, since our model will only be able to accurately translate words it has seen before, it is unlikely that our model will be able to generalize well to all possible sentences in the English language. This is why training models like this accurately requires extremely large NLP datasets (such as those Google has access to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English (Source) Vocabulary Size: 5972\n",
      "German (Target) Vocabulary Size: 7874\n"
     ]
    }
   ],
   "source": [
    "SOURCE.build_vocab(train_data, min_freq = 2)\n",
    "TARGET.build_vocab(train_data, min_freq = 2)\n",
    "print(\"English (Source) Vocabulary Size: \" +str(len(SOURCE.vocab)))\n",
    "print(\"German (Target) Vocabulary Size: \" +str(len(TARGET.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can create our data iterators from our datasets. As we did previously, we specify the usage of a\n",
    "# CUDA-enabled GPU (if it is available on our system) and specify our batch size:\n",
    "from torchtext import data\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data has been preprocessed, we can start building the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building the encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 First we begin by initializing our model by inheriting from our nn.Module class\n",
    "# as we've done with all our previous models. We initialize with a couple of parameters\n",
    "# which we will define later, as well as the number of dimensions in the hidden layers\n",
    "# within our LSTM layers and the number of LSTM layers:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dims, emb_dims, hid_dims, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dims = hid_dims\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # 2 Next, we define our embedding layer within our encoder, which is the length of the\n",
    "        # number of input dimensions and the depth of the number of embedding dimensions:\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_dims, embedding_dim=emb_dims)\n",
    "\n",
    "        # 3 Next, we define our actual LSTM layer. This takes our embedded sentences from\n",
    "        # the embedding layer, maintains a hidden state of a defined length, and consists\n",
    "        # of a number of layers (which we will define later as 2). We also implement\n",
    "        # dropout to apply regularization to our network:\n",
    "        self.rnn = nn.LSTM(emb_dims, hid_dims, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # 4 Then, we define the forward pass within our encoder. We apply the embeddings\n",
    "    # to our input sentences and apply dropout. Then, we pass these embeddings\n",
    "    # through our LSTM layer, which outputs our final hidden state. This will\n",
    "    # be used by our decoder to form our translated sentence:\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (h, cell) = self.rnn(embedded)\n",
    "        return h, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our encoders will consist of two LSTM layers, which means that our output will output two hidden states. This also means that our full LSTM layer, along with our encoder, will look something like this, with our model outputting two hidden states:\n",
    "\n",
    "![](lstm_with_encoder.png)\n",
    "\n",
    "Now that we have built our encoder, let's start building our decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building the decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our **decoder will take the final hidden states from our encoder's LSTM layer** and **translate them into an output sentence in another language**. We start by **initializing our decoder in almost exactly the same way as we did for the encoder**. The **only difference here is that we also add a fully connected linear layer**. This layer will **use the final hidden states from our LSTM in order to make predictions regarding the correct word in the sentence:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dims, emb_dims, hid_dims, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dims = output_dims\n",
    "        self.hid_dims = hid_dims\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dims, emb_dims)\n",
    "        self.rnn = nn.LSTM(emb_dims, hid_dims, n_layers, dropout = dropout)\n",
    "        self.fc_out = nn.Linear(hid_dims, output_dims)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Our forward pass is incredibly similar to that of our encoder, except\n",
    "    # with the addition of two key steps.\n",
    "    # * We first unsqueeze our input from the previous layer so that it's the correct size for entry into our\n",
    "    # embedding layer.\n",
    "    # * We also add a fully connected layer, which takes the output hidden layer of our RNN layers and uses it\n",
    "    # to make a prediction regarding the next word in the sequence:\n",
    "    def forward(self, input, h, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (h, cell) = self.rnn(embedded, (h, cell))\n",
    "        pred = self.fc_out(output.squeeze(0))\n",
    "        return pred, h, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, **similar to our encoder, we use a $2$-layer LSTM layer within our decoder**. We **take our final hidden state from our encoders and use these to generate the first word in our sequence**, $Y_1$. We then update our hidden state and use this and $Y_1$ to generate our next word, $Y_2$, repeating this process until our model generates an end token. Our decoder looks something like this:\n",
    "\n",
    "![](lstm_with_decoder.png)\n",
    "\n",
    "Here, we can see that defining the encoders and decoders individually is not particularly complicated. However, when we combine these steps into one larger sequence-to-sequence model, things begin to get interesting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Constructing the full sequence-to-sequence model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now stitch the two halves of our model together to produce the full sequence-to-sequence model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 We start by creating a new sequence-to-sequence class.\n",
    "# This will allow us to pass our encoder and decoder to it as arguments:\n",
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    # 2 Next, we create the forward method within our Seq2Seq class.\n",
    "    # This is arguably the most complicated part of the model. We\n",
    "    # combine our encoder with our decoder and use teacher forcing\n",
    "    # to help our model learn. We start by creating a tensor in which\n",
    "    # we still store our predictions. We initialize this as a tensor\n",
    "    # full of zeroes, but we still update this with our predictions as\n",
    "    # we make them. The shape of our tensor of zeroes will be the length\n",
    "    # of our target sentence, the width of our batch size, and the depth\n",
    "    # of our target (German) vocabulary size:\n",
    "    def forward(self, src, trg, teacher_forcing_rate=.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        target_length = trg.shape[0]\n",
    "        target_vocab_size = self.decoder.output_dims\n",
    "        outputs = torch.zeros(target_length, batch_size, target_vocab_size).to(self.device)\n",
    "\n",
    "        # 3 Next, we feed our input sentence into our encoder to get the output hidden states:\n",
    "        h, cell = self.encoder(src)\n",
    "\n",
    "        # 4 Then, we must loop through our decoder model to generate an\n",
    "        # output prediction for each step in our output sequence. The first\n",
    "        # element of our output sequence will always be the <start> token. Our\n",
    "        # target sequences already contain this as the first element, so we\n",
    "        # just set our initial input equal to this by taking the first element of the list:\n",
    "        input = trg[0,:]\n",
    "\n",
    "        # 5 & 6\n",
    "        # We then continue this loop until we have a full prediction for each word in the sequence:\n",
    "        for t in range(1, target_length):\n",
    "            output, h, cell = self.decoder(input, h, cell)\n",
    "            outputs[t] = output\n",
    "            top = output.argmax(1)\n",
    "            input = trg[t] if (random.random() < teacher_forcing_rate) else top\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Next, we loop through and make our predictions. We pass our hidden states (from the output of our encoder) to our decoder, along with our initial input (which is just the <start> token). This returns a prediction for all the words in our sequence. However, we are only interested in the word within our current step; that is, the next word in the sequence. Note how we start our loop from 1 instead of 0, so our first prediction is the second word in the sequence (as the first word that’s predicted will always be the start token).\n",
    "6. This output consists of a vector of the target vocabulary’s length, with a prediction for each word within the vocabulary. We take the `argmax` function to identify the actual word that is predicted by the model.\n",
    "\n",
    "We then need to select our new input for the next step. We **set our teacher forcing ratio to** $50\\%$, which means that $50\\%$ **of the time, we will use the prediction we just made as our next input into our decoder and that the other $50\\%$ of the time, we will take the true target**. As we discussed previously, this helps our model learn much more rapidly than relying on just the model’s predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "# We then continue this loop until we have a full prediction for each word in the sequence:\n",
    "for t in range(1, target_length):\n",
    "    outputs, h, cell = self.decoder(input, h, cell)\n",
    "    outputs[t] = output\n",
    "    top = output.argmax(1)\n",
    "    input = trg[t] if (random.random() < teacher_forcing_rate) else top\n",
    "return outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we create an instance of our Seq2Seq model that’s ready to be trained.\n",
    "# We initialize an encoder and a decoder with a selection of hyperparameters,\n",
    "# all of which can be changed to slightly alter the model:\n",
    "input_dimensions = len(SOURCE.vocab)\n",
    "output_dimensions = len(TARGET.vocab)\n",
    "encoder_embedding_dimensions = 256\n",
    "decoder_embedding_dimensions = 256\n",
    "hidden_layer_dimensions = 512\n",
    "number_of_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "\n",
    "# We then pass our encoder and decoder to our Seq2Seq model in order to create the complete model:\n",
    "encod = Encoder(input_dimensions,\\\n",
    "    encoder_embedding_dimensions,\\\n",
    "    hidden_layer_dimensions,\\\n",
    "    number_of_layers, encoder_dropout)\n",
    "\n",
    "decod = Decoder(output_dimensions,\\\n",
    "    decoder_embedding_dimensions,\\\n",
    "    hidden_layer_dimensions,\\\n",
    "    number_of_layers, decoder_dropout)\n",
    "\n",
    "model = Seq2Seq(encod, decod, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Try experimenting with different parameters here and see how it affects the performance of the model. For instance, having a larger number of dimensions in your hidden layers may cause the model to train slower, although the overall final performance of the model may be better. Alternatively, the model may overfit. Often, it is a matter of experimenting to find the best-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training our model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will begin initialized with weights of $0$ across all parts of the model. While the model should theoretically be able to learn with no (zero) weights, it has been shown that initializing with random weights can help the model learn faster. Let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Here, we will initialize our model with the weights of rangom samples\n",
    "# from a normal distribution, with the values being between -0.1 and 0.1:\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# 2 Next, as with all our other models, we define our optimizer and loss functions.\n",
    "# We’re using cross-entropy loss as we are performing multi-class classification\n",
    "# (as opposed to binary cross-entropy loss for a binary classification):\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TARGET.vocab.stoi[TARGET.pad_token])\n",
    "\n",
    "# 3 Next, we define the training process within a function called train().\n",
    "# First, we set our model to train mode and set the epoch loss to 0:\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # 4 We then loop through each batch within our training iterator and extract the\n",
    "    # sentence to be translated (src) and the correct translation of this sentence\n",
    "    # (trg). We then zero our gradients (to prevent gradient accumulation) and calculate\n",
    "    # the output of our model by passing our model function our inputs and outputs:\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src     \n",
    "        trg = batch.trg     \n",
    "        optimizer.zero_grad()  \n",
    "        output = model(src, trg)\n",
    "\n",
    "        # 5 Next, we need to calculate the loss of our model’s prediction by comparing\n",
    "        # our predicted output to the true, correct translated sentence. We reshape\n",
    "        # our output data and our target data using the shape and view functions in\n",
    "        # order to create two tensors that can be compared to calculate the loss. We\n",
    "        # calculate the loss criterion between our output and trg tensors and then\n",
    "        # backpropagate this loss through the network:\n",
    "        output_dims = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dims)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward() \n",
    "\n",
    "        # 6 We then implement gradient clipping to prevent exploding gradients within\n",
    "        # our model, step our optimizer in order to perform the necessary parameter\n",
    "        # updates via gradient descent, and finally add the loss of the batch to the\n",
    "        # epoch loss. This whole process is repeated for all the batches within a single\n",
    "        # training epoch, whereby the final averaged loss per batch is returned:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. After, we create a similar function called `evaluate()`. This function will calculate the loss of our validation data across the network in order to evaluate how our model performs when translating data it hasn’t seen before. This function is almost identical to our `train()` function, with the exception of the fact that we switch to evaluation mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    # 8 Since we don’t perform any updates for our weights, we need to make sure to implement no_grad mode:\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src     \n",
    "            trg = batch.trg     \n",
    "            optimizer.zero_grad()  \n",
    "            # 9 The only other difference is that we need to make sure we turn off teacher forcing when\n",
    "            # in evaluation mode. We wish to assess our model’s performance on unseen data, and enabling\n",
    "            # teacher forcing would use our correct (target) data to help our model make better predictions.\n",
    "            # We want our model to be able to make perfect, unaided predictions:\n",
    "            output = model(src, trg, 0)\n",
    "            output_dims = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dims)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Finally, we need to create a training loop, within which our train() and evaluate() functions are called. We begin by defining how many epochs we wish to train for and our maximum gradient (for use with gradient clipping). We also set our lowest validation loss to infinity. This will be used later to select our best-performing model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 196.0s\n",
      "\tTrain Loss: 4.1456\n",
      "\t Val. Loss: 4.6866\n",
      "Epoch: 02 | Time: 191.0s\n",
      "\tTrain Loss: 3.8113\n",
      "\t Val. Loss: 4.5953\n",
      "Epoch: 03 | Time: 196.0s\n",
      "\tTrain Loss: 3.5366\n",
      "\t Val. Loss: 4.3454\n",
      "Epoch: 04 | Time: 189.0s\n",
      "\tTrain Loss: 3.2772\n",
      "\t Val. Loss: 4.1235\n",
      "Epoch: 05 | Time: 185.0s\n",
      "\tTrain Loss: 3.0495\n",
      "\t Val. Loss: 4.1022\n",
      "Epoch: 06 | Time: 199.0s\n",
      "\tTrain Loss: 2.8668\n",
      "\t Val. Loss: 4.0610\n",
      "Epoch: 07 | Time: 5076.0s\n",
      "\tTrain Loss: 2.7054\n",
      "\t Val. Loss: 4.0180\n",
      "Epoch: 08 | Time: 197.0s\n",
      "\tTrain Loss: 2.5646\n",
      "\t Val. Loss: 4.0374\n",
      "Epoch: 09 | Time: 201.0s\n",
      "\tTrain Loss: 2.4347\n",
      "\t Val. Loss: 3.9692\n",
      "Epoch: 10 | Time: 197.0s\n",
      "\tTrain Loss: 2.3075\n",
      "\t Val. Loss: 4.0224\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "epochs = 10\n",
    "grad_clip = 1\n",
    "lowest_validation_loss = float('inf')\n",
    "\n",
    "# 11 We then loop through each of our epochs\n",
    "# and within each epoch, calculate our training\n",
    "# and validation loss using our train() and evaluate()\n",
    "# functions. We also time how long this takes by calling\n",
    "# time.time() before and after the training process:\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # 12 Next, for each epoch, we determine whether the model we just trained is\n",
    "    # the best-performing model we have seen thus far. If our model performs the\n",
    "    # best on our validation data (if the validation loss is the lowest we have\n",
    "    # seen so far), we save our model:\n",
    "    if valid_loss < lowest_validation_loss:\n",
    "        lowest_validation_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq.pt')\n",
    "    \n",
    "    # 13 Finally, we simply print our output:\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {np.round(end_time-start_time,0)}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our model, we will take our test set of data and run our English sentences through our model to obtain a prediction of the translation in German. We will then be able to compare this to the true prediction in order to see if our model is making accurate predictions. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We start by creating a `translate()` function. This is functionally identical to the `evaluate()` function we created to calculate the loss over our validation set. However, this time, we are not concerned with the loss of our model, but rather the predicted output. We pass the model our source and target sentences and also make sure we turn teacher forcing off so that our model does not use these to make predictions. We then take our model’s predictions and use an argmax function to determine the index of the word that our model predicted for each word in our predicted output sentence:\n",
    "\n",
    "```python\n",
    "output = model(src, trg, 0)\n",
    "preds = torch.tensor([[torch.argmax(x).item()] for x in output])\n",
    "```\n",
    "\n",
    "2. Then, we can use this index to obtain the actual predicted word from our German vocabulary. Finally, we compare the English input to our model that contains the correct German sentence and the predicted German sentence. Note that here, we use $[1:-1]$ to drop the start and end tokens from our predictions and we reverse the order of our English input (since the input sentences were reversed before they were fed into the model):\n",
    "\n",
    "```py\n",
    "print('English Input: ' + str([SOURCE.vocab.itos[x] for x in src][1:-1][::-1]))\n",
    "print('Correct German Output: ' + str([TARGET.vocab.itos[x] for x in trg][1:-1]))\n",
    "print('Predicted German Output: ' + str([TARGET.vocab.itos[x] for x in preds][1:-1]))\n",
    "```\n",
    "By doing this, we can compare our predicted output with the correct output to assess if our model is able to make accurate predictions. We can see from our model’s predictions that our model is able to translate English sentences into German, albeit far from perfectly. Some of our model’s predictions are exactly the same as the target data, showing that our model translated these sentences perfectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.8814\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('seq2seq.pt'))\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "def translate(model, iterator, limit = 4):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            if i < limit :\n",
    "                src = batch.src\n",
    "                trg = batch.trg\n",
    "                output = model(src, trg, 0)\n",
    "                preds = torch.tensor([[torch.argmax(x).item()] for x in output])\n",
    "                \n",
    "                print('English Input: ' + str([SOURCE.vocab.itos[x] for x in src][1:-1][::-1]))\n",
    "                print('Correct German Output: ' + str([TARGET.vocab.itos[x] for x in trg][1:-1]))\n",
    "                print('Predicted German Output: ' + str([TARGET.vocab.itos[x] for x in preds][1:-1]))\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "_, _, eval_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = 1, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have shown our sequence-to-sequence model to be effective at performing language translation, the model we trained from scratch is not a perfect translator by any means. This is, in part, due to the relatively small size of our training data. We trained our model on a set of $30,000$ English/German sentences. While this might seem very large, in order to train a perfect model, we would require a training set that's several orders of magnitude larger.\n",
    "\n",
    "In theory, we would require several examples of each word in the entire English and German languages for our model to truly understand its context and meaning. For context, the $30,000$ English sentences in our training set consisted of just $6,000$ unique words. The average vocabulary of an English speaker is said to be between $20,000$ and $30,000$ words, which gives us an idea of just how many examples sentences we would need to train a model that performs perfectly. This is probably why the most accurate translation tools are owned by companies with access to vast amounts of language data (such as Google)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Input: ['.', 'hats', 'wearing', 'men', 'two']\n",
      "Correct German Output: ['.', 'mützen', 'mit', 'männer', 'zwei']\n",
      "Predicted German Output: ['.', 'tragen', 'tragen', 'männer', 'zwei']\n",
      "\n",
      "\n",
      "English Input: ['face', 'rock', 'climbing', 'woman', 'young']\n",
      "Correct German Output: ['felswand', 'auf', 'klettert', 'frau', 'junge']\n",
      "Predicted German Output: ['.', 'hinauf', 'felswand', 'eine', 'klettert']\n",
      "\n",
      "\n",
      "English Input: ['.', 'volleyball', 'playing', 'is', 'woman', 'a']\n",
      "Correct German Output: ['.', 'volleyball', 'spielt', 'frau', 'eine']\n",
      "Predicted German Output: ['.', 'volleyball', 'spielt', 'frau', 'eine']\n",
      "\n",
      "\n",
      "English Input: ['.', 'hill', 'up', 'walking', 'are', 'men', 'three']\n",
      "Correct German Output: ['.', 'bergauf', 'gehen', 'männer', 'drei']\n",
      "Predicted German Output: ['.', 'hinunter', 'treppe', 'eine', 'gehen']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = translate(model, eval_iterator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "580318c0c86e0542a7f2f9882bbbc393e6c88c07c2a75daeff3d2ea36de686a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
