{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: The road ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorating state-of-art NLP machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the techniques we have learned in this book so far are highly useful methodologies for training our own machine learning model from scratch, they are far from the most sophisticated models being developed globally. Companies and research groups are constantly striving to create the most advanced machine learning models that will achieve the highest performance on a number of NLP tasks.\n",
    "\n",
    "Currently, there are two NLP models that have the best performance and could be considered state-of-the-art: **BERT** and **GPT-2**. Both models are forms of generalized language models. We will discuss these in more detail in the upcoming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BERT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT**, which stands for **Bidirectional Encoder Representations from Transformers** was developed by Google in 2018 and is widely considered to be the leading model in the foled of NLP, having achieved leading performance in natural language inference and question-answering tasks. Fortunately, this has been released as an open source model, so this can be downoaded and used for NLP tasks of your own.\n",
    "\n",
    "BERT was released as a pre-trained model, which means users can downloaded and implement BERT without the need to retrain the model from scratch each time. he pre-trained model is trained on several corpuses, including the whole of Wikipedia (consisting of $2.5$ billion words) and another corpus of books (which includes a further $800$ million words). However, the main element of BERT that sets it apart from other similar models is the fact that it provides a deep, bidirectional, unsupervised language representation, which is shown to provide a more sophisticated, detailed representation, thus leading to improved performance in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **traditional embedding layers (such as GLoVe) form a single representation of a word** that is **agnostic to the meaning of the word within the sentence**, the **bidirectional BERT model attempts to form representations based on its context**. For example, in these two sentences, the word `bat` **has two different meanings**.\n",
    "\n",
    "*\"The bat flew past my window\"*\n",
    "\n",
    "*\"He hit the baseball with the bat\"*\n",
    "\n",
    "Although the word `bat` is a noun in both sentences, we can discern that the context and meaning of the word is obviously very different, depending on the other words around it. Some words may also have different meanings, depending on whether they are a noun or verb within the sentence:\n",
    "\n",
    "*\"She used to match to light the fire\"*\n",
    "\n",
    "*\"His poor performance meant they had no choice but to fire him\"*\n",
    "\n",
    "**Using the bidirectional language model to form context-dependent representations of words is what truly makes BERT stand out as a state-of-the-art model**. For any given token, we obtain its **input representation** by **combining** the **token**, **position**, and **segment embeddings**:\n",
    "\n",
    "![](BERT_arch.png)\n",
    "\n",
    "However, it is important to **understand how the model arrives at these initial context-dependent token-embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Masked language modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the **bidirectional language representation**, BERT uses two different techniques, the first of which is **masked language modeing**. This methodology effectively **hides $15\\%$ of the words within the input sentences** by **replacing them with a masking token**. The model then **tries to predict the true values of the masked words, based on the context of the other words in the sentence**. This prediction **is made bidirectionally in order to capture the context of the sentence in both directions**:\n",
    "```bash\n",
    "Input: We [MASK_1] hide some of the [MASK_2] in the sentence\n",
    "\n",
    "Labels: MASK_1 = randomly, MASK_2 = words\n",
    "```\n",
    "\n",
    "If our model **can learn to predict the correct context-dependent words**, then we are one step closer to **context-dependent representation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Next sentence prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other technique that BERT uses to learn the language representation is **next sentence prediction**. In this methodology, **our model receives two sentences and our model learns to predict whether the second sentence is the sentence that follows the first sentence**; for example:\n",
    "\n",
    "```bash\n",
    "Sentence A: \"I like to drink coffee\"\n",
    "\n",
    "Sentence B: \"It is my favorite drink\"\n",
    "\n",
    "Is Next Sentence?: True\n",
    "\n",
    "Sentence A: \"I like to drink coffee\"\n",
    "\n",
    "Sentence B: \"The sky is blue\"\n",
    "\n",
    "Is Next Sentence?: False\n",
    "```\n",
    "\n",
    "By passing our model pairs of sentences like this, **it can learn to determine whether any two sentences are related and follow one another**, or **whether they are just two random, unrelated sentences**. Learning these sentence relationships is useful in a language model as many NLP-related tasks, such as question-answering, **require the model to understand the relationship between two sentences**. Training a model on next sentence prediction allows the model to identify some kind of relationship between a pair of sentences, even if that relationship is very basic.\n",
    "\n",
    "BERT is trained using both methodologies (**masked language modeling** and **next sentence prediction**), and the **combined loss function of both techniques is minimized**. By **using two different training methods, our language representation is sufficiently robust and learns how sentences are formed and structured, as well as how different sentences relate to one another**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BERT-Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture builds upon many of the principles we have seen in the previous chapters to provide a sophisticated language representation **using bidirectional encoding**. There are **two different variants of BERT, each consisting of a different number of layers and attention heads**:\n",
    "\n",
    "* **BERT Base**: $12$ *transformer* blocks (layers), $12$ *attention heads*, $~110$ million parameters\n",
    "* **BERT Large**: $24$ *transformer* blocks (layers), $16$ *attention heads*, $~340$ million parameters\n",
    "\n",
    "While BERT Large is just a deeper version of BERT Base with more parameters, we will focus on the architecture of BERT Base.\n",
    "\n",
    "BERT is built by following the principle of a **transformer**, which will now be explained in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture builds upon many of the principles we have seen so far in this book. By now, you should be familiar with the concept of encoders and decoders, **where our model learns an encoder to form a representation of an input sentence, and then learns a decoder to decode this representation into a final output, whether this be a classification or translation task**:\n",
    "\n",
    "![Alt text](transformer_workflow.png)\n",
    "\n",
    "However, **our transformer adds another element of sophistication to this approach**, where **a transformer actually has a stack of encoders and a stack of decoders**, with **each decoder receiving the output of the final encoder as its input**:\n",
    "\n",
    "![Alt text](transformer_workflow_with_multiple_encoders.png)\n",
    "\n",
    "**Within each encoder layer**, we find two constituent parts: **a self-attention layer** and **a feed-forward layer**.\n",
    "* **The self-attention layer** is the layer that **receives the model's input first**. This layer causes the encoder to **examine other words within the input sentence** as **it encodes any received word**, **making the encoding context aware**.\n",
    "* **The output from the self-attention layer is passed forward to a feed-forward layer**, which is applied independently to each position. This can be illustrated diagrammatically like so:\n",
    "\n",
    "![Alt text](feedforward_layer.png)\n",
    "\n",
    "Our decoder layers are almost identical in structure to our encoders, **but they incorporate an additional attention layer**. This attention layer **helps the decoder focus on the relevant part of the encoded representation**, similar to how we saw attention working within our sequence-to-sequence models:\n",
    "\n",
    "![Alt text](attention_methodology.png)\n",
    "\n",
    "We know that **our decoders take input from our final encoder**, so **one linked encoder/decoder** might look something like this:\n",
    "\n",
    "![Alt text](linked_encoder_decoder_array.png)\n",
    "\n",
    "This should provide you with a useful **overview of how the different encoders and decoders are stacked up within the larger model**. Next, we will examine the individual parts in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Encoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique property of transformers is that **words flow through the encoder layers individually and each word in each position has its own path**. While there are some dependencies within the self-attention layer, these don't exist within the feed-forward layer. **The vectors for the individual words are obtained from an embedding layer** and then **fed through a self-attention layer before being fed through a feed-forward network**:\n",
    "\n",
    "![Alt text](encoder_layout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-attention** is arguably the most complex component of the encoder, so we will examine this in more detail first. Let's say we have a $3$-word input sentence; for example, *`\"This is fine\"`*. For **each word** within this sentence, **we represent them as a single word vector** that was **obtained from the embedding layer of our model**. We then **extract $3$ vectors from this single word vector**: a <font color=pink>**query vector**</font>, <font color=pink>**a key vector**</font>, and <font color=pink>**a value vector**</font>. These $3$ vectors are obtained by **multiplying our word vector by $3$ different weight matrices that are obtained while training the model**.\n",
    "\n",
    "If we call our word embeddings for each word in our input sentence, $E_{this}$, $E_{is}$, and $E_{fine}$, we can calculate our **query**, **key**, and **value** vectors like so:\n",
    "\n",
    "##### **Query vectors**\n",
    "\n",
    "$$\n",
    "W^q = \\text{Learned Query Weight Matrix}\\\\\n",
    "Q_{this} = W^q \\times E_{this}\\\\\n",
    "Q_{is} = W^q \\times E_{is}\\\\\n",
    "Q_{fine} = W^q \\times E_{fine}\\\\\n",
    "$$\n",
    "\n",
    "##### **Key vectors**\n",
    "\n",
    "$$\n",
    "W^k = \\text{Learned Key Weight Matrix}\\\\\n",
    "K_{this} = W^k \\times E_{this}\\\\\n",
    "K_{is} = W^k \\times E_{is}\\\\\n",
    "K_{fine} = W^k \\times E_{fine}\\\\\n",
    "$$\n",
    "\n",
    "##### **Value vectors**\n",
    "\n",
    "$$\n",
    "W^v = \\text{Learned Value Weight Matrix}\\\\\n",
    "v_{this} = W^v \\times E_{this}\\\\\n",
    "v_{is} = W^v \\times E_{is}\\\\\n",
    "v_{fine} = W^v \\times E_{fine}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to calculate each of these vectors, it is **important to understand what each of them represents**. Essentially, **each of these is an abstraction of a concept within the attention mechanism**. This will become apparent once we see how they are calculated.\n",
    "\n",
    "Let's continue with our working example. We need to **consider each word within our input sentence in turn**. To do this, we **calculate a score for each pair of *`query/key`* vectors in our sentence**. This is **done by obtaining the dot product of each *`query/key`* vector pair for each word within our input sentence**.\n",
    "\n",
    "For example, to **calculate the scores for the first word** in the sentence, *`\"this\"`*, we **calculate the <font color=pink>dot product between the query vector for *`\"this\"`* and the key vector in position $0$**</font>. We **repeat this for the key vectors in all other positions within the input sentence**, so we **obtain $n$ scores for the first word in our input sentence**, where $n$ is the **length of the sentence**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Scores(*\"this\"*) vectors**\n",
    "\n",
    "$$\n",
    "S^0_{this} = q_{this} \\cdot k_{this}\\\\\n",
    "S^1_{this} = q_{this} \\cdot k_{is}\\\\\n",
    "S^2_{this} = q_{this} \\cdot k_{fine}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply a `softmax` function **to each of these scores** so that the value of each is now between $0$ and $1$ (as **this helps prevent exploding gradients and makes gradient descent more efficient and easily calculable**). We then **multiply each of these scores by the value vectors and sum these all up to obtain a final vector**, which is then passed forward within the encoder:\n",
    "\n",
    "##### **Final vector(*\"this\"*)**\n",
    "\n",
    "$$\n",
    "V^0 = S^0_{this} \\cdot V_{this}\\\\\n",
    "V^1 = S^1_{this} \\cdot V_{is}\\\\\n",
    "V^2 = S^2_{this} \\cdot V_{fine}\\\\\n",
    "Final = V^0 + V^1 + V^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then repeat this procedure for all the words within the input sentence so that we obtain a final vector for each word, incorporating an element of self-attention, which is then passed along the encoder to the feed-forward network. This self-attention process means that our encoder knows where to look within the input sentence to obtain the information it needs for the task.\n",
    "\n",
    "In this example, we **only learned a single matrix of weights for our query, key, and value vectors**. However, **we can actually learn multiple different matrices for each of these elements and apply these simultaneously across our input sentence to obtain our final outputs**. This is what's known as **multi-headed attention** and allows us **to perform more complex attention calculations, relying on multiple different learned patterns rather than just a single attention mechanism**.\n",
    "\n",
    "We know that BERT incorporates $12$ attention heads, meaning that $12$ different weight matrices are learned for $W_q$, $W_k$, and $W_v$. Finally, we need a way for our encoders to account for the order of words in the input sequence. Currently, our model treats each word in our input sequence independently, but in reality, the order of the words in the input sequence will make a huge difference to the overall meaning of the sentence. To account for this, we use **positional encoding**.\n",
    "\n",
    "To apply this, our model takes each input embedding and adds a positional encoding vector to each one individually. These positional vectors are learned by our model, following a specific pattern to help them determine the position of each word in the sequence. In theory, adding these positional vectors to our initial embeddings should translate into meaningful distances between our final vectors, once they are projected into the individual query, key, and value vectors:\n",
    "\n",
    "$x_0$ = Raw Embedding\n",
    "\n",
    "$t_0$ = Positional Encoding\n",
    "\n",
    "$E_0$ = Embedding with Time Signal\n",
    "\n",
    "$x_0 + t_0 = E_0$\n",
    "\n",
    "Our model learns different positional encoding vectors for each position ($t_0$, $t_1$, and so on), which we then apply to each word in our input sentence before these even enter our encoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](adding_input_to_encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have covered the main components of the encoder, it's time to look at the other side of the model and see how the decoder is constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Decoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components in decoders are much the same of those in encoders. However, rather than receiving the raw input sentence like encoders do, the decoders in our transformer receive their inputs from the outputs of our encoders.\n",
    "\n",
    "Our stacked encoders process our input sentence and we are left with a set of attention vectors, $K$ and $V$, which are used within the encoder-decoder attention layer of our decoder. This allows it to focus only on the relevant parts of the input sequence:\n",
    "\n",
    "![Alt text](stacked_decoders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step, our decoders use a combination of the previously generated words in the sentence and the $K$,$V$ attention vectors to generate the next word in the sentence. This process is repeated iteratively until the decoder generates an `<END>` token, indicating that it has completed generating the final output. One given time step on the transformer decoder may look like this:\n",
    "\n",
    "![Alt text](transformer_decoder.png)\n",
    "\n",
    "It is worth noting here that the self-attention layers within the decoders operate in a slightly different way to those found in our encoders. Within our decoder, the self-attention layer only focuses on earlier positions within the output sequence. This is done by masking any future positions of the sequence by setting them to minus infinity. This means that when the classification happens, the softmax calculation always results in a prediction of 0.\n",
    "\n",
    "The encoder-decoder attention layer works in the same way as the multi-headed self-attention layer within our encoder. However, the main difference is that it creates a query matrix from the layer below and takes the key and values matrix from the output of the encoders.\n",
    "\n",
    "These encoder and decoder parts comprise our transformer, which forms the basis for BERT."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
